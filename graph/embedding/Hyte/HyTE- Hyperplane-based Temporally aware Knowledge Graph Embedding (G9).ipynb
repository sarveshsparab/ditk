{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import *\n",
    "from helper import *\n",
    "from random import *\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt, uuid, sys, os, time, argparse\n",
    "import pickle, pdb, operator, random, sys\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "from collections import defaultdict as ddict\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from parentclass import GraphEmbedding\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inheriting the common parent class that loads/reads data, trains and saves the model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "YEARMIN = -50\n",
    "YEARMAX = 3000\n",
    "\n",
    "class GraphEmbeddingFromModel(Model):\n",
    "\n",
    "    def read_valid(self,filename):\n",
    "        valid_triples = []\n",
    "        with open(filename,'r') as filein:\n",
    "            temp = []\n",
    "            for line in filein:\n",
    "                temp = [int(x.strip()) for x in line.split()[0:3]]\n",
    "                temp.append([line.split()[3],line.split()[4]])\n",
    "                valid_triples.append(temp)\n",
    "        return valid_triples\n",
    "\n",
    "    def getOneHot(self, start_data, end_data, num_class):\n",
    "        temp = np.zeros((len(start_data), num_class), np.float32)\n",
    "        for i, ele in enumerate(start_data):\n",
    "            if end_data[i] >= start_data[i]:\n",
    "                temp[i,start_data[i]:end_data[i]+1] = 1/(end_data[i]+1-start_data[i]) \n",
    "            else:\n",
    "                pdb.set_trace()\n",
    "        return temp \n",
    "\n",
    "\n",
    "\n",
    "    def getBatches(self, data, shuffle = True):\n",
    "        if shuffle: random.shuffle(data)\n",
    "        num_batches = len(data) // self.p.batch_size\n",
    "\n",
    "        for i in range(num_batches):\n",
    "            start_idx = i * self.p.batch_size\n",
    "            yield data[start_idx : start_idx + self.p.batch_size]\n",
    "\n",
    "\n",
    "    def create_year2id(self,triple_time):\n",
    "        year2id = dict()\n",
    "        freq = ddict(int)\n",
    "        count = 0\n",
    "        year_list = []\n",
    "\n",
    "        for k,v in triple_time.items():\n",
    "            try:\n",
    "                start = v[0].split('-')[0]\n",
    "                end = v[1].split('-')[0]\n",
    "            except:\n",
    "                pdb.set_trace()\n",
    "\n",
    "            if start.find('#') == -1 and len(start) == 4: year_list.append(int(start))\n",
    "            if end.find('#') == -1 and len(end) ==4: year_list.append(int(end))\n",
    "\n",
    "        # for k,v in entity_time.items():\n",
    "        #   start = v[0].split('-')[0]\n",
    "        #   end = v[1].split('-')[0]\n",
    "            \n",
    "        #   if start.find('#') == -1 and len(start) == 4: year_list.append(int(start))\n",
    "        #   if end.find('#') == -1 and len(end) ==4: year_list.append(int(end))\n",
    "        #   # if int(start) > int(end):\n",
    "        #   #   pdb.set_trace()\n",
    "        \n",
    "        year_list.sort()\n",
    "        for year in year_list:\n",
    "            freq[year] = freq[year] + 1\n",
    "\n",
    "        year_class =[]\n",
    "        count = 0\n",
    "        for key in sorted(freq.keys()):\n",
    "            count += freq[key]\n",
    "            if count > 300:\n",
    "                year_class.append(key)\n",
    "                count = 0\n",
    "        prev_year = 0\n",
    "        i=0\n",
    "        for i,yr in enumerate(year_class):\n",
    "            year2id[(prev_year,yr)] = i\n",
    "            prev_year = yr+1\n",
    "        year2id[(prev_year, max(year_list))] = i + 1\n",
    "        self.year_list =year_list\n",
    "        # pdb.set_trace()\n",
    "\n",
    "        # pdb.set_trace()\n",
    "        # for k,v in entity_time.items():\n",
    "        #   if v[0] == '####-##-##' or v[1] == '####-##-##':\n",
    "        #       continue\n",
    "        #   if len(v[0].split('-')[0])!=4 or len(v[1].split('-')[0])!=4:\n",
    "        #       continue\n",
    "        #   start = v[0].split('-')[0]\n",
    "        #   end = v[1].split('-')[0]\n",
    "        # for start in start_list:\n",
    "        #   if start not in start_year2id:\n",
    "        #       start_year2id[start] = count_start\n",
    "        #       count_start+=1\n",
    "\n",
    "        # for end in end_list:\n",
    "        #   if end not in end_year2id:\n",
    "        #       end_year2id[end] = count_end\n",
    "        #       count_end+=1\n",
    "        \n",
    "        return year2id\n",
    "    def get_span_ids(self, start, end):\n",
    "        start =int(start)\n",
    "        end=int(end)\n",
    "        if start > end:\n",
    "            end = YEARMAX\n",
    "\n",
    "        if start == YEARMIN:\n",
    "            start_lbl = 0\n",
    "        else:\n",
    "            for key,lbl in sorted(self.year2id.items(), key=lambda x:x[1]):\n",
    "                if start >= key[0] and start <= key[1]:\n",
    "                    start_lbl = lbl\n",
    "        \n",
    "        if end == YEARMAX:\n",
    "            end_lbl = len(self.year2id.keys())-1\n",
    "        else:\n",
    "            for key,lbl in sorted(self.year2id.items(), key=lambda x:x[1]):\n",
    "                if end >= key[0] and end <= key[1]:\n",
    "                    end_lbl = lbl\n",
    "        return start_lbl, end_lbl\n",
    "\n",
    "    def create_id_labels(self,triple_time,dtype):\n",
    "        YEARMAX = 3000\n",
    "        YEARMIN =  -50\n",
    "        \n",
    "        inp_idx, start_idx, end_idx =[], [], []\n",
    "        \n",
    "        for k,v in triple_time.items():\n",
    "            start = v[0].split('-')[0]\n",
    "            end = v[1].split('-')[0]\n",
    "            if start == '####':\n",
    "                start = YEARMIN\n",
    "            elif start.find('#') != -1 or len(start)!=4:\n",
    "                continue\n",
    "\n",
    "            if end == '####':\n",
    "                end = YEARMAX\n",
    "            elif end.find('#')!= -1 or len(end)!=4:\n",
    "                continue\n",
    "            \n",
    "            start = int(start)\n",
    "            end = int(end)\n",
    "            \n",
    "            if start > end:\n",
    "                end = YEARMAX\n",
    "            inp_idx.append(k)\n",
    "            if start == YEARMIN:\n",
    "                start_idx.append(0)\n",
    "            else:\n",
    "                for key,lbl in sorted(self.year2id.items(), key=lambda x:x[1]):\n",
    "                    if start >= key[0] and start <= key[1]:\n",
    "                        start_idx.append(lbl)\n",
    "            \n",
    "            if end == YEARMAX:\n",
    "                end_idx.append(len(self.year2id.keys())-1)\n",
    "            else:\n",
    "                for key,lbl in sorted(self.year2id.items(), key=lambda x:x[1]):\n",
    "                    if end >= key[0] and end <= key[1]:\n",
    "                        end_idx.append(lbl)\n",
    "\n",
    "        return inp_idx, start_idx, end_idx\n",
    "\n",
    "    def load_data(self):\n",
    "        triple_set = []\n",
    "        with open(self.p.triple2id,'r') as filein:\n",
    "            for line in filein:\n",
    "                tup = (int(line.split()[0].strip()) , int(line.split()[1].strip()), int(line.split()[2].strip()))\n",
    "                triple_set.append(tup)\n",
    "        triple_set=set(triple_set)\n",
    "\n",
    "        train_triples = []\n",
    "        self.start_time , self.end_time, self.num_class  = ddict(dict), ddict(dict), ddict(dict)\n",
    "        triple_time, entity_time = dict(), dict()\n",
    "        self.inp_idx, self.start_idx, self.end_idx ,self.labels = ddict(list), ddict(list), ddict(list), ddict(list)\n",
    "        max_ent, max_rel, count = 0, 0, 0\n",
    "\n",
    "        with open(self.p.dataset,'r') as filein:\n",
    "            for line in filein:\n",
    "                train_triples.append([int(x.strip()) for x in line.split()[0:3]])\n",
    "                triple_time[count] = [x.split('-')[0] for x in line.split()[3:5]]\n",
    "                count+=1\n",
    "\n",
    "\n",
    "        # self.start_time['triple'], self.end_time['triple'] = self.create_year2id(triple_time,'triple')\n",
    "\n",
    "        with open(self.p.entity2id,'r') as filein2:\n",
    "            for line in filein2:\n",
    "                # entity_time[int(line.split('\\t')[1])]=[x.split()[0] for x in line.split()[2:4]]\n",
    "                max_ent = max_ent+1\n",
    "\n",
    "        self.year2id = self.create_year2id(triple_time)\n",
    "        # self.start_time['entity'], self.end_time['entity'] = self.create_year2id(entity_time,'entiy')\n",
    "        # self.inp_idx['entity'],self.start_idx['entity'], self.end_idx['entity'] = self.create_id_labels(entity_time,'entity')\n",
    "        self.inp_idx['triple'], self.start_idx['triple'], self.end_idx['triple'] = self.create_id_labels(triple_time,'triple')\n",
    "        #pdb.set_trace()    \n",
    "        for i,ele in enumerate(self.inp_idx['entity']):\n",
    "            if self.start_idx['entity'][i] > self.end_idx['entity'][i]:\n",
    "                print(self.inp_idx['entity'][i],self.start_idx['entity'][i],self.end_idx['entity'][i])\n",
    "        self.num_class = len(self.year2id.keys())\n",
    "        \n",
    "        # for dtype in ['entity','triple']:\n",
    "        #   self.labels[dtype] = self.getOneHot(self.start_idx[dtype],self.end_idx[dtype], self.num_class)# Representing labels by one hot notation\n",
    "\n",
    "        keep_idx = set(self.inp_idx['triple'])\n",
    "        for i in range (len(train_triples)-1,-1,-1):\n",
    "            if i not in keep_idx:\n",
    "                del train_triples[i]\n",
    "\n",
    "        with open(self.p.relation2id, 'r') as filein3:\n",
    "            for line in filein3:\n",
    "                max_rel = max_rel +1\n",
    "        index = randint(1,len(train_triples))-1\n",
    "        \n",
    "        posh, rela, post = zip(*train_triples)\n",
    "        head, rel, tail = zip(*train_triples)\n",
    "\n",
    "        posh = list(posh) \n",
    "        post = list(post)\n",
    "        rela = list(rela)\n",
    "\n",
    "        head  =  list(head) \n",
    "        tail  =  list(tail)\n",
    "        rel   =  list(rel)\n",
    "\n",
    "        for i in range(len(posh)):\n",
    "            if self.start_idx['triple'][i] < self.end_idx['triple'][i]:\n",
    "                for j in range(self.start_idx['triple'][i] + 1,self.end_idx['triple'][i] + 1):\n",
    "                    head.append(posh[i])\n",
    "                    rel.append(rela[i])\n",
    "                    tail.append(post[i])\n",
    "                    self.start_idx['triple'].append(j)\n",
    "\n",
    "        self.ph, self.pt, self.r,self.nh, self.nt , self.triple_time  = [], [], [], [], [], []\n",
    "        for triple in range(len(head)):\n",
    "            neg_set = set()\n",
    "            for k in range(self.p.M):\n",
    "                possible_head = randint(0,max_ent-1)\n",
    "                while (possible_head, rel[triple], tail[triple]) in triple_set or (possible_head, rel[triple],tail[triple]) in neg_set:\n",
    "                    possible_head = randint(0,max_ent-1)\n",
    "                self.nh.append(possible_head)\n",
    "                self.nt.append(tail[triple])\n",
    "                self.r.append(rel[triple])\n",
    "                self.ph.append(head[triple])\n",
    "                self.pt.append(tail[triple])\n",
    "                self.triple_time.append(self.start_idx['triple'][triple])\n",
    "                neg_set.add((possible_head, rel[triple],tail[triple]))\n",
    "        \n",
    "        for triple in range(len(tail)):\n",
    "            neg_set = set()\n",
    "            for k in range(self.p.M):\n",
    "                possible_tail = randint(0,max_ent-1)\n",
    "                while (head[triple], rel[triple],possible_tail) in triple_set or (head[triple], rel[triple],possible_tail) in neg_set:\n",
    "                    possible_tail = randint(0,max_ent-1)\n",
    "                self.nh.append(head[triple])\n",
    "                self.nt.append(possible_tail)\n",
    "                self.r.append(rel[triple])\n",
    "                self.ph.append(head[triple])\n",
    "                self.pt.append(tail[triple])\n",
    "                self.triple_time.append(self.start_idx['triple'][triple])\n",
    "                neg_set.add((head[triple], rel[triple],possible_tail))\n",
    "\n",
    "        # self.triple_time = triple_time\n",
    "        # self.entity_time = entity_time\n",
    "        self.max_rel = max_rel\n",
    "        self.max_ent = max_ent\n",
    "        self.max_time = len(self.year2id.keys())\n",
    "        self.data = list(zip(self.ph, self.pt, self.r , self.nh, self.nt, self.triple_time))\n",
    "        self.data = self.data + self.data[0:self.p.batch_size]\n",
    "\n",
    "    def add_placeholders(self):\n",
    "        self.start_year = tf.placeholder(tf.int32, shape=[None], name = 'start_time')\n",
    "        self.end_year   = tf.placeholder(tf.int32, shape=[None],name = 'end_time')\n",
    "        self.pos_head   = tf.placeholder(tf.int32, [None,1])\n",
    "        self.pos_tail   = tf.placeholder(tf.int32, [None,1])\n",
    "        self.rel        = tf.placeholder(tf.int32, [None,1])\n",
    "        self.neg_head   = tf.placeholder(tf.int32, [None,1])\n",
    "        self.neg_tail   = tf.placeholder(tf.int32, [None,1])\n",
    "        self.mode       = tf.placeholder(tf.int32, shape = ())\n",
    "        self.pred_mode  = tf.placeholder(tf.int32, shape = ())\n",
    "        self.query_mode = tf.placeholder(tf.int32, shape = ())\n",
    "\n",
    "    def create_feed_dict(self, batch, wLabels=True,dtype='train'):\n",
    "        ph, pt, r, nh, nt, start_idx = zip(*batch)\n",
    "        feed_dict = {}\n",
    "        feed_dict[self.pos_head] = np.array(ph).reshape(-1,1)\n",
    "        feed_dict[self.pos_tail] = np.array(pt).reshape(-1,1)\n",
    "        feed_dict[self.rel] = np.array(r).reshape(-1,1)\n",
    "        feed_dict[self.start_year] = np.array(start_idx)\n",
    "        # feed_dict[self.end_year]   = np.array(end_idx)\n",
    "        if dtype == 'train':\n",
    "            feed_dict[self.neg_head] = np.array(nh).reshape(-1,1)\n",
    "            feed_dict[self.neg_tail] = np.array(nt).reshape(-1,1)\n",
    "            feed_dict[self.mode]     = 1\n",
    "            feed_dict[self.pred_mode] = 0\n",
    "            feed_dict[self.query_mode] = 0\n",
    "        else: \n",
    "            feed_dict[self.mode] = -1\n",
    "\n",
    "        return feed_dict\n",
    "\n",
    "\n",
    "    def time_projection(self,data,t):\n",
    "        inner_prod  = tf.tile(tf.expand_dims(tf.reduce_sum(data*t,axis=1),axis=1),[1,self.p.inp_dim])\n",
    "        prod        = t*inner_prod\n",
    "        data = data - prod\n",
    "        return data\n",
    "\n",
    "    def add_model(self):\n",
    "        #nn_in = self.input_x\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "            self.ent_embeddings = tf.get_variable(name = \"ent_embedding\",  shape = [self.max_ent, self.p.inp_dim], initializer = tf.contrib.layers.xavier_initializer(uniform = False), regularizer=self.regularizer)\n",
    "            self.rel_embeddings = tf.get_variable(name = \"rel_embedding\",  shape = [self.max_rel, self.p.inp_dim], initializer = tf.contrib.layers.xavier_initializer(uniform = False), regularizer=self.regularizer)\n",
    "            self.time_embeddings = tf.get_variable(name = \"time_embedding\",shape = [self.max_time, self.p.inp_dim], initializer = tf.contrib.layers.xavier_initializer(uniform =False))\n",
    "\n",
    "        transE_in_dim = self.p.inp_dim\n",
    "        transE_in     = self.ent_embeddings\n",
    "        ####################------------------------ time aware GCN MODEL ---------------------------##############\n",
    "\n",
    "\n",
    "    \n",
    "        ## Some transE style model ####\n",
    "        \n",
    "        neutral = tf.constant(0)      ## mode = 1 for train mode = -1 test\n",
    "        test_type = tf.constant(0)    ##  pred_mode = 1 for head -1 for tail\n",
    "        query_type = tf.constant(0)   ## query mode  =1 for head tail , -1 for rel\n",
    "        \n",
    "        def f_train():\n",
    "            pos_h_e = tf.squeeze(tf.nn.embedding_lookup(transE_in, self.pos_head))\n",
    "            pos_t_e = tf.squeeze(tf.nn.embedding_lookup(transE_in, self.pos_tail))\n",
    "            pos_r_e = tf.squeeze(tf.nn.embedding_lookup(self.rel_embeddings, self.rel))\n",
    "            return pos_h_e, pos_t_e, pos_r_e\n",
    "        \n",
    "        def f_test():\n",
    "            def head_tail_query():\n",
    "                def f_head():\n",
    "                    e2 = tf.squeeze(tf.nn.embedding_lookup(transE_in, self.pos_tail))\n",
    "                    pos_h_e = transE_in\n",
    "                    pos_t_e = tf.reshape(tf.tile(e2,[self.max_ent]),(self.max_ent, transE_in_dim))\n",
    "                    return pos_h_e, pos_t_e\n",
    "                \n",
    "                def f_tail():\n",
    "                    e1 = tf.squeeze(tf.nn.embedding_lookup(transE_in, self.pos_head))\n",
    "                    pos_h_e = tf.reshape(tf.tile(e1,[self.max_ent]),(self.max_ent, transE_in_dim))\n",
    "                    pos_t_e = transE_in\n",
    "                    return pos_h_e, pos_t_e\n",
    "\n",
    "                pos_h_e, pos_t_e  = tf.cond(self.pred_mode > test_type, f_head, f_tail)\n",
    "                r  = tf.squeeze(tf.nn.embedding_lookup(self.rel_embeddings,self.rel))\n",
    "                pos_r_e = tf.reshape(tf.tile(r,[self.max_ent]),(self.max_ent,transE_in_dim))\n",
    "                return pos_h_e, pos_t_e, pos_r_e\n",
    "            \n",
    "            def rel_query():\n",
    "                e1 = tf.squeeze(tf.nn.embedding_lookup(transE_in, self.pos_head))\n",
    "                e2 = tf.squeeze(tf.nn.embedding_lookup(transE_in, self.pos_tail))\n",
    "                pos_h_e = tf.reshape(tf.tile(e1,[self.max_rel]),(self.max_rel, transE_in_dim))\n",
    "                pos_t_e = tf.reshape(tf.tile(e2,[self.max_rel]),(self.max_rel, transE_in_dim))\n",
    "                pos_r_e = self.rel_embeddings\n",
    "                return pos_h_e, pos_t_e, pos_r_e\n",
    "\n",
    "            pos_h_e, pos_t_e, pos_r_e = tf.cond(self.query_mode > query_type, head_tail_query, rel_query)\n",
    "            return pos_h_e, pos_t_e, pos_r_e\n",
    "\n",
    "        pos_h_e, pos_t_e, pos_r_e = tf.cond(self.mode > neutral, f_train, f_test)\n",
    "        neg_h_e = tf.squeeze(tf.nn.embedding_lookup(transE_in, self.neg_head))\n",
    "        neg_t_e = tf.squeeze(tf.nn.embedding_lookup(transE_in, self.neg_tail))\n",
    "\n",
    "        #### ----- time -----###\n",
    "        t_1 = tf.squeeze(tf.nn.embedding_lookup(self.time_embeddings, self.start_year))\n",
    "        \n",
    "        pos_h_e_t_1 = self.time_projection(pos_h_e,t_1)\n",
    "        neg_h_e_t_1 = self.time_projection(neg_h_e,t_1)\n",
    "        pos_t_e_t_1 = self.time_projection(pos_t_e,t_1)\n",
    "        neg_t_e_t_1 = self.time_projection(neg_t_e,t_1)\n",
    "        pos_r_e_t_1 = self.time_projection(pos_r_e,t_1)\n",
    "        # pos_r_e_t_1 = pos_r_e\n",
    "\n",
    "        if self.p.L1_flag:\n",
    "            pos = tf.reduce_sum(abs(pos_h_e_t_1 + pos_r_e_t_1 - pos_t_e_t_1), 1, keep_dims = True) \n",
    "            neg = tf.reduce_sum(abs(neg_h_e_t_1 + pos_r_e_t_1 - neg_t_e_t_1), 1, keep_dims = True) \n",
    "            #self.predict = pos\n",
    "        else:\n",
    "            pos = tf.reduce_sum((pos_h_e_t_1 + pos_r_e_t_1 - pos_t_e_t_1) ** 2, 1, keep_dims = True) \n",
    "            neg = tf.reduce_sum((neg_h_e_t_1 + pos_r_e_t_1 - neg_t_e_t_1) ** 2, 1, keep_dims = True) \n",
    "            #self.predict = pos\n",
    "\n",
    "        '''\n",
    "        debug_nn([self.pred_mode,self.mode], feed_dict = self.create_feed_dict(self.data[0:self.p.batch_size],dtype='test'))\n",
    "        '''\n",
    "        return pos, neg\n",
    "\n",
    "    def add_loss(self, pos, neg):\n",
    "        with tf.name_scope('Loss_op'):\n",
    "            loss     = tf.reduce_sum(tf.maximum(pos - neg + self.p.margin, 0))\n",
    "            if self.regularizer != None: loss += tf.contrib.layers.apply_regularization(self.regularizer, tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\n",
    "            return loss\n",
    "\n",
    "    def add_optimizer(self, loss):\n",
    "        with tf.name_scope('Optimizer'):\n",
    "            optimizer = tf.train.AdamOptimizer(self.p.lr)\n",
    "            train_op  = optimizer.minimize(loss)\n",
    "        time_normalizer = tf.assign(self.time_embeddings, tf.nn.l2_normalize(self.time_embeddings,dim = 1))\n",
    "        return train_op\n",
    "\n",
    "    def __init__(self, params):\n",
    "        self.p  = params\n",
    "        self.p.batch_size = self.p.batch_size\n",
    "        if self.p.l2 == 0.0:    self.regularizer = None\n",
    "        else:           self.regularizer = tf.contrib.layers.l2_regularizer(scale=self.p.l2)\n",
    "        self.load_data()\n",
    "        self.nbatches = len(self.data) // self.p.batch_size\n",
    "        self.add_placeholders()\n",
    "        self.pos, neg   = self.add_model()\n",
    "        self.loss       = self.add_loss(self.pos, neg)\n",
    "        self.train_op   = self.add_optimizer(self.loss)\n",
    "        self.merged_summ = tf.summary.merge_all()\n",
    "        self.summ_writer = None\n",
    "        print('model done')\n",
    "\n",
    "    def run_epoch(self, sess,data,epoch):\n",
    "        #sess, ph_addr, pt_addr, r_addr, nh_addr, nt_addr, self.p.batch_size, epoch\n",
    "        drop_rate = self.p.dropout\n",
    "\n",
    "        losses = []\n",
    "        # total_correct, total_cnt = 0, 0\n",
    "\n",
    "        for step, batch in enumerate(self.getBatches(data, shuffle)):\n",
    "            feed = self.create_feed_dict(batch)\n",
    "            l, a = sess.run([self.loss, self.train_op],feed_dict = feed)\n",
    "            # print(l,step)\n",
    "            losses.append(l)\n",
    "            # pdb.set_trace()\n",
    "        return np.mean(losses)\n",
    "\n",
    "    def read_dataset(self, file_names=None):\n",
    "        triple_set = []\n",
    "        with open(self.p.triple2id,'r') as filein:\n",
    "            for line in filein:\n",
    "                tup = (int(line.split()[0].strip()) , int(line.split()[1].strip()), int(line.split()[2].strip()))\n",
    "                triple_set.append(tup)\n",
    "        triple_set=set(triple_set)\n",
    "\n",
    "        train_triples = []\n",
    "        self.start_time , self.end_time, self.num_class  = ddict(dict), ddict(dict), ddict(dict)\n",
    "        triple_time, entity_time = dict(), dict()\n",
    "        self.inp_idx, self.start_idx, self.end_idx ,self.labels = ddict(list), ddict(list), ddict(list), ddict(list)\n",
    "        max_ent, max_rel, count = 0, 0, 0\n",
    "\n",
    "        with open(self.p.dataset,'r') as filein:\n",
    "            for line in filein:\n",
    "                train_triples.append([int(x.strip()) for x in line.split()[0:3]])\n",
    "                triple_time[count] = [x.split('-')[0] for x in line.split()[3:5]]\n",
    "                count+=1\n",
    "\n",
    "\n",
    "        # self.start_time['triple'], self.end_time['triple'] = self.create_year2id(triple_time,'triple')\n",
    "\n",
    "        with open(self.p.entity2id,'r') as filein2:\n",
    "            for line in filein2:\n",
    "                # entity_time[int(line.split('\\t')[1])]=[x.split()[0] for x in line.split()[2:4]]\n",
    "                max_ent = max_ent+1\n",
    "\n",
    "        self.year2id = self.create_year2id(triple_time)\n",
    "        # self.start_time['entity'], self.end_time['entity'] = self.create_year2id(entity_time,'entiy')\n",
    "        # self.inp_idx['entity'],self.start_idx['entity'], self.end_idx['entity'] = self.create_id_labels(entity_time,'entity')\n",
    "        self.inp_idx['triple'], self.start_idx['triple'], self.end_idx['triple'] = self.create_id_labels(triple_time,'triple')\n",
    "        #pdb.set_trace()    \n",
    "        for i,ele in enumerate(self.inp_idx['entity']):\n",
    "            if self.start_idx['entity'][i] > self.end_idx['entity'][i]:\n",
    "                print(self.inp_idx['entity'][i],self.start_idx['entity'][i],self.end_idx['entity'][i])\n",
    "        self.num_class = len(self.year2id.keys())\n",
    "        \n",
    "        # for dtype in ['entity','triple']:\n",
    "        #   self.labels[dtype] = self.getOneHot(self.start_idx[dtype],self.end_idx[dtype], self.num_class)# Representing labels by one hot notation\n",
    "\n",
    "        keep_idx = set(self.inp_idx['triple'])\n",
    "        for i in range (len(train_triples)-1,-1,-1):\n",
    "            if i not in keep_idx:\n",
    "                del train_triples[i]\n",
    "\n",
    "        with open(self.p.relation2id, 'r') as filein3:\n",
    "            for line in filein3:\n",
    "                max_rel = max_rel +1\n",
    "        index = randint(1,len(train_triples))-1\n",
    "        \n",
    "        posh, rela, post = zip(*train_triples)\n",
    "        head, rel, tail = zip(*train_triples)\n",
    "\n",
    "        posh = list(posh) \n",
    "        post = list(post)\n",
    "        rela = list(rela)\n",
    "\n",
    "        head  =  list(head) \n",
    "        tail  =  list(tail)\n",
    "        rel   =  list(rel)\n",
    "\n",
    "        for i in range(len(posh)):\n",
    "            if self.start_idx['triple'][i] < self.end_idx['triple'][i]:\n",
    "                for j in range(self.start_idx['triple'][i] + 1,self.end_idx['triple'][i] + 1):\n",
    "                    head.append(posh[i])\n",
    "                    rel.append(rela[i])\n",
    "                    tail.append(post[i])\n",
    "                    self.start_idx['triple'].append(j)\n",
    "\n",
    "        self.ph, self.pt, self.r,self.nh, self.nt , self.triple_time  = [], [], [], [], [], []\n",
    "        for triple in range(len(head)):\n",
    "            neg_set = set()\n",
    "            for k in range(self.p.M):\n",
    "                possible_head = randint(0,max_ent-1)\n",
    "                while (possible_head, rel[triple], tail[triple]) in triple_set or (possible_head, rel[triple],tail[triple]) in neg_set:\n",
    "                    possible_head = randint(0,max_ent-1)\n",
    "                self.nh.append(possible_head)\n",
    "                self.nt.append(tail[triple])\n",
    "                self.r.append(rel[triple])\n",
    "                self.ph.append(head[triple])\n",
    "                self.pt.append(tail[triple])\n",
    "                self.triple_time.append(self.start_idx['triple'][triple])\n",
    "                neg_set.add((possible_head, rel[triple],tail[triple]))\n",
    "        \n",
    "        for triple in range(len(tail)):\n",
    "            neg_set = set()\n",
    "            for k in range(self.p.M):\n",
    "                possible_tail = randint(0,max_ent-1)\n",
    "                while (head[triple], rel[triple],possible_tail) in triple_set or (head[triple], rel[triple],possible_tail) in neg_set:\n",
    "                    possible_tail = randint(0,max_ent-1)\n",
    "                self.nh.append(head[triple])\n",
    "                self.nt.append(possible_tail)\n",
    "                self.r.append(rel[triple])\n",
    "                self.ph.append(head[triple])\n",
    "                self.pt.append(tail[triple])\n",
    "                self.triple_time.append(self.start_idx['triple'][triple])\n",
    "                neg_set.add((head[triple], rel[triple],possible_tail))\n",
    "\n",
    "        # self.triple_time = triple_time\n",
    "        # self.entity_time = entity_time\n",
    "        self.max_rel = max_rel\n",
    "        self.max_ent = max_ent\n",
    "        self.max_time = len(self.year2id.keys())\n",
    "        self.data = list(zip(self.ph, self.pt, self.r , self.nh, self.nt, self.triple_time))\n",
    "        self.data = self.data + self.data[0:self.p.batch_size]\n",
    "        train_data = self.read_valid(self.p.dataset)\n",
    "        valid_data_tosend=self.read_valid(self.p.dataset)\n",
    "        test_data_tosend=self.read_valid(self.p.test_data);\n",
    "        \n",
    "        return train_data,valid_data_tosend,test_data_tosend\n",
    "\n",
    "    def load_model(self, sess, save_path=None):\n",
    "        print(\"loaded1.................\")\n",
    "        saver = tf.train.Saver(max_to_keep=None)\n",
    "        if save_path is None:\n",
    "            save_dir = 'checkpoints/' + self.p.model + '/'\n",
    "            save_path = os.path.join(save_dir, 'epoch_{}'.format(self.p.restore_epoch))\n",
    "            # save_path = os.path.join(save_dir)\n",
    "            # , 'epoch_{}'.format(self.p.restore_epoch))\n",
    "        print(\"loaded.................\")\n",
    "        saver.restore(sess, save_path)\n",
    "        print(\"loaded2.................\")\n",
    "\n",
    "\n",
    "    def save_model(self,epoch,save_path=None,sess=None):\n",
    "        saver = tf.train.Saver(max_to_keep=None)\n",
    "        if save_path is None:\n",
    "            save_dir = 'checkpoints/' + self.p.model + '/'\n",
    "            save_path = os.path.join(save_dir, 'epoch_{}'.format(epoch))   ## -- check pointing -- ##\n",
    "        saver.save(sess=sess, save_path=save_path)\n",
    "        \n",
    "    # save_dir = 'checkpoints/' + self.p.name + '/'\n",
    "\n",
    "    def evaluate(self,sess,validation_data=None):\n",
    "        print('start predicting over test')\n",
    "\n",
    "        if validation_data is None:\n",
    "            validation_data = self.read_valid(self.p.test_data)\n",
    "        else:\n",
    "            validation_data = self.read_valid(validation_data)\n",
    "        \n",
    "        \n",
    "            ## feed_dict = self.create_feed_dict()\n",
    "            ## l, a = sess.run([self.loss, self.train_op],feed_dict = feed_dict)\n",
    "            # l = self.run_epoch(sess,self.data,epoch)\n",
    "            #print(l)\n",
    "            # if epoch%50 == 0:\n",
    "            #   print('Epoch {}\\tLoss {}\\t model {}'.format(epoch,l,self.p.name))\n",
    "            \n",
    "            # if epoch % self.p.test_freq == 0 and epoch != 0:\n",
    "            #   save_path = os.path.join(save_dir, 'epoch_{}'.format(epoch))   ## -- check pointing -- ##\n",
    "            #   saver.save(sess=sess, save_path=save_path)\n",
    "                # if epoch == self.p.test_freq:\n",
    "        save_dir_results = './results/'+ self.p.model + '/'\n",
    "\n",
    "\n",
    "        if(self.p.restore):\n",
    "            f_valid  = open(save_dir_results  +'/valid.txt','w')\n",
    "            fileout_head = open(save_dir_results +'/valid_head_pred_test.txt','w')\n",
    "            fileout_tail = open(save_dir_results +'/valid_tail_pred_test.txt','w')\n",
    "            fileout_rel  = open(save_dir_results +'/valid_rel_pred_test.txt', 'w')\n",
    "            for i,t in enumerate(validation_data):\n",
    "                loss =np.zeros(self.max_ent)\n",
    "                start_trip  = t[3][0].split('-')[0]\n",
    "                end_trip    = t[3][1].split('-')[0]\n",
    "                if start_trip == '####':\n",
    "                    start_trip = YEARMIN\n",
    "                elif start_trip.find('#') != -1 or len(start_trip)!=4:\n",
    "                    continue\n",
    "\n",
    "                if end_trip == '####':\n",
    "                    end_trip = YEARMAX\n",
    "                elif end_trip.find('#')!= -1 or len(end_trip)!=4:\n",
    "                    continue\n",
    "                            \n",
    "                start_lbl, end_lbl = self.get_span_ids(start_trip, end_trip)\n",
    "                        # if epoch == self.p.test_freq:\n",
    "                f_valid.write(str(t[0])+'\\t'+str(t[1])+'\\t'+str(t[2])+'\\n')\n",
    "                pos_head = sess.run(self.pos ,feed_dict = { self.pos_head:      np.array([t[0]]).reshape(-1,1), \n",
    "                                                                    self.rel:           np.array([t[1]]).reshape(-1,1), \n",
    "                                                                    self.pos_tail:  np.array([t[2]]).reshape(-1,1),\n",
    "                                                                    self.start_year :np.array([start_lbl]*self.max_ent),\n",
    "                                                                    self.end_year : np.array([end_lbl]*self.max_ent),\n",
    "                                                                    self.mode:             -1,\n",
    "                                                                    self.pred_mode: 1,\n",
    "                                                                    self.query_mode: 1})\n",
    "                pos_head = np.squeeze(pos_head)\n",
    "                        \n",
    "                pos_tail = sess.run(self.pos ,feed_dict = {    self.pos_head:   np.array([t[0]]).reshape(-1,1), \n",
    "                                                                       self.rel:        np.array([t[1]]).reshape(-1,1), \n",
    "                                                                       self.pos_tail:   np.array([t[2]]).reshape(-1,1),\n",
    "                                                                       self.start_year :np.array([start_lbl]*self.max_ent),\n",
    "                                                                       self.end_year : np.array([end_lbl]*self.max_ent),\n",
    "                                                                       self.mode:              -1, \n",
    "                                                                       self.pred_mode:  -1,\n",
    "                                                                       self.query_mode:  1})\n",
    "                pos_tail = np.squeeze(pos_tail)\n",
    "\n",
    "\n",
    "                pos_rel = sess.run(self.pos ,feed_dict = {    self.pos_head:    np.array([t[0]]).reshape(-1,1), \n",
    "                                                                       self.rel:        np.array([t[1]]).reshape(-1,1), \n",
    "                                                                       self.pos_tail:   np.array([t[2]]).reshape(-1,1),\n",
    "                                                                       self.start_year :np.array([start_lbl]*self.max_rel),\n",
    "                                                                       self.end_year : np.array([end_lbl]*self.max_rel),\n",
    "                                                                       self.mode:              -1, \n",
    "                                                                       self.pred_mode: -1,\n",
    "                                                                       self.query_mode: -1})\n",
    "                pos_rel = np.squeeze(pos_rel)\n",
    "                fileout_head.write(' '.join([str(x) for x in pos_head]) + '\\n')\n",
    "                fileout_tail.write(' '.join([str(x) for x in pos_tail]) + '\\n')\n",
    "                fileout_rel.write (' '.join([str(x) for x in pos_rel] ) + '\\n')\n",
    "                        \n",
    "                if i%500 == 0:\n",
    "                    print('{}. no of valid_triples complete'.format(i))\n",
    "                            # if i%4000 == 0 and i!=0: break\n",
    "            fileout_head.close()\n",
    "            fileout_tail.close()\n",
    "            fileout_rel.close()\n",
    "                    # if epoch ==self.p.test_freq:\n",
    "            f_valid.close()\n",
    "\n",
    "        valid_output = open('results/'+self.p.model+'/valid.txt')\n",
    "\n",
    "        if(self.p.restore):\n",
    "            model_output_head = open('results/'+self.p.model+'/valid_head_pred_test.txt')\n",
    "            model_output_tail = open('results/'+self.p.model+'/valid_tail_pred_test.txt')\n",
    "\n",
    "        else:\n",
    "            model_output_head = open('results/'+args.model+'/valid_head_pred_20.txt')\n",
    "            model_output_tail = open('results/'+args.model+'/valid_tail_pred_20.txt')\n",
    "            \n",
    "        \n",
    "\n",
    "        model_out_head = []\n",
    "        model_out_tail = []\n",
    "        count = 0\n",
    "        for line in model_output_head:\n",
    "            count = 0\n",
    "            temp_out = []\n",
    "            for ele in line.split():\n",
    "                tup  = (float(ele),count)\n",
    "                temp_out.append(tup)\n",
    "                    # print(\"temp_out\"+ temp_out);\n",
    "                count = count+1\n",
    "            model_out_head.append(temp_out)\n",
    "                # print(\"model_out_head\"+ model_out_head);\n",
    "\n",
    "        for line in model_output_tail:\n",
    "            count = 0\n",
    "            temp_out = []\n",
    "            for ele in line.split():\n",
    "                tup  = (float(ele),count)\n",
    "                temp_out.append(tup)\n",
    "                count = count+1\n",
    "            model_out_tail.append(temp_out)\n",
    "                # print(\"model_out_tail\"+ model_out_tail);\n",
    "\n",
    "            \n",
    "        for row in model_out_head:\n",
    "            row.sort(key=lambda x:x[0])\n",
    "\n",
    "        for row in model_out_tail:\n",
    "            row.sort(key=lambda x:x[0])\n",
    "            \n",
    "        final_out_head , final_out_tail= [], []\n",
    "        for row in model_out_head:\n",
    "            temp_dict =dict()\n",
    "            count = 0\n",
    "            for ele in row:\n",
    "                temp_dict[ele[1]] = count\n",
    "                count += 1\n",
    "            final_out_head.append(temp_dict)\n",
    "\n",
    "        for row in model_out_tail:\n",
    "            temp_dict =dict()\n",
    "            count = 0\n",
    "            for ele in row:\n",
    "                temp_dict[ele[1]] = count\n",
    "                count += 1\n",
    "            final_out_tail.append(temp_dict)\n",
    "            \n",
    "        ranks_head = []\n",
    "        ranks_tail = []\n",
    "            # pdb.set_trace()\n",
    "        for i,row in enumerate(valid_output):\n",
    "            ranks_head.append(final_out_head[i][int(row.split()[0])])\n",
    "            ranks_tail.append(final_out_tail[i][int(row.split()[2])])\n",
    "        print('test_tail rank {}\\t test_head rank {}'.format(np.mean(np.array(ranks_tail))+1, np.mean(np.array(ranks_head))+1))\n",
    "\n",
    "        tail_array = np.array(ranks_tail)\n",
    "        head_array = np.array(ranks_head)\n",
    "\n",
    "        hit_at_10_tail = tail_array[np.where( tail_array < 10 ) ]\n",
    "        hit_at_10_head = head_array[np.where( head_array < 10 ) ]\n",
    "\n",
    "        print('test_tail HIT@10 {}\\t test_head HIT@10) {}'.format(len(hit_at_10_tail)/float(len(tail_array))*100, len(hit_at_10_head)/float(len(head_array))*100))\n",
    "\n",
    "        final_metrics = {\n",
    "        \"test_tail rank\":np.mean(np.array(ranks_tail))+1,\n",
    "        \"test_head rank\":np.mean(np.array(ranks_head))+1,\n",
    "        \"test_tail HIT@10\":len(hit_at_10_tail)/float(len(tail_array))*100,\n",
    "        \"test_head HIT@10\":len(hit_at_10_head)/float(len(head_array))*100\n",
    "        }\n",
    "\n",
    "        return final_metrics;\n",
    "\n",
    "    def learn_embeddings(self, sess, validation_data=None):\n",
    "        #self.best_val_acc, self.best_train_acc = 0.0, 0.0\n",
    "        \n",
    "        save_dir = 'checkpoints/' + self.p.model + '/'\n",
    "        if not os.path.exists(save_dir): os.makedirs(save_dir)\n",
    "        save_dir_results = './results/'+ self.p.model + '/'\n",
    "        if not os.path.exists(save_dir_results): os.makedirs(save_dir_results)\n",
    "        # if self.p.restore:\n",
    "        #   save_path = os.path.join(save_dir, 'epoch_{}'.format(self.p.restore_epoch))\n",
    "        #   # save_path = os.path.join(save_dir)\n",
    "        #   # , 'epoch_{}'.format(self.p.restore_epoch))\n",
    "        #   saver.restore(sess, save_path)\n",
    "        \n",
    "        print('start fitting')\n",
    "\n",
    "        if validation_data is None:\n",
    "            validation_data = self.read_valid(self.p.dataset)\n",
    "        else:\n",
    "            validation_data = self.read_valid(validation_data)\n",
    "        \n",
    "        for epoch in range(self.p.max_epochs):\n",
    "            # feed_dict = self.create_feed_dict()\n",
    "            # l, a = sess.run([self.loss, self.train_op],feed_dict = feed_dict)\n",
    "            l = self.run_epoch(sess,self.data,epoch)\n",
    "            #print(l)\n",
    "            if epoch%5 == 0:\n",
    "                print('Epoch {}\\tLoss {}\\t model {}'.format(epoch,l,self.p.model))\n",
    "            \n",
    "            if epoch % self.p.test_freq == 0 and epoch != 0:\n",
    "                self.save_model(epoch=epoch, sess=sess)\n",
    "                # save_path = os.path.join(save_dir, 'epoch_{}'.format(epoch))   ## -- check pointing -- ##\n",
    "                # saver.save(sess=sess, save_path=save_path)\n",
    "                if epoch == self.p.test_freq:\n",
    "                    f_valid  = open(save_dir_results  +'/valid.txt','w')\n",
    "                \n",
    "                fileout_head = open(save_dir_results +'/valid_head_pred_{}.txt'.format(epoch),'w')\n",
    "                fileout_tail = open(save_dir_results +'/valid_tail_pred_{}.txt'.format(epoch),'w')\n",
    "                fileout_rel  = open(save_dir_results +'/valid_rel_pred_{}.txt'.format(epoch), 'w')\n",
    "                for i,t in enumerate(validation_data):\n",
    "                    loss =np.zeros(self.max_ent)\n",
    "                    start_trip  = t[3][0].split('-')[0]\n",
    "                    end_trip    = t[3][1].split('-')[0]\n",
    "                    if start_trip == '####':\n",
    "                        start_trip = YEARMIN\n",
    "                    elif start_trip.find('#') != -1 or len(start_trip)!=4:\n",
    "                        continue\n",
    "\n",
    "                    if end_trip == '####':\n",
    "                        end_trip = YEARMAX\n",
    "                    elif end_trip.find('#')!= -1 or len(end_trip)!=4:\n",
    "                        continue\n",
    "                        \n",
    "                    start_lbl, end_lbl = self.get_span_ids(start_trip, end_trip)\n",
    "                    if epoch == self.p.test_freq:\n",
    "                        f_valid.write(str(t[0])+'\\t'+str(t[1])+'\\t'+str(t[2])+'\\n')\n",
    "                    pos_head = sess.run(self.pos ,feed_dict = { self.pos_head:      np.array([t[0]]).reshape(-1,1), \n",
    "                                                                self.rel:           np.array([t[1]]).reshape(-1,1), \n",
    "                                                                self.pos_tail:  np.array([t[2]]).reshape(-1,1),\n",
    "                                                                self.start_year :np.array([start_lbl]*self.max_ent),\n",
    "                                                                self.end_year : np.array([end_lbl]*self.max_ent),\n",
    "                                                                self.mode:             -1,\n",
    "                                                                self.pred_mode: 1,\n",
    "                                                                self.query_mode: 1})\n",
    "                    pos_head = np.squeeze(pos_head)\n",
    "                    \n",
    "                    pos_tail = sess.run(self.pos ,feed_dict = {    self.pos_head:   np.array([t[0]]).reshape(-1,1), \n",
    "                                                                   self.rel:        np.array([t[1]]).reshape(-1,1), \n",
    "                                                                   self.pos_tail:   np.array([t[2]]).reshape(-1,1),\n",
    "                                                                   self.start_year :np.array([start_lbl]*self.max_ent),\n",
    "                                                                   self.end_year : np.array([end_lbl]*self.max_ent),\n",
    "                                                                   self.mode:              -1, \n",
    "                                                                   self.pred_mode:  -1,\n",
    "                                                                   self.query_mode:  1})\n",
    "                    pos_tail = np.squeeze(pos_tail)\n",
    "\n",
    "\n",
    "                    pos_rel = sess.run(self.pos ,feed_dict = {    self.pos_head:    np.array([t[0]]).reshape(-1,1), \n",
    "                                                                   self.rel:        np.array([t[1]]).reshape(-1,1), \n",
    "                                                                   self.pos_tail:   np.array([t[2]]).reshape(-1,1),\n",
    "                                                                   self.start_year :np.array([start_lbl]*self.max_rel),\n",
    "                                                                   self.end_year : np.array([end_lbl]*self.max_rel),\n",
    "                                                                   self.mode:              -1, \n",
    "                                                                   self.pred_mode: -1,\n",
    "                                                                   self.query_mode: -1})\n",
    "                    pos_rel = np.squeeze(pos_rel)\n",
    "                    fileout_head.write(' '.join([str(x) for x in pos_head]) + '\\n')\n",
    "                    fileout_tail.write(' '.join([str(x) for x in pos_tail]) + '\\n')\n",
    "                    fileout_rel.write (' '.join([str(x) for x in pos_rel] ) + '\\n')\n",
    "                    \n",
    "                    if i%500 == 0:\n",
    "                        print('{}. no of valid_triples complete'.format(i))\n",
    "                        # if i%4000 == 0 and i!=0: break\n",
    "                fileout_head.close()\n",
    "                fileout_tail.close()\n",
    "                fileout_rel.close()\n",
    "                if epoch ==self.p.test_freq:\n",
    "                    f_valid.close()\n",
    "                print(\"Validation Ended\")\n",
    "\n",
    "class GraphEmbeddingChild(GraphEmbedding):\n",
    "    def __init__(self, params):\n",
    "        self.graphEmbeddingFromModel = GraphEmbeddingFromModel(params)\n",
    "        self.config = tf.ConfigProto()\n",
    "        self.config.gpu_options.allow_growth=True\n",
    "        self.sess = None\n",
    "\n",
    "    def set_session(self, sess):\n",
    "        self.sess = sess\n",
    "\n",
    "    def read_dataset(self, file_names=None, *args, **kwargs):  #<--- implemented PER class\n",
    "        \"\"\"\n",
    "        Reads datasets and convert them to proper format for train or test. Returns data in proper format for train,\n",
    "        validation and test.\n",
    "        Args:\n",
    "            file_names: list-like. List of files representing the dataset to read. Each element is str, representing\n",
    "                filename [possibly with filepath]\n",
    "                options: object to store any extra or implementation specific data\n",
    "        Returns:\n",
    "            data: data in proper [arbitrary] format for train, validation and test.\n",
    "        Raises:\n",
    "            None\n",
    "        \"\"\"\n",
    "        return self.graphEmbeddingFromModel.read_dataset(file_names)\n",
    "\n",
    "    def learn_embeddings(self, data, *args, **kwargs):  #<--- implemented PER class\n",
    "        \"\"\"\n",
    "        Learns embeddings with data, build model and train the model\n",
    "        Args:\n",
    "            data: iterable of arbitrary format. represents the data instances and features and labels you\n",
    "            need to train your model.\n",
    "            Note: formal subject to OPEN ITEM mentioned in read_dataset!\n",
    "            options: object to store any extra or implementation specific data\n",
    "        Returns:\n",
    "            ret: None. Trained model stored internally to class instance state.\n",
    "        Raises:\n",
    "            None\n",
    "        \"\"\"\n",
    "        self.graphEmbeddingFromModel.learn_embeddings(self.sess, validation_data=data)\n",
    "\n",
    "\n",
    "    def evaluate(self, data, *args, **kwargs):  #<--- common ACROSS ALL classes. Requirement that INPUT format uses output from predict()!\n",
    "        \"\"\"\n",
    "        Predicts the embeddings with test data and calculates evaluation metrics on chosen benchmark dataset\n",
    "        Args:\n",
    "            data: data used to test the model, may need further process\n",
    "            options: object to store any extra or implementation specific data\n",
    "        Returns:\n",
    "            metrics: cosine similarity, MMR or Hits\n",
    "        Raises:\n",
    "            None\n",
    "        \"\"\"\n",
    "        return self.graphEmbeddingFromModel.evaluate(self.sess, validation_data=data)\n",
    "\n",
    "\n",
    "    def save_model(self, file):\n",
    "        \"\"\"\n",
    "        :param file: Where to save the model - Optional function\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        with tf.Session(config=self.config) as sess:\n",
    "            # Note epoch is a useless parameter when passed here\n",
    "            self.graphEmbeddingFromModel.save_model(epoch=0, save_path=file, sess=sess)\n",
    "\n",
    "    def load_model(self, file):\n",
    "        \"\"\"\n",
    "        :param file: From where to load the model - Optional function\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.graphEmbeddingFromModel.load_model(sess=self.sess, save_path=file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intializes parameters to restore pre-trained model for 75 epochs of 'hyte_2_08_04_2019_14:42:12' model with a margin of 10 on YAGO dataset and sets up to test on the file testcase.txt\n",
    "\n",
    "## Creates object of GraphEmbedding class\n",
    "\n",
    "## Loads model from a previous checkpoint and calls the evaluate function to predict and evaluate on testcase.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(input_file_path):\n",
    "    print('here in main')\n",
    "    parser = argparse.ArgumentParser(description='HyTE')\n",
    "    parser.add_argument('-data_type', dest= \"data_type\", default ='yago', choices = ['yago','wiki_data'], help ='dataset to choose')\n",
    "    parser.add_argument('-version',dest = 'version', default = 'large', choices = ['large','small'], help = 'data version to choose')\n",
    "    parser.add_argument('-test_freq',    dest=\"test_freq\",  default = 25,       type=int,   help='Batch size')\n",
    "    parser.add_argument('-neg_sample',   dest=\"M\",      default = 5,    type=int,   help='Batch size')\n",
    "    parser.add_argument('-gpu',      dest=\"gpu\",        default='1',            help='GPU to use')\n",
    "    parser.add_argument('-model',    dest=\"model\",      default='hyte_2_08_04_2019_14:42:12',help='Name of the run')\n",
    "    parser.add_argument('-drop',     dest=\"dropout\",    default=1.0,    type=float, help='Dropout for full connected layer')\n",
    "    parser.add_argument('-rdrop',    dest=\"rec_dropout\",    default=1.0,    type=float, help='Recurrent dropout for LSTM')\n",
    "    parser.add_argument('-lr',   dest=\"lr\",         default=0.0001,  type=float,    help='Learning rate')\n",
    "    parser.add_argument('-lam_1',    dest=\"lambda_1\",       default=0.5,  type=float,   help='transE weight')\n",
    "    parser.add_argument('-lam_2',    dest=\"lambda_2\",       default=0.25,  type=float,  help='entitty loss weight')\n",
    "    parser.add_argument('-margin',   dest=\"margin\",     default=10,     type=float,     help='margin')\n",
    "    parser.add_argument('-batch',    dest=\"batch_size\",     default= 50000,     type=int,   help='Batch size')\n",
    "    parser.add_argument('-epoch',    dest=\"max_epochs\",     default= 25,    type=int,   help='Max epochs')\n",
    "    parser.add_argument('-l2',   dest=\"l2\",         default=0.0,    type=float,     help='L2 regularization')\n",
    "    parser.add_argument('-seed',     dest=\"seed\",       default=1234,   type=int,   help='Seed for randomization')\n",
    "    parser.add_argument('-inp_dim',  dest=\"inp_dim\",    default = 128,      type=int,   help='Hidden state dimension of Bi-LSTM')\n",
    "    parser.add_argument('-L1_flag',  dest=\"L1_flag\",    action='store_false',           help='Hidden state dimension of FC layer')\n",
    "    parser.add_argument('-onlytransE', dest=\"onlytransE\",   action='store_true',        help='Evaluate model on only transE loss')\n",
    "    parser.add_argument('-restore',  dest=\"restore\",    action='store_false',       help='Restore from the previous best saved model')\n",
    "    parser.add_argument('-res_epoch',        dest=\"restore_epoch\",  default=75,   type =int,        help='Restore from the previous best saved model')\n",
    "    args = parser.parse_args()\n",
    "    args.dataset = 'data/'+ args.data_type +'/'+ args.version+'/train.txt'\n",
    "    args.entity2id = 'data/'+ args.data_type +'/'+ args.version+'/entity2id.txt'\n",
    "    args.relation2id = 'data/'+ args.data_type +'/'+ args.version+'/relation2id.txt'\n",
    "    args.test_data  =  input_file_path # 'data/'+ args.data_type +'/'+ args.version+'/testcase.txt'\n",
    "    args.triple2id  =   'data/'+ args.data_type +'/'+ args.version+'/triple2id.txt'\n",
    "        # if not args.restore: args.name = args.model + '_' + time.strftime(\"%d_%m_%Y\") + '_' + time.strftime(\"%H:%M:%S\")\n",
    "    if not args.restore: args.name = args.model\n",
    "    tf.set_random_seed(args.seed)\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    set_gpu(args.gpu)\n",
    "    model  = GraphEmbeddingChild(args)\n",
    "    print('model object created')\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth=True\n",
    "    train__data,validation__data,test__data = model.read_dataset();\n",
    "    # print(train__data);\n",
    "    print(\"-------------------------------------\")\n",
    "    if(args.restore):\n",
    "        with tf.Session(config=config) as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            model.set_session(sess)\n",
    "            save_dir = 'checkpoints/' + args.model + '/'\n",
    "            model.load_model(os.path.join(save_dir, 'epoch_{}'.format(args.restore_epoch)))\n",
    "            eval_metrics = model.evaluate(args.test_data)\n",
    "        print(eval_metrics)    \n",
    "\n",
    "    else:\n",
    "        with tf.Session(config=config) as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            model.set_session(sess)\n",
    "            model.learn_embeddings(args.dataset)\n",
    "            eval_metrics = model.evaluate(args.test_data)\n",
    "        print(eval_metrics)\n",
    "    return 'valid_head_pred_test.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output embedings generated in valid_head_pred_test.txt,valid_tail_pred_test.txt and valid_rel_pred_test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation metrics -MR(lower the better) and Hit@10(higher the better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here in main\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-2-1e79a390f801>:377: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From <ipython-input-2-1e79a390f801>:400: calling l2_normalize (from tensorflow.python.ops.nn_impl) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n",
      "model done\n",
      "model object created\n",
      "-------------------------------------\n",
      "loaded1.................\n",
      "loaded.................\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/hyte_2_08_04_2019_14:42:12/epoch_75\n",
      "loaded2.................\n",
      "start predicting over test\n",
      "test_tail rank 3.769230769230769\t test_head rank 12.0\n",
      "test_tail HIT@10 92.3076923076923\t test_head HIT@10) 92.3076923076923\n",
      "{'test_tail rank': 3.769230769230769, 'test_head rank': 12.0, 'test_tail HIT@10': 92.3076923076923, 'test_head HIT@10': 92.3076923076923}\n"
     ]
    }
   ],
   "source": [
    "if __name__== \"__main__\":\n",
    "    sys.argv=['']; \n",
    "    del sys\n",
    "    main('data/yago/large/testcase.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
