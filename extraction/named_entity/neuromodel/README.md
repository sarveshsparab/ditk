# **NeuroNER_BIO**

##**Some explanations**
In my notebook file, I didn't go through the whole experiment process by using my own module because for one thing, the original NeuroNER code is very complex and twisted, so I didn't aggregrate the original functions into my reading, training, predicting, evaluating functions. 

Actually the original repository is very unreliable and contains many bugs, it is also renewing since first created. So I have already spent much time on debugging and experimenting. It's really not easy to run the whole system successfully and get the experiment results. With the final exam approaching, I can't afford the time spent on debugging and testing on my own module class.

For another thing, the original project has already created a class named `NeuroNER`, which the author has already aggregrated the loading, fitting, predicting functions, so I think this can also meet the requirement, to some extent.

So I write elaborative description about how to run the system by command line and invoking the original main() function, hopefully this works, too.

##**Paper Title**
Transfer Learning for Biomedical Named Entity Recognition with Neural Networks

##**Full Citation**
Bioinformatics, Volume 34, Issue 23, 1 December 2018, Pages 4087–4094 

##**Original Code**
data cleaning and corpora processing: 
https://github.com/BaderLab/Transfer-Learning-BNER-Bioinformatics-2018/
    
NeuroNER the model:
https://github.com/Franck-Dernoncourt/NeuroNER/

##**Description**
1. The paper's task is to do Named Entity Recognition on Biomedical corpora.
2. The model/tool to be used is called NeuroNER, the neural net architecture is: CNNs + BLSTM (char embedding + word embedding) + BLSTM + Fully Connected NN & CRF (do the prediction, input sentence sequence -> predicted labels).
You can see the detailed structure figure below.
3. This NN method is dependent on gold-standard corpora (GSCs) consisting of hand-labeled entities, which tend to be small but highly reliable. An alternative to GSCs are silver-standard corpora (SSCs), 
which are generated by harmonizing the annotations made by several automatic annotation systems
4. SSCs typically contain more noise than GSCs but have the advantage of containing many more training examples. Ideally, these corpora could be combined to achieve the benefits of both, which is an opportunity for transfer learning. 
5. So the whole step will be: we firstly trained on SSC corpora to get a pre-trained model, then based on this trained model to train on GSC (transfer learning parts). The results will be 
compared with that when we only train on the GSC.
6. The inspiration is that SSCs typically contain more noise than GSCs but have the advantage of containing many more training examples. Ideally, these corpora could be combined to achieve the benefits of both, which is an opportunity for transfer learning. 
![avatar](./neuromodel.png)

##**Input and Output**
For training and testing, corpora must be in the BRAT-Standoff annotation:

Each ’data pair’ contains a .txt file and a .ann file in brat-standoff format.

All the data pairs will be aggregated into one single conll-2003 format file to fit the model later

For example, as one 'data pair', the text file would be:

    Studies on T and B lymphocytes in the peripheral blood of discoid lupus erythematosus patients with and without chloroquine treatment. The proportions of T-B cells of peripheral lymphocytes in DLE patients as well as the effect of chloroquine treatment on T cell count were studied. In DLE, both the active and the total rosette-forming cell counts decreased, whereas the B cell count did not differ from normal values. Long-term chloroquine administration causes a further reduction in T cell count. 

While the annotation file would be:

    T1	CHED 112 123	chloroquine
    T2	CHED 231 242	chloroquine
    T3	CHED 430 441	chloroquine  

Testing or Predicting output is a text file in BRAT-Standoff format which aggregates all the testing files in the test/deploy folder,
the format in each row is (current word, text file name, start position, end position, ground label, predicted label)

One example is as follows: (this is an example not tested on biomedical corpora)

    This some_random_text 0 4 O O
    text some_random_text 5 9 O O
    was some_random_text 10 13 O O
    written some_random_text 14 21 O O
    in some_random_text 22 24 O O
    the some_random_text 25 28 O O
    United some_random_text 29 35 O B-LOC
    States some_random_text 36 42 O I-LOC
    . some_random_text 42 43 O O


##**Evaluation**
The SSC to be used is CALBC-|||-SMALL, the bio types we experimented on is CHED, DISO, LIVB

Benchmark datasets(GSCs):

    CDR_CHED
    CDR_DISO
    LINNAEUS
We also experimented on two general NER benchmark datasets using NeuroNER tool:

    Conll2003
    CHEMDNER
    
Evaluation metrics and results on BIO corpora with transfer learning:

|             |  Prediction   | | Recall    | |  F1-score | |    
| ---------- | :-----------:  | :-----------: |:-----------: |:-----------: |:-----------: |:-----------: |
|             | Baseline|Transfer|Baseline|Transfer|Baseline|Transfer||
| CDR_CHED     | 0.8838     | 0.8576     |0.8566     |0.8650     |0.8700 | 0.8613|
| CDR_DISO     | 0.7993     | 0.7879     |0.7854     |0.8051     |0.7923 | 0.7964|
| LINNAEUS     | 0.9469     | 0.9455     |0.8355     |0.8532     |0.8877 | 0.8972|

Evaluation metrics and results on general benchmarks without transfer learning:

|             |  Prediction    | Recall     |  F1-score |    
| ---------- | :-----------:  | :-----------: |:-----------: |
| Conll2003     | 0.8959     | 0.9079     |0.9019     |
| CHEMDNER      | 0.7891     | 0.7413     |0.7644     |


##**Demo**
Link to the Jupyter Notebook:
[Jupyter](extraction/named_entity/neuromodel/Whole_process_NeuroNER.ipynb)

Link to the video on Youtube:
[youtube](https://youtu.be/u2bYXC3Er2U)

##**Implementation Steps**
In order to implement the experiment, the following steps will be a good reference.
1. Please download embedding file of [Glove](https://nlp.stanford.edu/projects/glove/), 
which gives us the word embedding and character embedding vectors. Since NeuroNER requires the
embedding vector to be 100-dimensinal, we used the `glove.6B.100d.txt` embedding file. 
Then, the embedding file should be put under the `./data/word_vectors` directory.
2. After setting the embedding file, then we need to set the training, validation and testing file. 
Firstly, in order to experiment on the Biomedical corpora with transfer learning, you need to download
the corpora via the paper'author [github repository](https://github.com/BaderLab/Transfer-Learning-BNER-Bioinformatics-2018/), 
unzip the SSC and GSC corpora you want to experiment on, and put the file folders under the `./data/` directoty, there
are three different processing way and three different file requirements.

        for training without a pretrained model, your folder must contain a train and a valid folder (or a train.txt file and a valid.txt file)
        for training with a pretrained model, your folder must contain a train and a valid folder (or a train.txt file and a valid.txt file)
        for testing or predicting, your folder must contain a test folder (or a test.txt file)
    
3. By which processing way is determined by configurations in `./parameters.ini` file. There, you need to set `dataset_text_folder` and `token_pretrained_embedding_filepath` pointing
to your data folder and embedding file. The you need to set `train_model`, `use_pretrained_model` and `pretrained_model_folder` as follows:

        for training without a pretrained model, you need to set 'train_model' to be True and 'use_pretrained_model' to be False.
        for training with a pretrained model, you need to set 'train_model' to be True and 'use_pretrained_model' to be Talse, and 'pretrained_model_folder' pointing to the trained model's folder
        for testing or predicting, you need to set 'train_model' to be False and 'use_pretrained_model' to be True, and 'pretrained_model_folder' pointing to the trained model's folder.

4. After setting the configuration as you need, you are ready to run the codes, there are two ways to run the system.

    First, you can run it from the `command line`, just open your command line tool, set the directory to the `neuroner` folder,
then run the command `neuroner`, you will see the system running. 
    Second, you can run it from a Python Interpreter, just open the `__main__.py` file and run it, you will see the system running.
    
5. The following two items are specific for doing the transfer learning, after training on the SSC, you will have a trained model saved under the 
`output` folder, in order to use this pre-trained model, you need to move this folder under the `trained_models` folder, then delete
unnecessary files but only keep five 'pickle', 'ckpt' of your last epoch record and 'parameter.ini' files, and rename them according to files in the
`./trained_models/conll_2003_en` folder.
6. So now you have your owned pre-trained model, and you can do the transfer learning on the GSC, just set the parameters.ini under the root directory,
the setting method are as above.


##**Dependencies**
    matplotlib==3.0.2
    networkx==2.2
    pycorenlp==0.3.0
    scikit-learn==0.20.2
    scipy==1.2.0
    spacy==2.0.18
    tensorflow==1.12.0
    numpy==1.16.0
    # tensorflow-gpu==1.1.0
    Spacy  python -m spacy download en
    Perl is required because the official CoNLL-2003 evaluation script is written in this language: http://strawberryperl.com. For Unix and Mac OSX systems, Perl should already be installed. For Windows systems, you may need to install it.

