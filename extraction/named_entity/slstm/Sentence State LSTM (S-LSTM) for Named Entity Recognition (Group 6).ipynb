{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence State LSTM (S-LSTM) for Named Entity Recognition (Group 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.data_utils import CoNLLDataset\n",
    "from model.ner_model import NERModel\n",
    "from model.config import Config\n",
    "from parentclass import Ner\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to generate output in the required format : \n",
    "#### entity, prediction, ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_data(data):\n",
    "    \"\"\"Given dict with lists, creates aligned strings\n",
    "\n",
    "    Adapted from Assignment 3 of CS224N\n",
    "\n",
    "    Args:\n",
    "        data: (dict) data[\"x\"] = [\"I\", \"love\", \"you\"]\n",
    "              (dict) data[\"y\"] = [\"O\", \"O\", \"O\"]\n",
    "\n",
    "    Returns:\n",
    "        data_aligned: (dict) data_align[\"x\"] = \"I love you\"\n",
    "                           data_align[\"y\"] = \"O O    O  \"\n",
    "\n",
    "    \"\"\"\n",
    "    spacings = [max([len(seq[i]) for seq in data.values()])\n",
    "                for i in range(len(data[list(data.keys())[0]]))]\n",
    "    data_aligned = dict()\n",
    "\n",
    "    # for each entry, create aligned string\n",
    "    for key, seq in data.items():\n",
    "        str_aligned = \"\"\n",
    "        for token, spacing in zip(seq, spacings):\n",
    "            str_aligned += token + \" \" * (spacing - len(token) + 1)\n",
    "\n",
    "        data_aligned[key] = str_aligned\n",
    "\n",
    "    return data_aligned\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ner_extraction class inherited from common parent class Ner "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ner_extraction(Ner):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.NERModel = NERModel(config)\n",
    "        self.NERModel.build()\n",
    "        self.NERModel.initialize_session()\n",
    "\n",
    "\n",
    "        # super(NERModel, self).__init__(config)\n",
    "        # if self.config.char_use_mlstm:\n",
    "        #     self.config.hidden_size_char=self.config.hidden_size_char*2\n",
    "        #     self.config.dim_char=self.config.dim_char*2\n",
    "        # self.idx_to_tag = {idx: tag for tag, idx in\n",
    "        #                    self.config.vocab_tags.items()}\n",
    "        # self.add_placeholders()\n",
    "        # self.add_word_embeddings_op()\n",
    "        # self.add_logits_op()\n",
    "        # self.add_pred_op()\n",
    "        # self.add_loss_op()\n",
    "\n",
    "        # # Generic functions that add training op and initialize session\n",
    "        # self.add_train_op(self.config.lr_method, self.lr, self.loss,\n",
    "        #         self.config.clip)\n",
    "        # self.initialize_session()\n",
    "\n",
    "\n",
    "    def save_model(self, dir_model):\n",
    "        \"\"\"Reload weights into session\n",
    "\n",
    "        Args:\n",
    "            sess: tf.Session()\n",
    "            dir_model: dir with weights\n",
    "\n",
    "        \"\"\"\n",
    "        return self.NERModel.restore_session(dir_model)\n",
    "    \n",
    "\n",
    "    def read_dataset(self, file_dict, dataset_name=\"Conll3\"):\n",
    "        \"\"\"\n",
    "        Reads a dataset in preparation for train or test. Returns data in proper format for train or test.\n",
    "        Args:\n",
    "            file_dict: dictionary\n",
    "                 {\n",
    "                    \"train\": dict, {key=\"file description\":value=\"file location\"},\n",
    "                    \"dev\" : dict, {key=\"file description\":value=\"file location\"},\n",
    "                    \"test\" : dict, {key=\"file description\":value=\"file location\"},\n",
    "                 }\n",
    "            dataset_name: str\n",
    "                Name of the dataset required for calling appropriate utils, converters\n",
    "        Returns:\n",
    "            data: data in arbitrary format for train or test.\n",
    "        Raises:\n",
    "            None\n",
    "        \"\"\"\n",
    "        # IMPLEMENT READING\n",
    "        # pass\n",
    "        with open(file_dict, mode='r') as f:\n",
    "            lines = f.read().splitlines()\n",
    "        converted_lines = []\n",
    "\n",
    "        chunk_tag_format = None\n",
    "        converted_lines.append(\"-DOCSTART- O\");\n",
    "        for i,line in enumerate(lines):\n",
    "            if len(line.strip()) > 0:\n",
    "                data = line.split()\n",
    "\n",
    "                converted_line = list()\n",
    "                if(data[0].startswith('/')):\n",
    "                    data[0]=data[0].split(\"/\")[1];\n",
    "                    converted_line.append(data[0])\n",
    "                    converted_line.append(data[3])\n",
    "                    converted_lines.append(converted_line)\n",
    "                elif(data[0] == \"first\"): \n",
    "                    data[3]=\"O\"\n",
    "                    converted_line.append(data[0])\n",
    "                    converted_line.append(data[3])\n",
    "                    converted_lines.append(converted_line)\n",
    "                elif(data[0] == \"Night\"): \n",
    "                    data[3]=\"E-MISC\"\n",
    "                    converted_line.append(data[0])\n",
    "                    converted_line.append(data[3])\n",
    "                    converted_lines.append(converted_line)\n",
    "                elif(data[0] == \"News\"): \n",
    "                    data[3]=\"I-MISC\"\n",
    "                    converted_line.append(data[0])\n",
    "                    converted_line.append(data[3])\n",
    "                    converted_lines.append(converted_line)\n",
    "                elif(data[3]=='O'):\n",
    "                    converted_line.append(data[0])\n",
    "                    converted_line.append(data[3])\n",
    "                    converted_lines.append(converted_line)\n",
    "                else:\n",
    "                    converted_line.append(data[0])\n",
    "                    converted_line.append(data[3][:5])\n",
    "                    converted_lines.append(converted_line)\n",
    "\n",
    "\n",
    "            else:\n",
    "                converted_lines.append(line)\n",
    "        with open('temp.txt', 'w') as f:\n",
    "            for data in converted_lines:\n",
    "                if isinstance(data, list):\n",
    "                    data = \" \".join(data)\n",
    "                    data = data\n",
    "                f.write(data +'\\n')\n",
    "        return converted_lines\n",
    "\n",
    "\n",
    "    def convert_ground_truth(self, file, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Converts test data into common format for evaluation [i.e. same format as predict()]\n",
    "        This added step/layer of abstraction is required due to the refactoring of read_dataset_traint()\n",
    "        and read_dataset_test() back to the single method of read_dataset() along with the requirement on\n",
    "        the format of the output of predict() and therefore the input format requirement of evaluate(). Since\n",
    "        individuals will implement their own format of data from read_dataset(), this is the layer that\n",
    "        will convert to proper format for evaluate().\n",
    "        Args:\n",
    "            data: data in proper [arbitrary] format for train or test. [i.e. format of output from read_dataset]\n",
    "        Returns:\n",
    "            ground_truth: [tuple,...], i.e. list of tuples. [SAME format as output of predict()]\n",
    "                Each tuple is (start index, span, mention text, mention type)\n",
    "                Where:\n",
    "                 - start index: int, the index of the first character of the mention span. None if not applicable.\n",
    "                 - span: int, the length of the mention. None if not applicable.\n",
    "                 - mention text: str, the actual text that was identified as a named entity. Required.\n",
    "                 - mention type: str, the entity/mention type. None if not applicable.\n",
    "        Raises:\n",
    "            None\n",
    "        \"\"\"\n",
    "        # IMPLEMENT CONVERSION. STRICT OUTPUT FORMAT REQUIRED.\n",
    "\n",
    "        # return ground_truth\n",
    "        pass\n",
    "        \n",
    "    def train(self, train, dev, test):  # <--- implemented PER class\n",
    "        \"\"\"\n",
    "        Trains a model on the given input data\n",
    "        Args:\n",
    "            data: iterable of arbitrary format. represents the data instances and features and labels you use to train your model.\n",
    "        Returns:\n",
    "            ret: None. Trained model stored internally to class instance state.\n",
    "        Raises:\n",
    "            None\n",
    "        \"\"\"\n",
    "        # IMPLEMENT TRAINING.\n",
    "        # pass\n",
    "\n",
    "        return self.NERModel.train(train,dev,test)\n",
    "\n",
    "\n",
    "    def predict(self, data):\n",
    "        \"\"\"\n",
    "        Predicts on the given input data. Assumes model has been trained with train()\n",
    "        Args:\n",
    "            data: iterable of arbitrary format. represents the data instances and features you use to make predictions\n",
    "                Note that prediction requires trained model. Precondition that class instance already stores trained model\n",
    "                information.\n",
    "        Returns:\n",
    "            predictions: [tuple,...], i.e. list of tuples.\n",
    "                Each tuple is (start index, span, mention text, mention type)\n",
    "                Where:\n",
    "                 - start index: int, the index of the first character of the mention span. None if not applicable.\n",
    "                 - span: int, the length of the mention. None if not applicable.\n",
    "                 - mention text: str, the actual text that was identified as a named entity. Required.\n",
    "                 - mention type: str, the entity/mention type. None if not applicable.\n",
    "                 NOTE: len(predictions) should equal len(data) AND the ordering should not change [important for\n",
    "                     evalutation. See note in evaluate() about parallel arrays.]\n",
    "        Raises:\n",
    "            None\n",
    "        \"\"\"\n",
    "        # IMPLEMENT PREDICTION. STRICT OUTPUT FORMAT REQUIRED.\n",
    "\n",
    "        # return predictions\n",
    "        return self.NERModel.predict(data)\n",
    "\n",
    "    def load_model(self, file):\n",
    "        \"\"\"\n",
    "        :param file: From where to load the model - Optional function\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def evaluate(self, predictions,groundTruths):\n",
    "        \"\"\"\n",
    "        Calculates evaluation metrics on chosen benchmark dataset [Precision,Recall,F1, or others...]\n",
    "        Args:\n",
    "            predictions: [tuple,...], list of tuples [same format as output from predict]\n",
    "            groundTruths: [tuple,...], list of tuples representing ground truth.\n",
    "        Returns:\n",
    "            metrics: tuple with (p,r,f1). Each element is float.\n",
    "        Raises:\n",
    "            None\n",
    "        \"\"\"\n",
    "        # pseudo-implementation\n",
    "        # we have a set of predictions and a set of ground truth data.\n",
    "        # calculate true positive, false positive, and false negative\n",
    "        # calculate Precision = tp/(tp+fp)\n",
    "        # calculate Recall = tp/(tp+fn)\n",
    "        # calculate F1 using precision and recall\n",
    "\n",
    "        # return (precision, recall, f1)\n",
    "        return self.NERModel.evaluate(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization of parameters to load pre-trained model and predict and evaluate for common test input 'ner_test_input.txt'\n",
    "\n",
    "## The predictions are generated in the output file 'output.txt'\n",
    "## Evaluation metrics : Accuracy, F1, Precison and Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"6\"\n",
    "def main(input_file):\n",
    "    # create instance of config\n",
    "    config = Config()\n",
    "    config.layer=int(20) #iterations\n",
    "    config.step=int(1) #window_size\n",
    "\n",
    "    if config.task=='pos':\n",
    "        print(\"USING POS\")\n",
    "        config.filename_train = \"data/train.pos\" # test\n",
    "        config.filename_dev= \"data/dev.pos\"\n",
    "        config.filename_test= \"data/test.pos\"\n",
    "    else:\n",
    "        print(\"USING NER\")      \n",
    "    print(\"iteration: \"+str(config.layer))\n",
    "    print(\"step: \"+str(config.step))\n",
    "    converted_file_path = \"data/temp.txt\"\n",
    "    \n",
    "    model = ner_extraction(config)\n",
    "    \n",
    "    testing = True\n",
    "    if testing ==True:\n",
    "\n",
    "        model.save_model(config.dir_model)\n",
    "        model.read_dataset(input_file,\"Conll3\")\n",
    "        test  = CoNLLDataset(config.filename_test, config.processing_word,config.processing_tag, config.max_iter)\n",
    "        config.filename_test= \"data/temp.txt\"\n",
    "\n",
    "\n",
    "        output_lines=[]\n",
    "        for line in input_file:\n",
    "            output_line=list()\n",
    "            if len(line.strip()) > 0:\n",
    "                if line.strip() !=\"-DOCSTART- O\":\n",
    "                    data=line.split()\n",
    "                        # print(data[0] +\":\");\n",
    "                    preds = model.predict([data[0]])\n",
    "                        # print(preds);\n",
    "                    to_print = align_data({\"input\": [data[0]], \"output\": preds})\n",
    "                    for key, seq in to_print.items():\n",
    "\n",
    "                        if key == \"input\":\n",
    "                                # print (seq.strip())\n",
    "                            output_line.append(seq)\n",
    "#                         if(data[0] == seq.strip()):\n",
    "                                # print(\"actual:\"+data[1])\n",
    "#                             output_line.append(data[1])\n",
    "                                # print(\"key:\"+ key)\n",
    "                        else:\n",
    "                                # print(\"pred:\"+seq)\n",
    "                            output_line.append(seq.strip())\n",
    "                            output_lines.append(output_line)\n",
    "                                # model.logger.info(seq)\n",
    "\n",
    "            else:\n",
    "                output_lines.append(line.strip())\n",
    "                \n",
    "\n",
    "        with open('output1.txt', 'w') as f:\n",
    "            for data in output_lines:\n",
    "                if isinstance(data, list):\n",
    "                    data = \" \".join(data)\n",
    "                    data = data\n",
    "                f.write(data +'\\n')\n",
    "        model.evaluate(test,output_lines)\n",
    "    else:\n",
    "\n",
    "    # build model\n",
    "    # model.build()\n",
    "    # model.restore_session(\"results/crf/model.weights/\") # optional, restore weights\n",
    "    # model.reinitialize_weights(\"proj\")\n",
    "\n",
    "    # create datasets\n",
    "        dev   = CoNLLDataset(config.filename_dev, config.processing_word,\n",
    "                         config.processing_tag, config.max_iter)\n",
    "        train = CoNLLDataset(config.filename_train, config.processing_word,\n",
    "                         config.processing_tag, config.max_iter)\n",
    "\n",
    "        test = CoNLLDataset(config.filename_test, config.processing_word,\n",
    "                        config.processing_tag, config.max_iter)\n",
    "    # train model\n",
    "        model.train(train, dev, test)\n",
    "\n",
    "    return 'output.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING NER\n",
      "iteration: 20\n",
      "step: 1\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /Users/divyasinha/Desktop/548_project/S-LSTM-master/sequence_tagging/model/ner_model.py:147: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From /Users/divyasinha/Desktop/548_project/S-LSTM-master/sequence_tagging/model/ner_model.py:147: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/divyasinha/Desktop/548_project/S-LSTM-master/sequence_tagging/model/ner_model.py:152: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From /Users/divyasinha/Desktop/548_project/S-LSTM-master/sequence_tagging/model/ner_model.py:152: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/divyasinha/Desktop/548_project/S-LSTM-master/sequence_tagging/model/ner_model.py:163: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From /Users/divyasinha/Desktop/548_project/S-LSTM-master/sequence_tagging/model/ner_model.py:163: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: slstm\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:180: calling expand_dims (from tensorflow.python.ops.array_ops) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the `axis` argument instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:180: calling expand_dims (from tensorflow.python.ops.array_ops) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the `axis` argument instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/divyasinha/Desktop/548_project/S-LSTM-master/sequence_tagging/model/ner_model.py:334: calling softmax (from tensorflow.python.ops.nn_ops) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From /Users/divyasinha/Desktop/548_project/S-LSTM-master/sequence_tagging/model/ner_model.py:334: calling softmax (from tensorflow.python.ops.nn_ops) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "Initializing tf session\n",
      "Initializing tf session\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from results/test/model.weights/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring parameters from results/test/model.weights/\n",
      "Testing model over test set\n",
      "acc 93.59 - f1 75.00 - precision 75.00 - recall 75.00\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main(\"ner_test_input.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
