{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "# Fast and Accurate Sequence Labeling with Iterated Dilated Convolution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": "This paper is for using Iterated Dilated Convolution to predict the tag of entities. Dilated Convolution is faster than traditional CNN. In order to use the library, the users need to download embeddings of [GloVe](https://nlp.stanford.edu/projects/glove/) that translates words into float vectors. The embedding file should be put under `./data/embeddings/` folder. In this demonstration, we use `glove.6B.100d.txt` which is **100** dimensions. "
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": "Besides of embedding file, we also need training dataset to train the CNN model. In this document, we use [CoNLL 2003](https://www.clips.uantwerpen.be/conll2003/ner/) as the dataset. It could be downloaded from many places, e.g., from [here](https://github.com/synalp/NER/tree/master/corpus/CoNLL-2003). There will be 3 files in the dataset, i.e. `eng.train` for training, `eng.testa` for validating the model during the training and `eng.testb` for the evaluation."
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": "After all preparations are done, we need to confirm the Python is version 3 and TensorFlow is 1.13.1."
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.13.1\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf; print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## Import the library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "The first step to use the library is to *import* it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": "from extraction.named_entity.DilatedCNN import Ner\nd \u003d Ner.DilatedCNN()"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## Read the data set for training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "There are two parameters for reading the dataset. The first parameter is an array containing 3 elements, corresponding to the path of *train*,*testa* and *testb*. The second parameter is for embedding files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5848\n"
          ]
        }
      ],
      "source": [
        "total_training_sample \u003d d.read_dataset(input_files\u003d[\u0027/home/ubuntu/dilated-cnn-ner/data/conll2003/eng.train\u0027,\u0027/home/ubuntu/dilated-cnn-ner/data/conll2003/eng.testa\u0027,\u0027/home/ubuntu/dilated-cnn-ner/data/conll2003/eng.testb\u0027], embedding\u003d\u0027./data/embeddings/glove.6B.100d.txt\u0027)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "The `read_dataset` will preprocess the input data by vectorizing the words and return the number of sentences. So please **confirm** the Python thread have the previllege to create file and folder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "The most important part is to train the CNN model. By invoking `train` method, the library will start to train the model. As long as the training process, some training information will be given. **In order to do this demo, I set the epoch from 100 to 2 to speed up the training process. So the F1 score will be very low.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "infile/home/ubuntu/dilated-cnn-ner/data/conll2003/eng.train\n",
            "out_dir/home/ubuntu/dilated-cnn-ner/data/conll2003/conll2003-w3-lample/train.txt\n",
            "vocab/home/ubuntu/dilated-cnn-ner/data/conll2003/vocabs/conll2003_cutoff_4.txt\n",
            "{\u0027in_file\u0027: \u0027/home/ubuntu/dilated-cnn-ner/data/conll2003/eng.train\u0027, \u0027vocab\u0027: \u0027/home/ubuntu/dilated-cnn-ner/data/conll2003/vocabs/conll2003_cutoff_4.txt\u0027, \u0027labels\u0027: \u0027\u0027, \u0027shapes\u0027: \u0027\u0027, \u0027chars\u0027: \u0027\u0027, \u0027embeddings\u0027: \u0027./data/embeddings/glove.6B.100d.txt\u0027, \u0027out_dir\u0027: \u0027/home/ubuntu/dilated-cnn-ner/data/conll2003/conll2003-w3-lample/train.txt\u0027, \u0027window_size\u0027: 3, \u0027lowercase\u0027: False, \u0027start_end\u0027: False, \u0027debug\u0027: False, \u0027predict_pad\u0027: False, \u0027documents\u0027: False, \u0027update_maps\u0027: True, \u0027update_vocab\u0027: \u0027\u0027, \u0027dataset\u0027: \u0027conll2003\u0027, \u0027f\u0027: \u0027\u0027}\n",
            "start preprocess\n",
            "Embeddings coverage: 89.64%\n",
            "infile/home/ubuntu/dilated-cnn-ner/data/conll2003/eng.testa\n",
            "out_dir/home/ubuntu/dilated-cnn-ner/data/conll2003/conll2003-w3-lample/valid.txt\n",
            "vocab/home/ubuntu/dilated-cnn-ner/data/conll2003/vocabs/conll2003_cutoff_4.txt\n",
            "{\u0027in_file\u0027: \u0027/home/ubuntu/dilated-cnn-ner/data/conll2003/eng.testa\u0027, \u0027vocab\u0027: \u0027/home/ubuntu/dilated-cnn-ner/data/conll2003/vocabs/conll2003_cutoff_4.txt\u0027, \u0027labels\u0027: \u0027\u0027, \u0027shapes\u0027: \u0027\u0027, \u0027chars\u0027: \u0027\u0027, \u0027embeddings\u0027: \u0027./data/embeddings/glove.6B.100d.txt\u0027, \u0027out_dir\u0027: \u0027/home/ubuntu/dilated-cnn-ner/data/conll2003/conll2003-w3-lample/valid.txt\u0027, \u0027window_size\u0027: 3, \u0027lowercase\u0027: False, \u0027start_end\u0027: False, \u0027debug\u0027: False, \u0027predict_pad\u0027: False, \u0027documents\u0027: False, \u0027update_maps\u0027: True, \u0027update_vocab\u0027: \u0027\u0027, \u0027dataset\u0027: \u0027conll2003\u0027, \u0027f\u0027: \u0027\u0027}\n",
            "start preprocess\n",
            "Embeddings coverage: 86.13%\n",
            "infile/home/ubuntu/dilated-cnn-ner/data/conll2003/eng.testb\n",
            "out_dir/home/ubuntu/dilated-cnn-ner/data/conll2003/conll2003-w3-lample/test.txt\n",
            "vocab/home/ubuntu/dilated-cnn-ner/data/conll2003/vocabs/conll2003_cutoff_4.txt\n",
            "{\u0027in_file\u0027: \u0027/home/ubuntu/dilated-cnn-ner/data/conll2003/eng.testb\u0027, \u0027vocab\u0027: \u0027/home/ubuntu/dilated-cnn-ner/data/conll2003/vocabs/conll2003_cutoff_4.txt\u0027, \u0027labels\u0027: \u0027\u0027, \u0027shapes\u0027: \u0027\u0027, \u0027chars\u0027: \u0027\u0027, \u0027embeddings\u0027: \u0027./data/embeddings/glove.6B.100d.txt\u0027, \u0027out_dir\u0027: \u0027/home/ubuntu/dilated-cnn-ner/data/conll2003/conll2003-w3-lample/test.txt\u0027, \u0027window_size\u0027: 3, \u0027lowercase\u0027: False, \u0027start_end\u0027: False, \u0027debug\u0027: False, \u0027predict_pad\u0027: False, \u0027documents\u0027: False, \u0027update_maps\u0027: True, \u0027update_vocab\u0027: \u0027\u0027, \u0027dataset\u0027: \u0027conll2003\u0027, \u0027f\u0027: \u0027\u0027}\n",
            "start preprocess\n",
            "Embeddings coverage: 83.30%\n",
            "start train\n",
            "num classes: 17\n",
            "/home/ubuntu/dilated-cnn-ner/data/conll2003/conll2003-w3-lample/train.txt/sizes.txt\n",
            "num train examples: 14041\n",
            "num train tokens: 203621\n",
            "/home/ubuntu/dilated-cnn-ner/data/conll2003/conll2003-w3-lample/valid.txt\n",
            "/home/ubuntu/dilated-cnn-ner/data/conll2003/conll2003-w3-lample/valid.txt/sizes.txt\n",
            "num dev examples: 3250\n",
            "num dev tokens: 51362\n",
            "{\u0027ORG\u0027: 0, \u0027O\u0027: 1, \u0027MISC\u0027: 2, \u0027PER\u0027: 3, \u0027LOC\u0027: 4}\n",
            "Loaded 3226/5850 embeddings (55.15% coverage)\n",
            "WARNING:tensorflow:From /home/ubuntu/dilated-cnn-ner/src/data_utils.py:114: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type\u003dtf.int64)[0]).repeat(num_epochs)`. If `shuffle\u003dFalse`, omit the `.shuffle(...)`.\n",
            "WARNING:tensorflow:From /home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/training/input.py:278: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type\u003dtf.int64)[0]).repeat(num_epochs)`. If `shuffle\u003dFalse`, omit the `.shuffle(...)`.\n",
            "WARNING:tensorflow:From /home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/training/input.py:190: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
            "WARNING:tensorflow:From /home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/training/input.py:113: RefVariable.count_up_to (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Prefer Dataset.range instead.\n",
            "WARNING:tensorflow:From /home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/ops/variables.py:2132: count_up_to (from tensorflow.python.ops.state_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Prefer Dataset.range instead.\n",
            "WARNING:tensorflow:From /home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/training/input.py:199: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "WARNING:tensorflow:From /home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/training/input.py:199: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "WARNING:tensorflow:From /home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/training/input.py:202: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /home/ubuntu/dilated-cnn-ner/src/data_utils.py:91: TFRecordReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.TFRecordDataset`.\n",
            "WARNING:tensorflow:From /home/ubuntu/dilated-cnn-ner/src/data_utils.py:130: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad\u003dTrue`).\n",
            "[\u003ctf.Tensor \u0027forward/embedding_lookup/Identity:0\u0027 shape\u003d(?, ?, 100) dtype\u003dfloat32\u003e, \u003ctf.Tensor \u0027forward/embedding_lookup_1/Identity:0\u0027 shape\u003d(?, ?, 5) dtype\u003dfloat32\u003e]\n",
            "Adding initial layer conv0: width: 3; filters: 300\n",
            "WARNING:tensorflow:From /home/ubuntu/dilated-cnn-ner/src/cnn.py:186: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate \u003d 1 - keep_prob`.\n",
            "input feats expanded drop (?, 1, ?, 105)\n",
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "last out shape (?, 1, ?, 300)\n",
            "last dims 300\n",
            "Adding layer conv1: dilation: 1; width: 3; filters: 300; take: False\n",
            "Adding layer conv2: dilation: 2; width: 3; filters: 300; take: False\n",
            "Adding layer conv3: dilation: 1; width: 3; filters: 300; take: True\n",
            "input feats expanded drop (?, 1, ?, 105)\n",
            "last out shape (?, 1, ?, 300)\n",
            "last dims 300\n",
            "WARNING:tensorflow:From /home/ubuntu/dilated-cnn-ner/src/cnn.py:117: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "model vars: 15\n",
            "[\u0027w_e:0\u0027, \u0027forward/w_s:0\u0027, \u0027forward/conv0_w:0\u0027, \u0027forward/conv0_b:0\u0027, \u0027forward/block/conv1_w:0\u0027, \u0027forward/block/conv1_b:0\u0027, \u0027forward/block/conv2_w:0\u0027, \u0027forward/block/conv2_b:0\u0027, \u0027forward/block/conv3_w:0\u0027, \u0027forward/block/conv3_b:0\u0027, \u0027forward/block/w_p:0\u0027, \u0027forward/block/b_p:0\u0027, \u0027forward/block/w_o:0\u0027, \u0027forward/block/b_o:0\u0027, \u0027global_step:0\u0027]\n",
            "Total trainable parameters: 1538337\n",
            "WARNING:tensorflow:From /home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /home/ubuntu/dilated-cnn-ner/Ner.py:974: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.MonitoredTrainingSession\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Starting standard services.\n",
            "INFO:tensorflow:Starting queue runners.\n",
            "WARNING:tensorflow:From /home/ubuntu/dilated-cnn-ner/Ner.py:1035: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "Training on 14041 sentences (14041 examples)\n",
            "                2816 examples at 65.04 examples/sec. Error: 0.61038\n",
            "                5632 examples at 63.54 examples/sec. Error: 0.51379\n",
            "                8448 examples at 65.65 examples/sec. Error: 0.45537\n",
            "               11353 examples at 65.80 examples/sec. Error: 0.40900\n",
            "Segment evaluation TRAIN (iteration 1):\n",
            "\t        F1\tPrec\tRecall\n",
            "Micro (Seg)\t44.80\t51.75\t39.50\n",
            "Macro (Seg)\t35.65\t58.64\t25.61\n",
            "-------\n",
            "-------\n",
            "       ORG\t35.65\t58.64\t25.61\n",
            "         O\t0.00\t0.00\t100.00\n",
            "      MISC\t0.00\t0.00\t0.00\n",
            "       PER\t48.51\t42.73\t56.09\n",
            "       LOC\t58.02\t60.82\t55.46\n",
            "Processed 203621 tokens with 23499 phrases; found: 17936 phrases; correct: 9281.\n",
            "\n",
            "Segment evaluation TEST (iteration 1):\n",
            "\t        F1\tPrec\tRecall\n",
            "Micro (Seg)\t0.62\t3.93\t0.34\n",
            "Macro (Seg)\t0.16\t6.18\t0.08\n",
            "-------\n",
            "-------\n",
            "       ORG\t0.16\t6.18\t0.08\n",
            "         O\t0.00\t0.00\t100.00\n",
            "      MISC\t0.00\t0.00\t0.00\n",
            "       PER\t6.24\t4.83\t8.81\n",
            "       LOC\t1.83\t1.90\t1.76\n",
            "Processed 51362 tokens with 49889 phrases; found: 4275 phrases; correct: 168.\n",
            "Avg training speed: 65.218554 examples/second\n",
            "Serialized model: /home/ubuntu/dilated-cnn-ner/data/conll2003/models/dilated-cnn.tf\n",
            "                2816 examples at 69.96 examples/sec. Error: 0.22489\n",
            "                5632 examples at 69.89 examples/sec. Error: 0.21202\n",
            "                8537 examples at 67.86 examples/sec. Error: 0.20520\n",
            "               11353 examples at 66.15 examples/sec. Error: 0.20213\n",
            "Segment evaluation TRAIN (iteration 2):\n",
            "\t        F1\tPrec\tRecall\n",
            "Micro (Seg)\t56.38\t58.52\t54.39\n",
            "Macro (Seg)\t48.64\t50.12\t47.24\n",
            "-------\n",
            "-------\n",
            "       ORG\t48.64\t50.12\t47.24\n",
            "         O\t0.00\t0.00\t100.00\n",
            "      MISC\t31.99\t58.70\t21.99\n",
            "       PER\t59.59\t55.53\t64.30\n",
            "       LOC\t68.04\t68.97\t67.14\n",
            "Processed 203621 tokens with 23499 phrases; found: 21840 phrases; correct: 12780.\n",
            "\n",
            "Segment evaluation TEST (iteration 2):\n",
            "\t        F1\tPrec\tRecall\n",
            "Micro (Seg)\t1.91\t9.90\t1.06\n",
            "Macro (Seg)\t0.14\t2.52\t0.07\n",
            "-------\n",
            "-------\n",
            "       ORG\t0.14\t2.52\t0.07\n",
            "         O\t0.00\t0.00\t100.00\n",
            "      MISC\t17.01\t84.84\t9.45\n",
            "       PER\t13.33\t10.57\t18.05\n",
            "       LOC\t1.17\t1.17\t1.18\n",
            "Processed 51362 tokens with 49889 phrases; found: 5324 phrases; correct: 527.\n",
            "Avg training speed: 66.462412 examples/second\n",
            "Serialized model: /home/ubuntu/dilated-cnn-ner/data/conll2003/models/dilated-cnn.tf\n",
            "Deserializing model: /home/ubuntu/dilated-cnn-ner/data/conll2003/models/dilated-cnn.tf\n",
            "WARNING:tensorflow:From /home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "INFO:tensorflow:Restoring parameters from /home/ubuntu/dilated-cnn-ner/data/conll2003/models/dilated-cnn.tf\n",
            "Training time: 8 minutes, 2 iterations (4.28 minutes/iteration)\n",
            "Avg training speed: 66.462412 examples/second\n",
            "Best dev F1: 1.91\n"
          ]
        }
      ],
      "source": [
        "d.train(data\u003dNone)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## Predict the tags"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": "After training the model, we can predict new dataset by invoking `predict` method. The `predict` method has 1 parameter and return a list containing the tags. The return value is a list containing the predictions. "
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num classes: 17\n",
            "/home/ubuntu/dilated-cnn-ner/data/conll2003/conll2003-w3-lample/train.txt/sizes.txt\n",
            "num train examples: 14041\n",
            "num train tokens: 203621\n",
            "/home/ubuntu/dilated-cnn-ner/data/conll2003/conll2003-w3-lample/valid.txt\n",
            "/home/ubuntu/dilated-cnn-ner/data/conll2003/conll2003-w3-lample/valid.txt/sizes.txt\n",
            "num dev examples: 3250\n",
            "num dev tokens: 51362\n",
            "{\u0027ORG\u0027: 0, \u0027O\u0027: 1, \u0027MISC\u0027: 2, \u0027PER\u0027: 3, \u0027LOC\u0027: 4}\n",
            "Loaded 3226/5850 embeddings (55.15% coverage)\n",
            "[\u003ctf.Tensor \u0027forward/embedding_lookup/Identity:0\u0027 shape\u003d(?, ?, 100) dtype\u003dfloat32\u003e, \u003ctf.Tensor \u0027forward/embedding_lookup_1/Identity:0\u0027 shape\u003d(?, ?, 5) dtype\u003dfloat32\u003e]\n",
            "Adding initial layer conv0: width: 3; filters: 300\n",
            "input feats expanded drop (?, 1, ?, 105)\n",
            "last out shape (?, 1, ?, 300)\n",
            "last dims 300\n",
            "Adding layer conv1: dilation: 1; width: 3; filters: 300; take: False\n",
            "Adding layer conv2: dilation: 2; width: 3; filters: 300; take: False\n",
            "Adding layer conv3: dilation: 1; width: 3; filters: 300; take: True\n",
            "input feats expanded drop (?, 1, ?, 105)\n",
            "last out shape (?, 1, ?, 300)\n",
            "last dims 300\n",
            "model vars: 15\n",
            "[\u0027w_e:0\u0027, \u0027forward/w_s:0\u0027, \u0027forward/conv0_w:0\u0027, \u0027forward/conv0_b:0\u0027, \u0027forward/block/conv1_w:0\u0027, \u0027forward/block/conv1_b:0\u0027, \u0027forward/block/conv2_w:0\u0027, \u0027forward/block/conv2_b:0\u0027, \u0027forward/block/conv3_w:0\u0027, \u0027forward/block/conv3_b:0\u0027, \u0027forward/block/w_p:0\u0027, \u0027forward/block/b_p:0\u0027, \u0027forward/block/w_o:0\u0027, \u0027forward/block/b_o:0\u0027, \u0027global_step:0\u0027]\n",
            "Total trainable parameters: 1538337\n",
            "WARNING: Loading pretrained model, but not loading:  \u003cmap object at 0x7f5518961e10\u003e\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Starting standard services.\n",
            "INFO:tensorflow:Starting queue runners.\n",
            "Deserializing model: /home/ubuntu/dilated-cnn-ner/data/conll2003/models/dilated-cnn.tf\n",
            "INFO:tensorflow:Restoring parameters from /home/ubuntu/dilated-cnn-ner/data/conll2003/models/dilated-cnn.tf\n",
            "Segment evaluation (train):\n",
            "\t        F1\tPrec\tRecall\n",
            "Micro (Seg)\t56.38\t58.52\t54.39\n",
            "Macro (Seg)\t48.64\t50.12\t47.24\n",
            "-------\n",
            "-------\n",
            "       ORG\t48.64\t50.12\t47.24\n",
            "         O\t0.00\t0.00\t100.00\n",
            "      MISC\t31.99\t58.70\t21.99\n",
            "       PER\t59.59\t55.53\t64.30\n",
            "       LOC\t68.04\t68.97\t67.14\n",
            "Processed 203621 tokens with 23499 phrases; found: 21840 phrases; correct: 12780.\n",
            "\n",
            "Segment evaluation (test):\n",
            "\t        F1\tPrec\tRecall\n",
            "Micro (Seg)\t1.91\t9.90\t1.06\n",
            "Macro (Seg)\t0.14\t2.52\t0.07\n",
            "-------\n",
            "-------\n",
            "       ORG\t0.14\t2.52\t0.07\n",
            "         O\t0.00\t0.00\t100.00\n",
            "      MISC\t17.01\t84.84\t9.45\n",
            "       PER\t13.33\t10.57\t18.05\n",
            "       LOC\t1.17\t1.17\t1.18\n",
            "Processed 51362 tokens with 49889 phrases; found: 5324 phrases; correct: 527.\n",
            "Testing time: 44 seconds\n",
            "CRICKET B-ORG O \n",
            "\n",
            "- B-ORG O \n",
            "\n",
            "\u003cOOV\u003e O O \n",
            "\n",
            "\u003cOOV\u003e B-ORG O \n",
            "\n",
            "\u003cOOV\u003e B-ORG O \n",
            "\n",
            "AT B-ORG O \n",
            "\n",
            "\u003cOOV\u003e B-ORG O\n"
          ]
        }
      ],
      "source": [
        "result \u003d d.predict(\u0027/home/ubuntu/dilated-cnn-ner/data/conll2003-w3-lample/eng.testb\u0027)\n",
        "result[0:100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " \n",
            "\n",
            "AFTER B-ORG O \n",
            "\n",
            "\u003cOOV\u003e B-ORG O \n",
            "\n",
            "\u003cOOV\u003e B-ORG O \n",
            "\n",
            ". B-ORG O \n",
            "\n",
            "\n",
            "\n",
            "LONDON B-MISC B-LOC \n",
            "\n",
            "0000-00-00 B-ORG O \n",
            "\n",
            "\n",
            "\n",
            "West B-PER B-PER \n",
            "\n",
            "Indian I-PER I-PER \n",
            "\n",
            "\u003cOOV\u003e B-ORG O \n",
            "\n",
            "Phil B-LOC B-PER \n",
            "\n",
            "\u003cOOV\u003e B-ORG I-PER \n",
            "\n",
            "took B-ORG O \n",
            "\n",
            "four B-ORG O \n",
            "\n",
            "for B-ORG O \n",
            "\n",
            "00 B-ORG O \n",
            "\n",
            "on B-ORG O \n",
            "\n",
            "Friday B-ORG O \n",
            "\n",
            "as B-ORG O \n",
            "\n",
            "Leicestershire O B-ORG \n",
            "\n",
            "beat B-ORG O \n",
            "\n",
            "Somerset O B-ORG \n",
            "\n",
            "by B-ORG O \n",
            "\n",
            "an B-ORG O \n",
            "\n",
            "innings B-\n"
          ]
        }
      ],
      "source": [
        "print(result[100:500])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## Evaluate the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "The `evaluation` method will return the performance of the evaluation. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "scrolled": true,
        "pycharm": {}
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[\u0027\\t        F1\\tPrec\\tRecall\\n\u0027,\n",
              " \u0027Micro (Seg)\\t1.91\\t9.90\\t1.06\\n\u0027,\n",
              " \u0027Macro (Seg)\\t0.14\\t2.52\\t0.07\\n\u0027]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "d.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## Convert to ground truth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "The `convert_ground_truth` will return the tags in the test file whose path is given in the parameter. The result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[\u0027O\u0027,\n",
              " \u0027O\u0027,\n",
              " \u0027O\u0027,\n",
              " \u0027B-LOC\u0027,\n",
              " \u0027O\u0027,\n",
              " \u0027O\u0027,\n",
              " \u0027O\u0027,\n",
              " \u0027O\u0027,\n",
              " \u0027B-PER\u0027,\n",
              " \u0027O\u0027,\n",
              " \u0027O\u0027,\n",
              " \u0027O\u0027,\n",
              " \u0027O\u0027,\n",
              " \u0027B-PER\u0027,\n",
              " \u0027I-PER\u0027,\n",
              " \u0027B-LOC\u0027,\n",
              " \u0027O\u0027,\n",
              " \u0027B-LOC\u0027,\n",
              " \u0027I-LOC\u0027,\n",
              " \u0027I-LOC\u0027]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result \u003d d.convert_ground_truth(\u0027/home/ubuntu/dilated-cnn-ner/data/conll2003/eng.testb\u0027)\n",
        "result[0:20]"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}