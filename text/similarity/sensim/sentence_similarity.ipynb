{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentence-similarity.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "hE3ogY_1Nu89",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "%cd '/content/drive/My Drive/sentence_similarity'"
      ]
    },
    {
      "metadata": {
        "id": "80wya6N7PmHJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import pickle\n",
        "import gensim\n",
        "from text2digits import text2digits\n",
        "from gensim.models import doc2vec\n",
        "from collections import namedtuple\n",
        "from gensim.models import Word2Vec\n",
        "from scipy import spatial\n",
        "import nltk\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import scipy\n",
        "from scipy import stats\n",
        "from nltk.chunk import  tree2conlltags\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rV5v8PkhPo-R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def read_dataset(filename):\n",
        "    sentences1 = []\n",
        "    sentences2 = []\n",
        "    all_sentences = []\n",
        "    similarity_score = []\n",
        "    with open(filename, 'r', encoding='utf-8') as f:\n",
        "\n",
        "        data = csv.reader(f)\n",
        "        for row in data:\n",
        "            if row[5] != '' and row[6] != '':\n",
        "                sentences1.append(row[5])\n",
        "                sentences2.append(row[6])\n",
        "                all_sentences.append(row[5])\n",
        "                all_sentences.append(row[6])\n",
        "\n",
        "                similarity_score.append(float(float(row[4])/5))\n",
        "\n",
        "    return sentences1,sentences2,all_sentences,similarity_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "viI74uRKQQvf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "filename=\"/content/drive/My Drive/sentence_similarity/sts-train.csv\"\n",
        "sentences1,sentences2,all_sentences,similarity_score=read_dataset(filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PIrcV9hmQ8th",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "9cb8f8ea-400b-4b3f-d7a7-85070c14bcff"
      },
      "cell_type": "code",
      "source": [
        "print(sentences1[:10])\n",
        "\n",
        "print(sentences2[:10])\n",
        "\n",
        "print(similarity_score[:10])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['A plane is taking off.', 'A man is playing a large flute.', 'A man is spreading shreded cheese on a pizza.', 'Three men are playing chess.', 'A man is playing the cello.', 'Some men are fighting.', 'A man is smoking.', 'The man is playing the piano.', 'A man is playing on a guitar and singing.', 'A person is throwing a cat on to the ceiling.']\n",
            "['An air plane is taking off.', 'A man is playing a flute.', 'A man is spreading shredded cheese on an uncooked pizza.', 'Two men are playing chess.', 'A man seated is playing the cello.', 'Two men are fighting.', 'A man is skating.', 'The man is playing the guitar.', 'A woman is playing an acoustic guitar and singing.', 'A person throws a cat on the ceiling.']\n",
            "[1.0, 0.76, 0.76, 0.52, 0.85, 0.85, 0.1, 0.32, 0.44000000000000006, 1.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kLJ1JgJlRQxU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def preprocess(sentences1, sentences2):\n",
        "    # function to convert it into lower case\n",
        "    sentences3 = []\n",
        "    sentences4 = []\n",
        "    for sentence in sentences1:\n",
        "        sentences3.append(sentence.lower())\n",
        "    for sentence in sentences2:\n",
        "        sentences4.append(sentence.lower())\n",
        "    return sentences3, sentences4\n",
        "\n",
        "\n",
        "sentences1, sentences2 = preprocess(sentences1, sentences2)\n",
        "\n",
        "\n",
        "def contractionsEN(sentences):\n",
        "    # copywrite source: https://github.com/cipriantruica/CATS/blob/master/cats/nlplib/static.py\n",
        "    # slightly modified dict than the source\n",
        "    # takes words like he'll and convert it into he will\n",
        "    # tokenize the input by splitting on spaces\n",
        "    result = []\n",
        "    for sentence in sentences:\n",
        "        sentence = sentence.replace(\".,'[]\", \" \")\n",
        "        words = sentence.split()\n",
        "\n",
        "        contractions_en2 = {\n",
        "            \"'s\": \" is\",\n",
        "            \"'ve\": \" have\",\n",
        "            \"'d\": \" had\",\n",
        "            \"ain't\": \"am not\",\n",
        "            \"aren't\": \"are not\",\n",
        "            \"can't\": \"cannot\",\n",
        "            \"can't've\": \"cannot have\",\n",
        "            \"'cause\": \"because\",\n",
        "            \"could've\": \"could have\",\n",
        "            \"couldn't\": \"could not\",\n",
        "            \"couldn't've\": \"could not have\",\n",
        "            \"didn't\": \"did not\",\n",
        "            \"doesn't\": \"does not\",\n",
        "            \"don't\": \"do not\",\n",
        "            \"hadn't\": \"had not\",\n",
        "            \"hadn't've\": \"had not have\",\n",
        "            \"hasn't\": \"has not\",\n",
        "            \"haven't\": \"have not\",\n",
        "            \"he'd\": \"he had\",\n",
        "            \"he'd've\": \"he would have\",\n",
        "            \"he'll\": \"he will\",\n",
        "            \"he'll've\": \"he will have\",\n",
        "            \"he's\": \"he has\",\n",
        "            \"how'd\": \"how did\",\n",
        "            \"how'd'y\": \"how do you\",\n",
        "            \"how'll\": \"how will\",\n",
        "            \"how's\": \"how has\",\n",
        "            \"i'd\": \"i had\",\n",
        "            \"i'd've\": \"i would have\",\n",
        "            \"i'll\": \"i will\",\n",
        "            \"i'll've\": \"i will have\",\n",
        "            \"i'm\": \"i am\",\n",
        "            \"i've\": \"i have\",\n",
        "            \"isn't\": \"is not\",\n",
        "            \"it'd\": \"it had\",\n",
        "            \"it'd've\": \"it would have\",\n",
        "            \"it'll\": \"it will\",\n",
        "            \"it'll've\": \"it will have\",\n",
        "            \"it's\": \"it is\",\n",
        "            \"let's\": \"let us\",\n",
        "            \"ma'am\": \"madam\",\n",
        "            \"mayn't\": \"may not\",\n",
        "            \"might've\": \"might have\",\n",
        "            \"mightn't\": \"might not\",\n",
        "            \"mightn't've\": \"might not have\",\n",
        "            \"must've\": \"must have\",\n",
        "            \"mustn't\": \"must not\",\n",
        "            \"mustn't've\": \"must not have\",\n",
        "            \"needn't\": \"need not\",\n",
        "            \"needn't've\": \"need not have\",\n",
        "            \"o'clock\": \"of the clock\",\n",
        "            \"oughtn't\": \"ought not\",\n",
        "            \"oughtn't've\": \"ought not have\",\n",
        "            \"shan't\": \"shall not\",\n",
        "            \"sha'n't\": \"shall not\",\n",
        "            \"shan't've\": \"shall not have\",\n",
        "            \"she'd\": \"she had\",\n",
        "            \"she'd've\": \"she would have\",\n",
        "            \"she'll\": \"she will\",\n",
        "            \"she'll've\": \"she will have\",\n",
        "            \"she's\": \"she is\",\n",
        "            \"should've\": \"should have\",\n",
        "            \"shouldn't\": \"should not\",\n",
        "            \"shouldn't've\": \"should not have\",\n",
        "            \"so've\": \"so have\",\n",
        "            \"so's\": \"so is\",\n",
        "            \"that'd\": \"that had\",\n",
        "            \"that'd've\": \"that would have\",\n",
        "            \"that's\": \"that is\",\n",
        "            \"there'd\": \"there would\",\n",
        "            \"there'd've\": \"there would have\",\n",
        "            \"there's\": \"there is\",\n",
        "            \"they'd\": \"they would\",\n",
        "            \"they'd've\": \"they would have\",\n",
        "            \"they'll\": \"they will\",\n",
        "            \"they'll've\": \"they will have\",\n",
        "            \"they're\": \"they are\",\n",
        "            \"they've\": \"they have\",\n",
        "            \"to've\": \"to have\",\n",
        "            \"wasn't\": \"was not\",\n",
        "            \"we'd\": \"we would\",\n",
        "            \"we'd've\": \"we would have\",\n",
        "            \"we'll\": \"we will\",\n",
        "            \"we'll've\": \"we will have\",\n",
        "            \"we're\": \"we are\",\n",
        "            \"we've\": \"we have\",\n",
        "            \"weren't\": \"were not\",\n",
        "            \"what'll\": \"what will\",\n",
        "            \"what'll've\": \"what will have\",\n",
        "            \"what're\": \"what are\",\n",
        "            \"what's\": \"what is\",\n",
        "            \"what've\": \"what have\",\n",
        "            \"when's\": \"when has\",\n",
        "            \"when've\": \"when have\",\n",
        "            \"where'd\": \"where did\",\n",
        "            \"where's\": \"where has\",\n",
        "            \"where've\": \"where have\",\n",
        "            \"who'll\": \"who will\",\n",
        "            \"who'll've\": \"who will have\",\n",
        "            \"who's\": \"who has\",\n",
        "            \"who've\": \"who have\",\n",
        "            \"why's\": \"why has\",\n",
        "            \"why've\": \"why have\",\n",
        "            \"will've\": \"will have\",\n",
        "            \"won't\": \"will not\",\n",
        "            \"won't've\": \"will not have\",\n",
        "            \"would've\": \"would have\",\n",
        "            \"wouldn't\": \"would not\",\n",
        "            \"wouldn't've\": \"would not have\",\n",
        "            \"y'all\": \"you all\",\n",
        "            \"y'all'd\": \"you all would\",\n",
        "            \"y'all'd've\": \"you all would have\",\n",
        "            \"y'all're\": \"you all are\",\n",
        "            \"y'all've\": \"you all have\",\n",
        "            \"you'd\": \"you had\",\n",
        "            \"you'd've\": \"you would have\",\n",
        "            \"you'll\": \"you will\",\n",
        "            \"you'll've\": \"you will have\",\n",
        "            \"you're\": \"you are\",\n",
        "            \"you've\": \"you have\"\n",
        "        }\n",
        "        words2 = []\n",
        "        for word in words:\n",
        "            try:\n",
        "\n",
        "                words2.append(contractions_en2[word])\n",
        "            except:\n",
        "                words2.append(word)\n",
        "        sen = ' '.join(i for i in words2)\n",
        "        result.append(sen)\n",
        "    return result\n",
        "\n",
        "\n",
        "sentences1 = contractionsEN(sentences1)\n",
        "sentences2 = contractionsEN(sentences2)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MoG8STDIRl9V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def tokenize(sentences1, sentences2):\n",
        "    # splits the sentence into words based on space\n",
        "    words_sentences1 = []\n",
        "    words_sentences2 = []\n",
        "    for sentence in sentences1:\n",
        "        words_sentences1.append(sentence.split())\n",
        "    for sentence in sentences2:\n",
        "        words_sentences2.append(sentence.split())\n",
        "    return words_sentences1, words_sentences2\n",
        "\n",
        "\n",
        "words_sentences1, words_sentences2 = tokenize(sentences1, sentences2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_X1Q6vcbRqMD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "4aab862a-26cf-4fc1-e9be-8f7cb676e086"
      },
      "cell_type": "code",
      "source": [
        "print(words_sentences1[:10])\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['a', 'plane', 'is', 'taking', 'off.'], ['a', 'man', 'is', 'playing', 'a', 'large', 'flute.'], ['a', 'man', 'is', 'spreading', 'shreded', 'cheese', 'on', 'a', 'pizza.'], ['three', 'men', 'are', 'playing', 'chess.'], ['a', 'man', 'is', 'playing', 'the', 'cello.'], ['some', 'men', 'are', 'fighting.'], ['a', 'man', 'is', 'smoking.'], ['the', 'man', 'is', 'playing', 'the', 'piano.'], ['a', 'man', 'is', 'playing', 'on', 'a', 'guitar', 'and', 'singing.'], ['a', 'person', 'is', 'throwing', 'a', 'cat', 'on', 'to', 'the', 'ceiling.']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5ROY2x-9RyU_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_postags(words_sentences1, words_sentences2):\n",
        "    # takes as input a tokenized sentence and returns their NE tags\n",
        "    ne_tags_sentences1 = []\n",
        "    ne_tags_sentences2 = []\n",
        "    for words in words_sentences1:\n",
        "        ne_tags_sentences1.append(nltk.pos_tag(words))\n",
        "    for words in words_sentences2:\n",
        "        ne_tags_sentences2.append(nltk.pos_tag(words))\n",
        "    return ne_tags_sentences1, ne_tags_sentences2\n",
        "\n",
        "\n",
        "ne_tags_sentences1, ne_tags_sentences2 = generate_postags(words_sentences1, words_sentences2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "viojbhUUSJsp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "ff15170f-5a68-4169-d513-b22e21e6b83c"
      },
      "cell_type": "code",
      "source": [
        "print(ne_tags_sentences1[:10])"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[('a', 'DT'), ('plane', 'NN'), ('is', 'VBZ'), ('taking', 'VBG'), ('off.', 'RP')], [('a', 'DT'), ('man', 'NN'), ('is', 'VBZ'), ('playing', 'VBG'), ('a', 'DT'), ('large', 'JJ'), ('flute.', 'NN')], [('a', 'DT'), ('man', 'NN'), ('is', 'VBZ'), ('spreading', 'VBG'), ('shreded', 'VBN'), ('cheese', 'NN'), ('on', 'IN'), ('a', 'DT'), ('pizza.', 'NN')], [('three', 'CD'), ('men', 'NNS'), ('are', 'VBP'), ('playing', 'VBG'), ('chess.', 'NNS')], [('a', 'DT'), ('man', 'NN'), ('is', 'VBZ'), ('playing', 'VBG'), ('the', 'DT'), ('cello.', 'NN')], [('some', 'DT'), ('men', 'NNS'), ('are', 'VBP'), ('fighting.', 'JJ')], [('a', 'DT'), ('man', 'NN'), ('is', 'VBZ'), ('smoking.', 'JJ')], [('the', 'DT'), ('man', 'NN'), ('is', 'VBZ'), ('playing', 'VBG'), ('the', 'DT'), ('piano.', 'NN')], [('a', 'DT'), ('man', 'NN'), ('is', 'VBZ'), ('playing', 'VBG'), ('on', 'IN'), ('a', 'DT'), ('guitar', 'NN'), ('and', 'CC'), ('singing.', 'NN')], [('a', 'DT'), ('person', 'NN'), ('is', 'VBZ'), ('throwing', 'VBG'), ('a', 'DT'), ('cat', 'NN'), ('on', 'IN'), ('to', 'TO'), ('the', 'DT'), ('ceiling.', 'NN')]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "owvAuqLxSPGh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def group_by_tag(ne_tags_sentences1, ne_tags_sentences2):\n",
        "    res1 = []\n",
        "    res2 = []\n",
        "    for words in ne_tags_sentences1:\n",
        "        tags = set()\n",
        "\n",
        "        for i in range(0, len(words)):\n",
        "            tags.add(words[i][1])\n",
        "        tags = list(tags)\n",
        "        sen_tag_pair = {}\n",
        "        for i in range(0, len(tags)):\n",
        "            temp = []\n",
        "            for j in range(0, len(words)):\n",
        "                if words[j][1] == tags[i]:\n",
        "                    temp.append(words[j][0])\n",
        "            sen_tag_pair[tags[i]] = temp\n",
        "\n",
        "        res1.append(sen_tag_pair)\n",
        "\n",
        "    for words in ne_tags_sentences2:\n",
        "        tags = set()\n",
        "\n",
        "        for i in range(0, len(words)):\n",
        "            tags.add(words[i][1])\n",
        "        tags = list(tags)\n",
        "        sen_tag_pair = {}\n",
        "        for i in range(0, len(tags)):\n",
        "            temp = []\n",
        "            for j in range(0, len(words)):\n",
        "                if words[j][1] == tags[i]:\n",
        "                    temp.append(words[j][0])\n",
        "            sen_tag_pair[tags[i]] = temp\n",
        "\n",
        "        res2.append(sen_tag_pair)\n",
        "    return res1, res2\n",
        "\n",
        "\n",
        "ne_grouped_sentence1, ne_grouped_sentence2 = group_by_tag(ne_tags_sentences1, ne_tags_sentences2)\n",
        "ne_grouped_sentence1 = ne_grouped_sentence1[:5708]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w2Hs8o1uSVJ_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "11fe2247-313d-41a2-a67d-d972858fd8d5"
      },
      "cell_type": "code",
      "source": [
        "print(ne_grouped_sentence1[:10])"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'DT': ['a'], 'RP': ['off.'], 'NN': ['plane'], 'VBG': ['taking'], 'VBZ': ['is']}, {'DT': ['a', 'a'], 'JJ': ['large'], 'NN': ['man', 'flute.'], 'VBG': ['playing'], 'VBZ': ['is']}, {'VBN': ['shreded'], 'DT': ['a', 'a'], 'NN': ['man', 'cheese', 'pizza.'], 'VBG': ['spreading'], 'IN': ['on'], 'VBZ': ['is']}, {'VBP': ['are'], 'VBG': ['playing'], 'NNS': ['men', 'chess.'], 'CD': ['three']}, {'NN': ['man', 'cello.'], 'VBG': ['playing'], 'DT': ['a', 'the'], 'VBZ': ['is']}, {'VBP': ['are'], 'JJ': ['fighting.'], 'DT': ['some'], 'NNS': ['men']}, {'NN': ['man'], 'JJ': ['smoking.'], 'DT': ['a'], 'VBZ': ['is']}, {'NN': ['man', 'piano.'], 'VBG': ['playing'], 'DT': ['the', 'the'], 'VBZ': ['is']}, {'DT': ['a', 'a'], 'CC': ['and'], 'NN': ['man', 'guitar', 'singing.'], 'VBG': ['playing'], 'IN': ['on'], 'VBZ': ['is']}, {'DT': ['a', 'a', 'the'], 'NN': ['person', 'cat', 'ceiling.'], 'VBG': ['throwing'], 'TO': ['to'], 'IN': ['on'], 'VBZ': ['is']}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WhpUZdFoSdRB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def pair_similar_tags(ne_grouped_sentences1, ne_grouped_sentences2):\n",
        "    # takes the ne tagged sentences and groups together the words that have the same NE tags\n",
        "\n",
        "    matched_pairs = []\n",
        "    for i in range(0, len(ne_grouped_sentence1)):\n",
        "        result = {}\n",
        "        for key, val in ne_grouped_sentence1[i].items():\n",
        "\n",
        "            for key1, val1 in ne_grouped_sentence2[i].items():\n",
        "\n",
        "                if key == key1:\n",
        "                    res = []\n",
        "                    res.append(val)\n",
        "                    res.append(val1)\n",
        "                    result[key] = res\n",
        "        matched_pairs.append(result)\n",
        "    return matched_pairs\n",
        "\n",
        "\n",
        "matched_pairs = pair_similar_tags(ne_grouped_sentence1,ne_grouped_sentence2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mDxs2tF_Se7m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "a9a9d0f7-a742-474c-af36-fa4e6885e4b9"
      },
      "cell_type": "code",
      "source": [
        "print(matched_pairs[:10])"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'DT': [['a'], ['an']], 'RP': [['off.'], ['off.']], 'NN': [['plane'], ['air', 'plane']], 'VBG': [['taking'], ['taking']], 'VBZ': [['is'], ['is']]}, {'DT': [['a', 'a'], ['a', 'a']], 'NN': [['man', 'flute.'], ['man', 'flute.']], 'VBG': [['playing'], ['playing']], 'VBZ': [['is'], ['is']]}, {'VBN': [['shreded'], ['shredded']], 'DT': [['a', 'a'], ['a', 'an']], 'NN': [['man', 'cheese', 'pizza.'], ['man', 'cheese', 'pizza.']], 'VBG': [['spreading'], ['spreading']], 'IN': [['on'], ['on']], 'VBZ': [['is'], ['is']]}, {'VBP': [['are'], ['are']], 'VBG': [['playing'], ['playing']], 'NNS': [['men', 'chess.'], ['men', 'chess.']], 'CD': [['three'], ['two']]}, {'NN': [['man', 'cello.'], ['man', 'cello.']], 'VBG': [['playing'], ['playing']], 'DT': [['a', 'the'], ['a', 'the']], 'VBZ': [['is'], ['is']]}, {'VBP': [['are'], ['are']], 'JJ': [['fighting.'], ['fighting.']], 'NNS': [['men'], ['men']]}, {'NN': [['man'], ['man']], 'JJ': [['smoking.'], ['skating.']], 'DT': [['a'], ['a']], 'VBZ': [['is'], ['is']]}, {'NN': [['man', 'piano.'], ['man', 'guitar.']], 'VBG': [['playing'], ['playing']], 'DT': [['the', 'the'], ['the', 'the']], 'VBZ': [['is'], ['is']]}, {'DT': [['a', 'a'], ['a', 'an']], 'CC': [['and'], ['and']], 'NN': [['man', 'guitar', 'singing.'], ['woman', 'guitar', 'singing.']], 'VBG': [['playing'], ['playing']], 'VBZ': [['is'], ['is']]}, {'DT': [['a', 'a', 'the'], ['a', 'a', 'the']], 'NN': [['person', 'cat', 'ceiling.'], ['person', 'cat', 'ceiling.']], 'IN': [['on'], ['on']], 'VBZ': [['is'], ['throws']]}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9QLGFzgGSjKQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def bag_of_words(words1, words2):\n",
        "    # takes as an input two lists which are the words present in 2 sentences and combines them into a single list\n",
        "\n",
        "    bag = []\n",
        "    for word in words1:\n",
        "        bag.append(word)\n",
        "    for word in words2:\n",
        "        bag.append(word)\n",
        "    return bag\n",
        "\n",
        "\n",
        "def skipgram_word2vec(bag):\n",
        "    # takes two sentences(their bag of words representation)\n",
        "    # converts each word into their word vector representation\n",
        "    # uses the skipgram model\n",
        "\n",
        "    model = gensim.models.Word2Vec([bag], min_count=1, window=3, sg=1)\n",
        "    return model\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jTgM0KwJSqfw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "30610e4a-a97b-46ff-d95c-f29537cd1124"
      },
      "cell_type": "code",
      "source": [
        "def calculate_similarity(matched_pairs, word_sentences1, word_sentences2):\n",
        "    # calculate the cosine similarity of the vector representation of 2 sentences\n",
        "    cosine_similarity = []\n",
        "    for i in range(0, len(words_sentences1)):\n",
        "        bag = bag_of_words(word_sentences1[i], word_sentences2[i])\n",
        "        model = skipgram_word2vec(bag)\n",
        "\n",
        "        final = 0\n",
        "        match = 0\n",
        "        for key, val in matched_pairs[i].items():\n",
        "\n",
        "            first_list = val[0]\n",
        "            second_list = val[1]\n",
        "\n",
        "            result = 0\n",
        "            if len(first_list) > len(second_list):\n",
        "                match += len(second_list)\n",
        "                for word1 in second_list:\n",
        "                    max = -100\n",
        "                    for word2 in first_list:\n",
        "                        similar = model.similarity(word1, word2)\n",
        "                        if similar > max:\n",
        "                            max = similar\n",
        "\n",
        "                    result += max\n",
        "\n",
        "\n",
        "            else:\n",
        "                match += len(first_list)\n",
        "                for word1 in first_list:\n",
        "                    max = -100\n",
        "                    for word2 in second_list:\n",
        "                        similar = model.similarity(word1, word2)\n",
        "                        if similar > max:\n",
        "                            max = similar\n",
        "\n",
        "                    result += max\n",
        "\n",
        "            final += result\n",
        "        try:\n",
        "            temp1 = final / match\n",
        "            cosine_similarity.append(temp1)\n",
        "        except:\n",
        "            cosine_similarity.append(0)\n",
        "\n",
        "    return cosine_similarity\n",
        "\n",
        "\n",
        "cos_sim = calculate_similarity(matched_pairs, words_sentences1, words_sentences2)\n",
        "\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "vW-6cZ3aSvZP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "a51f107b-bed8-4450-e67b-06daff17b722"
      },
      "cell_type": "code",
      "source": [
        "print(cos_sim[:10])"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.8152072742581368, 0.9999999701976776, 0.8865301108194722, 0.7830357909202575, 1.0000000099341075, 0.9999999403953552, 0.7656959872692823, 0.8600303182999293, 0.8804805977270007, 0.8496381919831038]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DgzDEnbJTav0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "a4feb72d-9f7f-46af-832d-c34d1036ae9e"
      },
      "cell_type": "code",
      "source": [
        "def generate_ne_tags(words_sentences1):\n",
        "    # takes a tokenized sentence and generates the NE tags for it\n",
        "    answer = []\n",
        "    for sentence in words_sentences1:\n",
        "        result = []\n",
        "        sent = nltk.pos_tag(sentence)\n",
        "        pattern = 'NP: {<DT>?<JJ>*<NN>}'\n",
        "        cp = nltk.RegexpParser(pattern)\n",
        "        cs = cp.parse(sent)\n",
        "        iob_tagged = tree2conlltags(cs)\n",
        "        for i in range(0, len(iob_tagged)):\n",
        "            result.append((sentence[i], iob_tagged[i][-1]))\n",
        "        answer.append(result)\n",
        "    return answer\n",
        "\n",
        "\n",
        "ne_tagged_sentences1 = generate_ne_tags(words_sentences1)\n",
        "ne_tagged_sentences2 = generate_ne_tags(words_sentences2)\n",
        "\n",
        "\n",
        "def group_ne_tagged(ne_tagged_sentences1):\n",
        "    answer = []\n",
        "\n",
        "    for i in range(0, len(ne_tagged_sentences1)):\n",
        "        tags = set()\n",
        "        for j in range(0, len(ne_tagged_sentences1[i])):\n",
        "            tags.add(ne_tagged_sentences1[i][j][1])\n",
        "        tags = list(tags)\n",
        "        final_tag = {}\n",
        "        for tag in tags:\n",
        "            result = []\n",
        "            for j in range(0, len(ne_tagged_sentences1[i])):\n",
        "                if ne_tagged_sentences1[i][j][1] == tag:\n",
        "                    result.append(ne_tagged_sentences1[i][j][0])\n",
        "            final_tag[tag] = result\n",
        "        answer.append(final_tag)\n",
        "\n",
        "    return answer\n",
        "\n",
        "\n",
        "grouped_sentence1 = group_ne_tagged(ne_tagged_sentences1)\n",
        "grouped_sentence2 = group_ne_tagged(ne_tagged_sentences2)\n",
        "\n",
        "matched_pairs2 = pair_similar_tags(grouped_sentence1, grouped_sentence2)\n",
        "cos_sim2 = calculate_similarity(matched_pairs2, words_sentences1, words_sentences2)\n",
        "\n"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "nuaIasG9TjYI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def bow_vec(sentences1, sentences2):\n",
        "    # takes two sentences(their bag of words representation)\n",
        "    # converts each word into their word vector representation\n",
        "    # uses the continuous bag of words model\n",
        "\n",
        "    docs = []\n",
        "    analyzedDocument = namedtuple('AnalyzedDocument', 'words tags')\n",
        "    for i, text in enumerate(sentences1):\n",
        "        words = text.lower().split()\n",
        "        tags = [i]\n",
        "        docs.append(analyzedDocument(words, tags))\n",
        "    model11 = doc2vec.Doc2Vec(docs, vector_size=50, min_count=1)\n",
        "\n",
        "    docs2 = []\n",
        "    analyzedDocument2 = namedtuple('AnalyzedDocument', 'words tags')\n",
        "    for i, text in enumerate(sentences2):\n",
        "        words = text.lower().split()\n",
        "        tags = [i]\n",
        "        docs2.append(analyzedDocument2(words, tags))\n",
        "    model22 = doc2vec.Doc2Vec(docs2, vector_size=50, min_count=1)\n",
        "    return model11, model22\n",
        "\n",
        "\n",
        "model11, model22 = bow_vec(sentences1, sentences2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PxbYmEUATsbA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def written_num():\n",
        "    # program to convert thirty five to 35\n",
        "\n",
        "    converter = text2digits.Text2Digits()\n",
        "    num_similarity = []\n",
        "    for i in range(0, len(sentences1)):\n",
        "        sen1 = converter.convert(sentences1[i])\n",
        "        sen2 = converter.convert(sentences2[i])\n",
        "        num1 = 0\n",
        "        num2 = 0\n",
        "        for s in sen1.split():\n",
        "            if s.isdigit():\n",
        "                num1 += int(s)\n",
        "        for s in sen2.split():\n",
        "            if s.isdigit():\n",
        "                num2 += int(s)\n",
        "\n",
        "        if num1 != 0 and num2 != 0:\n",
        "            num_similarity.append(min(num1, num2) / max(num1, num2))\n",
        "        else:\n",
        "            num_similarity.append(0)\n",
        "\n",
        "    return num_similarity\n",
        "\n",
        "\n",
        "num_sim = written_num()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P5NvpMSPT1w2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "2c71e0ad-bc87-4205-d4f8-a9f1669e1cb5"
      },
      "cell_type": "code",
      "source": [
        "def length_sentences(sentences1, sentences2):\n",
        "    diff = []\n",
        "    for i in range(0, len(sentences1)):\n",
        "        diff.append(abs(len(sentences1[i]) - len(sentences2[i])))\n",
        "    return diff\n",
        "\n",
        "\n",
        "diff = length_sentences(sentences1, sentences2)\n",
        "\n",
        "\n",
        "def bow_similarity():\n",
        "    # input: sentences\n",
        "    # output: tf-idf weighted vector of the sentence\n",
        "\n",
        "    all_sentences.append(\" \")\n",
        "    model = Word2Vec(all_sentences, min_count=1, size=100)\n",
        "    vocab = model.wv.vocab.keys()\n",
        "    wordsInVocab = len(vocab)\n",
        "\n",
        "    import numpy as np\n",
        "\n",
        "    def sent_vectorizer(sent, model):\n",
        "        sent_vec = np.zeros(100)\n",
        "        numw = 0\n",
        "        for w in sent:\n",
        "            try:\n",
        "                sent_vec = np.add(sent_vec, model[w])\n",
        "                numw += 1\n",
        "            except:\n",
        "                pass\n",
        "        return sent_vec / np.sqrt(sent_vec.dot(sent_vec))\n",
        "\n",
        "    V = []\n",
        "    for sentence in all_sentences:\n",
        "        V.append(sent_vectorizer(sentence, model))\n",
        "\n",
        "    from numpy import dot\n",
        "    from numpy.linalg import norm\n",
        "\n",
        "    results = [[0 for i in range(len(V))] for j in range(len(V))]\n",
        "\n",
        "    for i in range(len(V) - 1):\n",
        "        for j in range(i + 1, len(V)):\n",
        "            results[i][j] = dot(V[i], V[j]) / norm(V[i]) / norm(V[j])\n",
        "\n",
        "    # print(results)\n",
        "\n",
        "    from sklearn.feature_extraction.text import TfidfTransformer\n",
        "    tfidf = TfidfTransformer(norm=\"l2\")\n",
        "    tfidf.fit(results)\n",
        "    # print(\"IDF:\", tfidf.idf_)\n",
        "    tf_idf_matrix = tfidf.transform(results)\n",
        "\n",
        "    answer = tf_idf_matrix.todense()\n",
        "\n",
        "    sparse_similarity = []\n",
        "    evens = [x for x in range(len(all_sentences)) if x % 2 == 0]\n",
        "    odds = [x for x in range(len(all_sentences)) if x % 2 != 0]\n",
        "\n",
        "    for z in range(0, len(evens)):\n",
        "        try:\n",
        "            sparse_similarity.append(1 - spatial.distance.cosine(answer[evens[z]], answer[odds[z]]))\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    return sparse_similarity\n",
        "\n",
        "\n",
        "xyz = bow_similarity()\n"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "rl_ic1ZdT7HY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/My Drive/sentence_similarity/train_features_v1.csv\", 'w+', encoding='utf-8') as g:\n",
        "    for i in range(0, len(sentences1)):\n",
        "\n",
        "\n",
        "        # feature 1\n",
        "        g.write(str(cos_sim[i]) + \"\\t\")\n",
        "\n",
        "        # features 2\n",
        "\n",
        "        g.write(str(cos_sim2[i]) + \"\\t\")\n",
        "\n",
        "        # feature 3\n",
        "\n",
        "        g.write(str(xyz[i]) + \"\\t\")\n",
        "\n",
        "        # feature 4\n",
        "\n",
        "        g.write(str(num_sim[i]) + \"\\t\")\n",
        "\n",
        "        # for feature 5\n",
        "        g.write(str(diff[i]) + \"\\t\")\n",
        "\n",
        "\n",
        "        #actual\n",
        "        g.write(str(similarity_score[i])+\"\\t\")\n",
        "        g.write(\"\\n\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jlzUty9-ULqU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/My Drive/sentence_similarity/train_features_v1.csv', header=None, sep='\\t')\n",
        "df.rename(columns={0: 'ne_tagged', 1: 'pos_tagged', 2: 'bag_of_words', 3: 'numeric_sum', 4: 'diff_in_length',5: 'actual_similarity'}, inplace=True)\n",
        "df.to_csv('/content/drive/My Drive/sentence_similarity/train_features.csv', index=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HfiJlQS_Ud28",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "128a28a6-bd12-42b3-9a53-6f0749cf5635"
      },
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv(\"/content/drive/My Drive/sentence_similarity/train_features.csv\")\n",
        "\n",
        "X = dataset.drop('actual_similarity', axis=1)\n",
        "y = dataset['actual_similarity']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25,random_state=42)\n",
        "X_train=X_train.values\n",
        "X_train=np.nan_to_num(X_train)\n",
        "y_train=y_train.values\n",
        "y_train=np.nan_to_num(y_train)\n",
        "X_test=np.nan_to_num(X_test.values)\n",
        "y_test=np.nan_to_num(y_test.values)\n",
        "\n",
        "def save_model(rf):\n",
        "\n",
        "\n",
        "\n",
        "    pickle.dump(rf, open(\"/content/drive/My Drive/sentence_similarity/finalized_model.sav\", 'wb'))\n",
        "\n",
        "def load_model():\n",
        "    loaded_model = pickle.load(open(\"/content/drive/My Drive/sentence_similarity/finalized_model.sav\", 'rb'))\n",
        "    return loaded_model\n",
        "\n",
        "\n",
        "rf = RandomForestRegressor(n_estimators = 1024, random_state = 48,max_depth=8)\n",
        "rf.fit(X_train,y_train)\n",
        "save_model(rf)\n",
        "model=load_model()\n",
        "predictions =model.predict(X_test)\n",
        "\n",
        "print(scipy.stats.pearsonr(predictions,y_test))\n",
        "\n",
        "a=scipy.stats.pearsonr(predictions,y_test)\n",
        "print(\"The pearson correlation Coefficient is \",a[0].__round__(6)*100)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(0.5386824939137813, 1.9425424695711184e-85)\n",
            "The pearson correlation Coefficient is  53.8682\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VWlln0kaWvIv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}