{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Similarity.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "_P01xyATbHkW",
        "colab_type": "code",
        "outputId": "9bb9ff93-918d-4f6a-f9cb-0f155b5684c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!ls \"/content/drive/My Drive/Text Similarity/semantic-neural-network/datasets\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SemEval2014  SemEval2017  sick_2014\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "z0cdK-kFckRa",
        "colab_type": "code",
        "outputId": "212d0122-4740-40e8-e7ab-745f4370aa13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NxqfU9KeeuaG",
        "colab_type": "code",
        "outputId": "1f49bee5-a508-4264-c1b1-8414ace22b19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "cell_type": "code",
      "source": [
        "pip install numpy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (2.8.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from h5py) (1.14.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py) (1.11.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gQspRty2eIrB",
        "colab_type": "code",
        "outputId": "160eb7f2-fa5f-43b1-e4cc-b2384141a80c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 11373
        }
      },
      "cell_type": "code",
      "source": [
        "!python3 \"/content/drive/My Drive/Text Similarity/semantic-neural-network/main.py\" \"SemEval2017/SemEval2017_train.csv\" \"SemEval2017/SemEval2017_dev.csv\" \"SemEval2017/SemEval2017_test.csv\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "2019-04-20 20:52:44,321 : paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
            "2019-04-20 20:52:44,359 : 'pattern' package not found; tag filters are not available for English\n",
            "[1.   0.7  0.7  ... 0.25 0.   0.  ]\n",
            "[0.375 0.65  1.    ... 0.    0.    0.   ]\n",
            "True\n",
            "2019-04-20 20:52:46,820 : Loading embedding model from /content/drive/My Drive/Text Similarity/semantic-neural-network/word2vec/GoogleNews-vectors-negative300.bin\n",
            "2019-04-20 20:52:46,821 : File SemEval2017_WORD2VEC_14823.npy not found. Loading new embedding matrix from: WORD2VEC. Embedding binary: True\n",
            "2019-04-20 20:52:46,821 : loading projection weights from /content/drive/My Drive/Text Similarity/semantic-neural-network/word2vec/GoogleNews-vectors-negative300.bin\n",
            "2019-04-20 20:52:46,821 : this function is deprecated, use smart_open.open instead\n",
            "tcmalloc: large alloc 3600007168 bytes == 0x65fa000 @  0x7f3683117001 0x7f3680c9ff95 0x7f3680d04851 0x7f3680d0692f 0x7f3680d9edd8 0x5030d5 0x507641 0x504c28 0x502540 0x502f3d 0x507641 0x504c28 0x502540 0x502f3d 0x507641 0x504c28 0x502540 0x502f3d 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x506393\n",
            "2019-04-20 20:53:47,531 : loaded (3000000, 300) matrix from /content/drive/My Drive/Text Similarity/semantic-neural-network/word2vec/GoogleNews-vectors-negative300.bin\n",
            "2019-04-20 20:53:47,723 : Creating the embedding matrix\n",
            "2019-04-20 20:53:47,770 : Embedding matrix as been created, removing embedding model from memory\n",
            "2019-04-20 20:53:47,770 : Unknown tokens = 0\n",
            "2019-04-20 20:53:48,174 : Saving matrix in file /content/drive/My Drive/Text Similarity/semantic-neural-network/embedding_matrix/SemEval2017_WORD2VEC_14823.npy\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "2019-04-20 20:53:51,847 : From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "2019-04-20 20:53:51.853956: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\n",
            "2019-04-20 20:53:51.856149: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x1b8dfa0 executing computations on platform Host. Devices:\n",
            "2019-04-20 20:53:51.856189: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-04-20 20:53:52.048738: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-04-20 20:53:52.049244: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x1b8db80 executing computations on platform CUDA. Devices:\n",
            "2019-04-20 20:53:52.049274: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-04-20 20:53:52.049756: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "totalMemory: 14.73GiB freeMemory: 14.60GiB\n",
            "2019-04-20 20:53:52.049785: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
            "2019-04-20 20:53:52.371198: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-04-20 20:53:52.371245: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
            "2019-04-20 20:53:52.371258: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
            "2019-04-20 20:53:52.371520: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-04-20 20:53:52.371569: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14115 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "2019-04-20 20:53:52,765 : START TRAIN\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "2019-04-20 20:53:52,817 : From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 6524 samples, validate on 725 samples\n",
            "Epoch 1/300\n",
            "2019-04-20 20:53:54.262578: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\n",
            "6524/6524 [==============================] - 52s 8ms/step - loss: 0.2164 - val_loss: 0.2106\n",
            "Epoch 2/300\n",
            "6524/6524 [==============================] - 52s 8ms/step - loss: 0.2021 - val_loss: 0.2072\n",
            "Epoch 3/300\n",
            "6524/6524 [==============================] - 52s 8ms/step - loss: 0.1937 - val_loss: 0.2036\n",
            "Epoch 4/300\n",
            "6524/6524 [==============================] - 50s 8ms/step - loss: 0.1866 - val_loss: 0.1982\n",
            "Epoch 5/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.1791 - val_loss: 0.1920\n",
            "Epoch 6/300\n",
            "6524/6524 [==============================] - 53s 8ms/step - loss: 0.1686 - val_loss: 0.1772\n",
            "Epoch 7/300\n",
            "6524/6524 [==============================] - 50s 8ms/step - loss: 0.1354 - val_loss: 0.1102\n",
            "Epoch 8/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.1081 - val_loss: 0.1039\n",
            "Epoch 9/300\n",
            "6524/6524 [==============================] - 50s 8ms/step - loss: 0.0971 - val_loss: 0.0981\n",
            "Epoch 10/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0889 - val_loss: 0.0942\n",
            "Epoch 11/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0822 - val_loss: 0.0891\n",
            "Epoch 12/300\n",
            "6524/6524 [==============================] - 52s 8ms/step - loss: 0.0768 - val_loss: 0.0878\n",
            "Epoch 13/300\n",
            "6524/6524 [==============================] - 52s 8ms/step - loss: 0.0726 - val_loss: 0.0852\n",
            "Epoch 14/300\n",
            "6524/6524 [==============================] - 52s 8ms/step - loss: 0.0690 - val_loss: 0.0860\n",
            "Epoch 15/300\n",
            "6524/6524 [==============================] - 50s 8ms/step - loss: 0.0662 - val_loss: 0.0827\n",
            "Epoch 16/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0636 - val_loss: 0.0823\n",
            "Epoch 17/300\n",
            "6524/6524 [==============================] - 50s 8ms/step - loss: 0.0613 - val_loss: 0.0802\n",
            "Epoch 18/300\n",
            "6524/6524 [==============================] - 52s 8ms/step - loss: 0.0595 - val_loss: 0.0804\n",
            "Epoch 19/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0580 - val_loss: 0.0800\n",
            "Epoch 20/300\n",
            "6524/6524 [==============================] - 50s 8ms/step - loss: 0.0563 - val_loss: 0.0801\n",
            "Epoch 21/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0552 - val_loss: 0.0816\n",
            "Epoch 22/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0540 - val_loss: 0.0789\n",
            "Epoch 23/300\n",
            "6524/6524 [==============================] - 50s 8ms/step - loss: 0.0530 - val_loss: 0.0808\n",
            "Epoch 24/300\n",
            "6524/6524 [==============================] - 53s 8ms/step - loss: 0.0520 - val_loss: 0.0799\n",
            "Epoch 25/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0510 - val_loss: 0.0802\n",
            "Epoch 26/300\n",
            "6524/6524 [==============================] - 50s 8ms/step - loss: 0.0503 - val_loss: 0.0789\n",
            "Epoch 27/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0495 - val_loss: 0.0790\n",
            "Epoch 28/300\n",
            "6524/6524 [==============================] - 50s 8ms/step - loss: 0.0489 - val_loss: 0.0775\n",
            "Epoch 29/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0481 - val_loss: 0.0804\n",
            "Epoch 30/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0475 - val_loss: 0.0796\n",
            "Epoch 31/300\n",
            "6524/6524 [==============================] - 50s 8ms/step - loss: 0.0470 - val_loss: 0.0792\n",
            "Epoch 32/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0464 - val_loss: 0.0801\n",
            "Epoch 33/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0459 - val_loss: 0.0774\n",
            "Epoch 34/300\n",
            "6524/6524 [==============================] - 50s 8ms/step - loss: 0.0453 - val_loss: 0.0789\n",
            "Epoch 35/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0448 - val_loss: 0.0795\n",
            "Epoch 36/300\n",
            "6524/6524 [==============================] - 52s 8ms/step - loss: 0.0444 - val_loss: 0.0784\n",
            "Epoch 37/300\n",
            "6524/6524 [==============================] - 50s 8ms/step - loss: 0.0440 - val_loss: 0.0780\n",
            "Epoch 38/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0435 - val_loss: 0.0790\n",
            "Epoch 39/300\n",
            "6524/6524 [==============================] - 50s 8ms/step - loss: 0.0431 - val_loss: 0.0805\n",
            "Epoch 40/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0428 - val_loss: 0.0777\n",
            "Epoch 41/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0423 - val_loss: 0.0793\n",
            "Epoch 42/300\n",
            "6524/6524 [==============================] - 52s 8ms/step - loss: 0.0420 - val_loss: 0.0776\n",
            "Epoch 43/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0417 - val_loss: 0.0795\n",
            "Epoch 44/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0412 - val_loss: 0.0801\n",
            "Epoch 45/300\n",
            "6524/6524 [==============================] - 50s 8ms/step - loss: 0.0411 - val_loss: 0.0799\n",
            "Epoch 46/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0408 - val_loss: 0.0778\n",
            "Epoch 47/300\n",
            "6524/6524 [==============================] - 49s 8ms/step - loss: 0.0403 - val_loss: 0.0815\n",
            "Epoch 48/300\n",
            "6524/6524 [==============================] - 53s 8ms/step - loss: 0.0401 - val_loss: 0.0778\n",
            "Epoch 49/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0399 - val_loss: 0.0788\n",
            "Epoch 50/300\n",
            "6524/6524 [==============================] - 49s 8ms/step - loss: 0.0396 - val_loss: 0.0795\n",
            "Epoch 51/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0394 - val_loss: 0.0785\n",
            "Epoch 52/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0391 - val_loss: 0.0813\n",
            "Epoch 53/300\n",
            "6524/6524 [==============================] - 49s 8ms/step - loss: 0.0388 - val_loss: 0.0802\n",
            "Epoch 54/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0386 - val_loss: 0.0788\n",
            "Epoch 55/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0384 - val_loss: 0.0811\n",
            "Epoch 56/300\n",
            "6524/6524 [==============================] - 50s 8ms/step - loss: 0.0381 - val_loss: 0.0781\n",
            "Epoch 57/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0379 - val_loss: 0.0782\n",
            "Epoch 58/300\n",
            "6524/6524 [==============================] - 49s 8ms/step - loss: 0.0376 - val_loss: 0.0796\n",
            "Epoch 59/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0374 - val_loss: 0.0788\n",
            "Epoch 60/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0372 - val_loss: 0.0785\n",
            "Epoch 61/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0369 - val_loss: 0.0805\n",
            "Epoch 62/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0367 - val_loss: 0.0793\n",
            "Epoch 63/300\n",
            "6524/6524 [==============================] - 50s 8ms/step - loss: 0.0367 - val_loss: 0.0795\n",
            "Epoch 64/300\n",
            "6524/6524 [==============================] - 50s 8ms/step - loss: 0.0364 - val_loss: 0.0798\n",
            "Epoch 65/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0363 - val_loss: 0.0796\n",
            "Epoch 66/300\n",
            "6524/6524 [==============================] - 50s 8ms/step - loss: 0.0361 - val_loss: 0.0795\n",
            "Epoch 67/300\n",
            "6524/6524 [==============================] - 52s 8ms/step - loss: 0.0358 - val_loss: 0.0777\n",
            "Epoch 68/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0357 - val_loss: 0.0796\n",
            "Epoch 69/300\n",
            "6524/6524 [==============================] - 49s 8ms/step - loss: 0.0356 - val_loss: 0.0787\n",
            "Epoch 70/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0354 - val_loss: 0.0791\n",
            "Epoch 71/300\n",
            "6524/6524 [==============================] - 50s 8ms/step - loss: 0.0353 - val_loss: 0.0796\n",
            "Epoch 72/300\n",
            "6524/6524 [==============================] - 50s 8ms/step - loss: 0.0351 - val_loss: 0.0792\n",
            "Epoch 73/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0349 - val_loss: 0.0788\n",
            "Epoch 74/300\n",
            "6524/6524 [==============================] - 49s 8ms/step - loss: 0.0348 - val_loss: 0.0794\n",
            "Epoch 75/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0347 - val_loss: 0.0782\n",
            "Epoch 76/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0345 - val_loss: 0.0809\n",
            "Epoch 77/300\n",
            "6524/6524 [==============================] - 49s 8ms/step - loss: 0.0345 - val_loss: 0.0808\n",
            "Epoch 78/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0342 - val_loss: 0.0795\n",
            "Epoch 79/300\n",
            "6524/6524 [==============================] - 52s 8ms/step - loss: 0.0341 - val_loss: 0.0795\n",
            "Epoch 80/300\n",
            "6524/6524 [==============================] - 49s 8ms/step - loss: 0.0340 - val_loss: 0.0803\n",
            "Epoch 81/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0338 - val_loss: 0.0818\n",
            "Epoch 82/300\n",
            "6524/6524 [==============================] - 49s 8ms/step - loss: 0.0336 - val_loss: 0.0798\n",
            "Epoch 83/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0336 - val_loss: 0.0789\n",
            "Epoch 84/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0334 - val_loss: 0.0793\n",
            "Epoch 85/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0333 - val_loss: 0.0783\n",
            "Epoch 86/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0333 - val_loss: 0.0805\n",
            "Epoch 87/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0331 - val_loss: 0.0795\n",
            "Epoch 88/300\n",
            "6524/6524 [==============================] - 49s 8ms/step - loss: 0.0330 - val_loss: 0.0818\n",
            "Epoch 89/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0329 - val_loss: 0.0812\n",
            "Epoch 90/300\n",
            "6524/6524 [==============================] - 49s 8ms/step - loss: 0.0327 - val_loss: 0.0813\n",
            "Epoch 91/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0327 - val_loss: 0.0802\n",
            "Epoch 92/300\n",
            "6524/6524 [==============================] - 52s 8ms/step - loss: 0.0325 - val_loss: 0.0837\n",
            "Epoch 93/300\n",
            "6524/6524 [==============================] - 49s 8ms/step - loss: 0.0325 - val_loss: 0.0836\n",
            "Epoch 94/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0324 - val_loss: 0.0807\n",
            "Epoch 95/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0322 - val_loss: 0.0794\n",
            "Epoch 96/300\n",
            "6524/6524 [==============================] - 49s 8ms/step - loss: 0.0321 - val_loss: 0.0826\n",
            "Epoch 97/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0320 - val_loss: 0.0819\n",
            "Epoch 98/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0319 - val_loss: 0.0812\n",
            "Epoch 99/300\n",
            "6524/6524 [==============================] - 50s 8ms/step - loss: 0.0318 - val_loss: 0.0827\n",
            "Epoch 100/300\n",
            "6524/6524 [==============================] - 50s 8ms/step - loss: 0.0317 - val_loss: 0.0845\n",
            "Epoch 101/300\n",
            "6524/6524 [==============================] - 49s 8ms/step - loss: 0.0316 - val_loss: 0.0800\n",
            "Epoch 102/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0314 - val_loss: 0.0824\n",
            "Epoch 103/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0315 - val_loss: 0.0792\n",
            "Epoch 104/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0314 - val_loss: 0.0795\n",
            "Epoch 105/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0311 - val_loss: 0.0805\n",
            "Epoch 106/300\n",
            "6524/6524 [==============================] - 49s 8ms/step - loss: 0.0311 - val_loss: 0.0809\n",
            "Epoch 107/300\n",
            "6524/6524 [==============================] - 50s 8ms/step - loss: 0.0311 - val_loss: 0.0821\n",
            "Epoch 108/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0310 - val_loss: 0.0786\n",
            "Epoch 109/300\n",
            "6524/6524 [==============================] - 50s 8ms/step - loss: 0.0308 - val_loss: 0.0803\n",
            "Epoch 110/300\n",
            "6524/6524 [==============================] - 52s 8ms/step - loss: 0.0308 - val_loss: 0.0804\n",
            "Epoch 111/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0307 - val_loss: 0.0826\n",
            "Epoch 112/300\n",
            "6524/6524 [==============================] - 49s 8ms/step - loss: 0.0306 - val_loss: 0.0805\n",
            "Epoch 113/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0305 - val_loss: 0.0808\n",
            "Epoch 114/300\n",
            "6524/6524 [==============================] - 50s 8ms/step - loss: 0.0305 - val_loss: 0.0811\n",
            "Epoch 115/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0303 - val_loss: 0.0800\n",
            "Epoch 116/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0303 - val_loss: 0.0839\n",
            "Epoch 117/300\n",
            "6524/6524 [==============================] - 49s 8ms/step - loss: 0.0302 - val_loss: 0.0796\n",
            "Epoch 118/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0302 - val_loss: 0.0821\n",
            "Epoch 119/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0300 - val_loss: 0.0822\n",
            "Epoch 120/300\n",
            "6524/6524 [==============================] - 49s 8ms/step - loss: 0.0299 - val_loss: 0.0811\n",
            "Epoch 121/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0298 - val_loss: 0.0800\n",
            "Epoch 122/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0297 - val_loss: 0.0797\n",
            "Epoch 123/300\n",
            "6524/6524 [==============================] - 50s 8ms/step - loss: 0.0297 - val_loss: 0.0808\n",
            "Epoch 124/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0297 - val_loss: 0.0798\n",
            "Epoch 125/300\n",
            "6524/6524 [==============================] - 49s 8ms/step - loss: 0.0296 - val_loss: 0.0804\n",
            "Epoch 126/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0294 - val_loss: 0.0821\n",
            "Epoch 127/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0294 - val_loss: 0.0806\n",
            "Epoch 128/300\n",
            "6524/6524 [==============================] - 50s 8ms/step - loss: 0.0292 - val_loss: 0.0825\n",
            "Epoch 129/300\n",
            "6524/6524 [==============================] - 52s 8ms/step - loss: 0.0292 - val_loss: 0.0804\n",
            "Epoch 130/300\n",
            "6524/6524 [==============================] - 50s 8ms/step - loss: 0.0292 - val_loss: 0.0818\n",
            "Epoch 131/300\n",
            "6524/6524 [==============================] - 50s 8ms/step - loss: 0.0291 - val_loss: 0.0820\n",
            "Epoch 132/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0290 - val_loss: 0.0824\n",
            "Epoch 133/300\n",
            "6524/6524 [==============================] - 50s 8ms/step - loss: 0.0288 - val_loss: 0.0815\n",
            "Epoch 134/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0288 - val_loss: 0.0807\n",
            "Epoch 135/300\n",
            "6524/6524 [==============================] - 53s 8ms/step - loss: 0.0287 - val_loss: 0.0818\n",
            "Epoch 136/300\n",
            "6524/6524 [==============================] - 50s 8ms/step - loss: 0.0288 - val_loss: 0.0814\n",
            "Epoch 137/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0286 - val_loss: 0.0831\n",
            "Epoch 138/300\n",
            "6524/6524 [==============================] - 52s 8ms/step - loss: 0.0286 - val_loss: 0.0815\n",
            "Epoch 139/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0285 - val_loss: 0.0827\n",
            "Epoch 140/300\n",
            "6524/6524 [==============================] - 52s 8ms/step - loss: 0.0284 - val_loss: 0.0847\n",
            "Epoch 141/300\n",
            "6524/6524 [==============================] - 53s 8ms/step - loss: 0.0284 - val_loss: 0.0820\n",
            "Epoch 142/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0284 - val_loss: 0.0817\n",
            "Epoch 143/300\n",
            "6524/6524 [==============================] - 52s 8ms/step - loss: 0.0282 - val_loss: 0.0820\n",
            "Epoch 144/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0281 - val_loss: 0.0814\n",
            "Epoch 145/300\n",
            "6524/6524 [==============================] - 53s 8ms/step - loss: 0.0281 - val_loss: 0.0824\n",
            "Epoch 146/300\n",
            "6524/6524 [==============================] - 52s 8ms/step - loss: 0.0281 - val_loss: 0.0825\n",
            "Epoch 147/300\n",
            "6524/6524 [==============================] - 52s 8ms/step - loss: 0.0279 - val_loss: 0.0838\n",
            "Epoch 148/300\n",
            "6524/6524 [==============================] - 52s 8ms/step - loss: 0.0278 - val_loss: 0.0841\n",
            "Epoch 149/300\n",
            "6524/6524 [==============================] - 52s 8ms/step - loss: 0.0278 - val_loss: 0.0805\n",
            "Epoch 150/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0277 - val_loss: 0.0804\n",
            "Epoch 151/300\n",
            "6524/6524 [==============================] - 53s 8ms/step - loss: 0.0276 - val_loss: 0.0807\n",
            "Epoch 152/300\n",
            "6524/6524 [==============================] - 52s 8ms/step - loss: 0.0276 - val_loss: 0.0827\n",
            "Epoch 153/300\n",
            "6524/6524 [==============================] - 52s 8ms/step - loss: 0.0274 - val_loss: 0.0850\n",
            "Epoch 154/300\n",
            "6524/6524 [==============================] - 52s 8ms/step - loss: 0.0275 - val_loss: 0.0818\n",
            "Epoch 155/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0274 - val_loss: 0.0826\n",
            "Epoch 156/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0274 - val_loss: 0.0821\n",
            "Epoch 157/300\n",
            "6524/6524 [==============================] - 53s 8ms/step - loss: 0.0273 - val_loss: 0.0821\n",
            "Epoch 158/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0272 - val_loss: 0.0828\n",
            "Epoch 159/300\n",
            "6524/6524 [==============================] - 52s 8ms/step - loss: 0.0271 - val_loss: 0.0851\n",
            "Epoch 160/300\n",
            "6524/6524 [==============================] - 52s 8ms/step - loss: 0.0270 - val_loss: 0.0856\n",
            "Epoch 161/300\n",
            "6524/6524 [==============================] - 50s 8ms/step - loss: 0.0270 - val_loss: 0.0819\n",
            "Epoch 162/300\n",
            "6524/6524 [==============================] - 52s 8ms/step - loss: 0.0270 - val_loss: 0.0831\n",
            "Epoch 163/300\n",
            "6524/6524 [==============================] - 52s 8ms/step - loss: 0.0270 - val_loss: 0.0824\n",
            "Epoch 164/300\n",
            "6524/6524 [==============================] - 50s 8ms/step - loss: 0.0268 - val_loss: 0.0803\n",
            "Epoch 165/300\n",
            "6524/6524 [==============================] - 53s 8ms/step - loss: 0.0268 - val_loss: 0.0829\n",
            "Epoch 166/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0267 - val_loss: 0.0838\n",
            "Epoch 167/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0266 - val_loss: 0.0832\n",
            "Epoch 168/300\n",
            "6524/6524 [==============================] - 52s 8ms/step - loss: 0.0266 - val_loss: 0.0837\n",
            "Epoch 169/300\n",
            "6524/6524 [==============================] - 50s 8ms/step - loss: 0.0265 - val_loss: 0.0816\n",
            "Epoch 170/300\n",
            "6524/6524 [==============================] - 52s 8ms/step - loss: 0.0266 - val_loss: 0.0838\n",
            "Epoch 171/300\n",
            "6524/6524 [==============================] - 53s 8ms/step - loss: 0.0266 - val_loss: 0.0835\n",
            "Epoch 172/300\n",
            "6524/6524 [==============================] - 50s 8ms/step - loss: 0.0263 - val_loss: 0.0832\n",
            "Epoch 173/300\n",
            "6524/6524 [==============================] - 52s 8ms/step - loss: 0.0263 - val_loss: 0.0842\n",
            "Epoch 174/300\n",
            "6524/6524 [==============================] - 52s 8ms/step - loss: 0.0263 - val_loss: 0.0839\n",
            "Epoch 175/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0263 - val_loss: 0.0842\n",
            "Epoch 176/300\n",
            "6524/6524 [==============================] - 52s 8ms/step - loss: 0.0263 - val_loss: 0.0834\n",
            "Epoch 177/300\n",
            "6524/6524 [==============================] - 55s 8ms/step - loss: 0.0261 - val_loss: 0.0850\n",
            "Epoch 178/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0259 - val_loss: 0.0823\n",
            "Epoch 179/300\n",
            "6524/6524 [==============================] - 52s 8ms/step - loss: 0.0260 - val_loss: 0.0856\n",
            "Epoch 180/300\n",
            "6524/6524 [==============================] - 52s 8ms/step - loss: 0.0261 - val_loss: 0.0844\n",
            "Epoch 181/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0259 - val_loss: 0.0871\n",
            "Epoch 182/300\n",
            "6524/6524 [==============================] - 52s 8ms/step - loss: 0.0258 - val_loss: 0.0849\n",
            "Epoch 183/300\n",
            "6524/6524 [==============================] - 53s 8ms/step - loss: 0.0258 - val_loss: 0.0845\n",
            "Epoch 184/300\n",
            "6524/6524 [==============================] - 53s 8ms/step - loss: 0.0258 - val_loss: 0.0832\n",
            "Epoch 185/300\n",
            "6524/6524 [==============================] - 53s 8ms/step - loss: 0.0258 - val_loss: 0.0836\n",
            "Epoch 186/300\n",
            "6524/6524 [==============================] - 53s 8ms/step - loss: 0.0256 - val_loss: 0.0839\n",
            "Epoch 187/300\n",
            "6524/6524 [==============================] - 52s 8ms/step - loss: 0.0256 - val_loss: 0.0830\n",
            "Epoch 188/300\n",
            "6524/6524 [==============================] - 52s 8ms/step - loss: 0.0256 - val_loss: 0.0849\n",
            "Epoch 189/300\n",
            "6524/6524 [==============================] - 52s 8ms/step - loss: 0.0254 - val_loss: 0.0823\n",
            "Epoch 190/300\n",
            "6524/6524 [==============================] - 52s 8ms/step - loss: 0.0255 - val_loss: 0.0855\n",
            "Epoch 191/300\n",
            "6524/6524 [==============================] - 55s 9ms/step - loss: 0.0255 - val_loss: 0.0846\n",
            "Epoch 192/300\n",
            "6524/6524 [==============================] - 52s 8ms/step - loss: 0.0253 - val_loss: 0.0848\n",
            "Epoch 193/300\n",
            "6524/6524 [==============================] - 55s 8ms/step - loss: 0.0254 - val_loss: 0.0851\n",
            "Epoch 194/300\n",
            "6524/6524 [==============================] - 60s 9ms/step - loss: 0.0252 - val_loss: 0.0834\n",
            "Epoch 195/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0252 - val_loss: 0.0842\n",
            "Epoch 196/300\n",
            "6524/6524 [==============================] - 58s 9ms/step - loss: 0.0251 - val_loss: 0.0844\n",
            "Epoch 197/300\n",
            "6524/6524 [==============================] - 84s 13ms/step - loss: 0.0251 - val_loss: 0.0849\n",
            "Epoch 198/300\n",
            "6524/6524 [==============================] - 102s 16ms/step - loss: 0.0250 - val_loss: 0.0864\n",
            "Epoch 199/300\n",
            "6524/6524 [==============================] - 104s 16ms/step - loss: 0.0250 - val_loss: 0.0841\n",
            "Epoch 200/300\n",
            "6524/6524 [==============================] - 103s 16ms/step - loss: 0.0248 - val_loss: 0.0839\n",
            "Epoch 201/300\n",
            "6524/6524 [==============================] - 102s 16ms/step - loss: 0.0249 - val_loss: 0.0858\n",
            "Epoch 202/300\n",
            "6524/6524 [==============================] - 104s 16ms/step - loss: 0.0249 - val_loss: 0.0852\n",
            "Epoch 203/300\n",
            "6524/6524 [==============================] - 105s 16ms/step - loss: 0.0248 - val_loss: 0.0829\n",
            "Epoch 204/300\n",
            "6524/6524 [==============================] - 100s 15ms/step - loss: 0.0248 - val_loss: 0.0837\n",
            "Epoch 205/300\n",
            "6524/6524 [==============================] - 103s 16ms/step - loss: 0.0247 - val_loss: 0.0863\n",
            "Epoch 206/300\n",
            "6524/6524 [==============================] - 104s 16ms/step - loss: 0.0247 - val_loss: 0.0865\n",
            "Epoch 207/300\n",
            "6524/6524 [==============================] - 103s 16ms/step - loss: 0.0247 - val_loss: 0.0838\n",
            "Epoch 208/300\n",
            "6524/6524 [==============================] - 105s 16ms/step - loss: 0.0246 - val_loss: 0.0849\n",
            "Epoch 209/300\n",
            "6524/6524 [==============================] - 102s 16ms/step - loss: 0.0245 - val_loss: 0.0847\n",
            "Epoch 210/300\n",
            "6524/6524 [==============================] - 104s 16ms/step - loss: 0.0245 - val_loss: 0.0830\n",
            "Epoch 211/300\n",
            "6524/6524 [==============================] - 104s 16ms/step - loss: 0.0244 - val_loss: 0.0848\n",
            "Epoch 212/300\n",
            "6524/6524 [==============================] - 102s 16ms/step - loss: 0.0244 - val_loss: 0.0855\n",
            "Epoch 213/300\n",
            "6524/6524 [==============================] - 103s 16ms/step - loss: 0.0243 - val_loss: 0.0859\n",
            "Epoch 214/300\n",
            "6524/6524 [==============================] - 103s 16ms/step - loss: 0.0243 - val_loss: 0.0861\n",
            "Epoch 215/300\n",
            "6524/6524 [==============================] - 101s 16ms/step - loss: 0.0242 - val_loss: 0.0867\n",
            "Epoch 216/300\n",
            "6524/6524 [==============================] - 101s 16ms/step - loss: 0.0242 - val_loss: 0.0861\n",
            "Epoch 217/300\n",
            "6524/6524 [==============================] - 106s 16ms/step - loss: 0.0241 - val_loss: 0.0844\n",
            "Epoch 218/300\n",
            "6524/6524 [==============================] - 104s 16ms/step - loss: 0.0240 - val_loss: 0.0852\n",
            "Epoch 219/300\n",
            "6524/6524 [==============================] - 103s 16ms/step - loss: 0.0240 - val_loss: 0.0846\n",
            "Epoch 220/300\n",
            "6524/6524 [==============================] - 105s 16ms/step - loss: 0.0240 - val_loss: 0.0852\n",
            "Epoch 221/300\n",
            "6524/6524 [==============================] - 88s 13ms/step - loss: 0.0240 - val_loss: 0.0862\n",
            "Epoch 222/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0240 - val_loss: 0.0843\n",
            "Epoch 223/300\n",
            "6524/6524 [==============================] - 52s 8ms/step - loss: 0.0239 - val_loss: 0.0836\n",
            "Epoch 224/300\n",
            "6524/6524 [==============================] - 54s 8ms/step - loss: 0.0239 - val_loss: 0.0864\n",
            "Epoch 225/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0238 - val_loss: 0.0857\n",
            "Epoch 226/300\n",
            "6524/6524 [==============================] - 53s 8ms/step - loss: 0.0237 - val_loss: 0.0856\n",
            "Epoch 227/300\n",
            "6524/6524 [==============================] - 52s 8ms/step - loss: 0.0237 - val_loss: 0.0845\n",
            "Epoch 228/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0236 - val_loss: 0.0864\n",
            "Epoch 229/300\n",
            "6524/6524 [==============================] - 68s 10ms/step - loss: 0.0237 - val_loss: 0.0854\n",
            "Epoch 230/300\n",
            "6524/6524 [==============================] - 54s 8ms/step - loss: 0.0236 - val_loss: 0.0845\n",
            "Epoch 231/300\n",
            "6524/6524 [==============================] - 60s 9ms/step - loss: 0.0236 - val_loss: 0.0861\n",
            "Epoch 232/300\n",
            "6524/6524 [==============================] - 61s 9ms/step - loss: 0.0235 - val_loss: 0.0849\n",
            "Epoch 233/300\n",
            "6524/6524 [==============================] - 59s 9ms/step - loss: 0.0234 - val_loss: 0.0856\n",
            "Epoch 234/300\n",
            "6524/6524 [==============================] - 82s 13ms/step - loss: 0.0234 - val_loss: 0.0851\n",
            "Epoch 235/300\n",
            "6524/6524 [==============================] - 61s 9ms/step - loss: 0.0234 - val_loss: 0.0844\n",
            "Epoch 236/300\n",
            "6524/6524 [==============================] - 75s 11ms/step - loss: 0.0234 - val_loss: 0.0854\n",
            "Epoch 237/300\n",
            "6524/6524 [==============================] - 75s 11ms/step - loss: 0.0233 - val_loss: 0.0894\n",
            "Epoch 238/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0233 - val_loss: 0.0859\n",
            "Epoch 239/300\n",
            "6524/6524 [==============================] - 53s 8ms/step - loss: 0.0231 - val_loss: 0.0851\n",
            "Epoch 240/300\n",
            "6524/6524 [==============================] - 53s 8ms/step - loss: 0.0232 - val_loss: 0.0845\n",
            "Epoch 241/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0232 - val_loss: 0.0886\n",
            "Epoch 242/300\n",
            "6524/6524 [==============================] - 52s 8ms/step - loss: 0.0230 - val_loss: 0.0860\n",
            "Epoch 243/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0231 - val_loss: 0.0853\n",
            "Epoch 244/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0230 - val_loss: 0.0868\n",
            "Epoch 245/300\n",
            "6524/6524 [==============================] - 53s 8ms/step - loss: 0.0230 - val_loss: 0.0889\n",
            "Epoch 246/300\n",
            "6524/6524 [==============================] - 52s 8ms/step - loss: 0.0230 - val_loss: 0.0888\n",
            "Epoch 247/300\n",
            "6524/6524 [==============================] - 53s 8ms/step - loss: 0.0229 - val_loss: 0.0860\n",
            "Epoch 248/300\n",
            "6524/6524 [==============================] - 70s 11ms/step - loss: 0.0228 - val_loss: 0.0862\n",
            "Epoch 249/300\n",
            "6524/6524 [==============================] - 76s 12ms/step - loss: 0.0229 - val_loss: 0.0852\n",
            "Epoch 250/300\n",
            "6524/6524 [==============================] - 52s 8ms/step - loss: 0.0228 - val_loss: 0.0855\n",
            "Epoch 251/300\n",
            "6524/6524 [==============================] - 52s 8ms/step - loss: 0.0227 - val_loss: 0.0885\n",
            "Epoch 252/300\n",
            "6524/6524 [==============================] - 53s 8ms/step - loss: 0.0227 - val_loss: 0.0869\n",
            "Epoch 253/300\n",
            "6524/6524 [==============================] - 76s 12ms/step - loss: 0.0227 - val_loss: 0.0882\n",
            "Epoch 254/300\n",
            "6524/6524 [==============================] - 69s 11ms/step - loss: 0.0226 - val_loss: 0.0860\n",
            "Epoch 255/300\n",
            "6524/6524 [==============================] - 61s 9ms/step - loss: 0.0226 - val_loss: 0.0870\n",
            "Epoch 256/300\n",
            "6524/6524 [==============================] - 80s 12ms/step - loss: 0.0226 - val_loss: 0.0852\n",
            "Epoch 257/300\n",
            "6524/6524 [==============================] - 58s 9ms/step - loss: 0.0225 - val_loss: 0.0875\n",
            "Epoch 258/300\n",
            "6524/6524 [==============================] - 53s 8ms/step - loss: 0.0225 - val_loss: 0.0869\n",
            "Epoch 259/300\n",
            "6524/6524 [==============================] - 77s 12ms/step - loss: 0.0224 - val_loss: 0.0853\n",
            "Epoch 260/300\n",
            "6524/6524 [==============================] - 70s 11ms/step - loss: 0.0224 - val_loss: 0.0858\n",
            "Epoch 261/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0223 - val_loss: 0.0864\n",
            "Epoch 262/300\n",
            "6524/6524 [==============================] - 53s 8ms/step - loss: 0.0224 - val_loss: 0.0889\n",
            "Epoch 263/300\n",
            "6524/6524 [==============================] - 52s 8ms/step - loss: 0.0223 - val_loss: 0.0857\n",
            "Epoch 264/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0222 - val_loss: 0.0850\n",
            "Epoch 265/300\n",
            "6524/6524 [==============================] - 52s 8ms/step - loss: 0.0222 - val_loss: 0.0874\n",
            "Epoch 266/300\n",
            "6524/6524 [==============================] - 53s 8ms/step - loss: 0.0221 - val_loss: 0.0864\n",
            "Epoch 267/300\n",
            "6524/6524 [==============================] - 52s 8ms/step - loss: 0.0221 - val_loss: 0.0875\n",
            "Epoch 268/300\n",
            "6524/6524 [==============================] - 53s 8ms/step - loss: 0.0221 - val_loss: 0.0875\n",
            "Epoch 269/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0221 - val_loss: 0.0878\n",
            "Epoch 270/300\n",
            "6524/6524 [==============================] - 52s 8ms/step - loss: 0.0220 - val_loss: 0.0870\n",
            "Epoch 271/300\n",
            "6524/6524 [==============================] - 52s 8ms/step - loss: 0.0219 - val_loss: 0.0865\n",
            "Epoch 272/300\n",
            "6524/6524 [==============================] - 53s 8ms/step - loss: 0.0220 - val_loss: 0.0857\n",
            "Epoch 273/300\n",
            "6524/6524 [==============================] - 53s 8ms/step - loss: 0.0219 - val_loss: 0.0861\n",
            "Epoch 274/300\n",
            "6524/6524 [==============================] - 52s 8ms/step - loss: 0.0219 - val_loss: 0.0872\n",
            "Epoch 275/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0219 - val_loss: 0.0865\n",
            "Epoch 276/300\n",
            "6524/6524 [==============================] - 52s 8ms/step - loss: 0.0218 - val_loss: 0.0876\n",
            "Epoch 277/300\n",
            "6524/6524 [==============================] - 53s 8ms/step - loss: 0.0219 - val_loss: 0.0864\n",
            "Epoch 278/300\n",
            "6524/6524 [==============================] - 52s 8ms/step - loss: 0.0218 - val_loss: 0.0878\n",
            "Epoch 279/300\n",
            "6524/6524 [==============================] - 53s 8ms/step - loss: 0.0216 - val_loss: 0.0879\n",
            "Epoch 280/300\n",
            "6524/6524 [==============================] - 52s 8ms/step - loss: 0.0216 - val_loss: 0.0861\n",
            "Epoch 281/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0215 - val_loss: 0.0856\n",
            "Epoch 282/300\n",
            "6524/6524 [==============================] - 52s 8ms/step - loss: 0.0216 - val_loss: 0.0881\n",
            "Epoch 283/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0215 - val_loss: 0.0860\n",
            "Epoch 284/300\n",
            "6524/6524 [==============================] - 53s 8ms/step - loss: 0.0215 - val_loss: 0.0861\n",
            "Epoch 285/300\n",
            "6524/6524 [==============================] - 52s 8ms/step - loss: 0.0214 - val_loss: 0.0870\n",
            "Epoch 286/300\n",
            "6524/6524 [==============================] - 50s 8ms/step - loss: 0.0216 - val_loss: 0.0888\n",
            "Epoch 287/300\n",
            "6524/6524 [==============================] - 52s 8ms/step - loss: 0.0213 - val_loss: 0.0885\n",
            "Epoch 288/300\n",
            "6524/6524 [==============================] - 52s 8ms/step - loss: 0.0215 - val_loss: 0.0894\n",
            "Epoch 289/300\n",
            "6524/6524 [==============================] - 50s 8ms/step - loss: 0.0213 - val_loss: 0.0888\n",
            "Epoch 290/300\n",
            "6524/6524 [==============================] - 53s 8ms/step - loss: 0.0212 - val_loss: 0.0878\n",
            "Epoch 291/300\n",
            "6524/6524 [==============================] - 53s 8ms/step - loss: 0.0213 - val_loss: 0.0880\n",
            "Epoch 292/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0212 - val_loss: 0.0871\n",
            "Epoch 293/300\n",
            "6524/6524 [==============================] - 52s 8ms/step - loss: 0.0211 - val_loss: 0.0891\n",
            "Epoch 294/300\n",
            "6524/6524 [==============================] - 52s 8ms/step - loss: 0.0212 - val_loss: 0.0855\n",
            "Epoch 295/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0211 - val_loss: 0.0886\n",
            "Epoch 296/300\n",
            "6524/6524 [==============================] - 52s 8ms/step - loss: 0.0211 - val_loss: 0.0872\n",
            "Epoch 297/300\n",
            "6524/6524 [==============================] - 52s 8ms/step - loss: 0.0209 - val_loss: 0.0871\n",
            "Epoch 298/300\n",
            "6524/6524 [==============================] - 53s 8ms/step - loss: 0.0211 - val_loss: 0.0881\n",
            "Epoch 299/300\n",
            "6524/6524 [==============================] - 52s 8ms/step - loss: 0.0210 - val_loss: 0.0874\n",
            "Epoch 300/300\n",
            "6524/6524 [==============================] - 51s 8ms/step - loss: 0.0209 - val_loss: 0.0873\n",
            "2019-04-21 01:36:33,802 : \n",
            "Training time finished.\n",
            "300 epochs in 4:42:41.036551\n",
            "2019-04-21 01:36:33,802 : Saving Model\n",
            "2019-04-21 01:36:38,421 : Creating result file\n",
            " Pearson: 0.7672312\n",
            "Results stored in<_io.TextIOWrapper name='/content/drive/My Drive/Text Similarity/semantic-neural-network/results/output/output_SemEval2017.txt' mode='w' encoding='UTF-8'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "U8EElLGLoRVj",
        "colab_type": "code",
        "outputId": "76681837-daa5-4e97-fc77-7c8bad580bfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 11776
        }
      },
      "cell_type": "code",
      "source": [
        "!python3 \"/content/drive/My Drive/Text Similarity/semantic-neural-network/main.py\" \"SemEval2014/SemEval2014_train.txt\" \"SemEval2014/SemEval2014_dev.txt\" \"SemEval2014/SemEval2014_test.txt\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "2019-04-20 17:56:50,897 : paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
            "2019-04-20 17:56:50,935 : 'pattern' package not found; tag filters are not available for English\n",
            "[0.5  0.   0.7  ... 0.5  0.7  0.75]\n",
            "[0.75 0.8  0.8  0.55 0.   0.65 0.65 0.25 0.85 0.1  0.9  0.25 0.3  0.7\n",
            " 0.65 0.4  0.95 0.7  0.45 0.65 0.8  0.6  0.75 0.75 0.55 0.85 1.   0.\n",
            " 0.7  0.8  0.35 0.65 0.7  0.25 0.3  0.9  0.   0.55 0.75 0.25 0.6  0.7\n",
            " 0.8  0.55 1.   0.2  0.15 0.9  0.2  0.2  0.95 0.85 0.6  0.85 0.05 0.7\n",
            " 0.7  0.1  0.75 0.85 0.1  0.6  0.6  0.6  0.5  0.7  0.85 0.6  0.75 0.\n",
            " 0.9  0.   0.55 0.65 0.8  0.05 0.8  0.65 0.25 0.8  0.7  0.05 0.55 0.8\n",
            " 0.35 0.9  0.85 0.2  0.55 0.6  0.45 0.7  0.6  0.2  0.45 0.55 0.2  0.6\n",
            " 1.   0.1  0.3  1.   0.   0.   0.6  0.9  0.2  0.8  0.4  0.45 0.95 0.85\n",
            " 0.75 0.8  0.7  0.55 0.6  0.5  0.6  0.6  0.7  1.   1.   0.   0.35 0.65\n",
            " 0.25 0.8  0.7  0.75 0.5  0.65 0.2  0.95 0.55 0.7  1.   0.55 0.7  0.\n",
            " 0.75 0.3  0.55 0.75 0.7  0.4  0.6  0.1  0.55 1.   0.   1.   1.   0.8\n",
            " 0.35 0.8  0.4  0.6  0.65 0.2  0.1  0.65 0.75 0.05 0.5  0.2  0.55 0.8\n",
            " 0.75 0.55 0.6  0.7  0.35 0.6  0.25 0.55 0.6  0.55 0.6  0.9  0.15 1.\n",
            " 0.95 0.05 0.9  0.45 0.6  0.4  0.2  0.85 0.75 0.7  0.15 0.65 0.65 0.\n",
            " 0.3  0.65 0.05 0.8  0.6  0.   0.7  0.85 0.6  0.5  0.75 0.   0.75 1.\n",
            " 0.   1.   1.   0.3  0.95 0.2  0.3  0.4  0.85 0.05 0.95 0.7  0.05 0.5\n",
            " 0.9  0.55 0.   0.95 0.   0.65 0.8  0.4  0.7  0.7  0.15 0.55 0.55 0.5\n",
            " 0.4  0.   0.   0.   0.8  0.65 0.6  0.65 0.   0.   0.1  0.35 0.05 0.55\n",
            " 0.65 0.75 0.3  0.4  0.3  0.45 0.65 0.5  0.25 0.6  0.5  0.4  0.5  0.25\n",
            " 0.   0.85 0.   0.65 0.   0.2  0.2  0.5  0.6  0.05 0.   0.1  0.25 0.65\n",
            " 0.75 0.55 0.2  0.75 0.5  0.65 0.55 0.65 0.25 0.   0.5  0.5  0.6  0.5\n",
            " 0.4  0.2  0.7  0.35 0.   0.25]\n",
            "True\n",
            "2019-04-20 17:56:52,239 : Loading embedding model from /content/drive/My Drive/Text Similarity/semantic-neural-network/word2vec/GoogleNews-vectors-negative300.bin\n",
            "2019-04-20 17:56:52,240 : Loading existing embedding matrix\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "2019-04-20 17:56:54,075 : From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "2019-04-20 17:56:54.082603: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\n",
            "2019-04-20 17:56:54.082801: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x159b1e0 executing computations on platform Host. Devices:\n",
            "2019-04-20 17:56:54.082845: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-04-20 17:56:54.245629: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-04-20 17:56:54.246128: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x159adc0 executing computations on platform CUDA. Devices:\n",
            "2019-04-20 17:56:54.246157: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-04-20 17:56:54.246516: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "totalMemory: 14.73GiB freeMemory: 14.60GiB\n",
            "2019-04-20 17:56:54.246543: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
            "2019-04-20 17:56:54.683915: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-04-20 17:56:54.683975: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
            "2019-04-20 17:56:54.683989: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
            "2019-04-20 17:56:54.684236: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-04-20 17:56:54.684276: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14115 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "2019-04-20 17:56:55,045 : START TRAIN\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "2019-04-20 17:56:55,105 : From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 2025 samples, validate on 225 samples\n",
            "Epoch 1/300\n",
            "2019-04-20 17:56:56.554291: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\n",
            "2025/2025 [==============================] - 18s 9ms/step - loss: 0.2652 - val_loss: 0.3031\n",
            "Epoch 2/300\n",
            "2025/2025 [==============================] - 16s 8ms/step - loss: 0.2575 - val_loss: 0.2990\n",
            "Epoch 3/300\n",
            "2025/2025 [==============================] - 16s 8ms/step - loss: 0.2511 - val_loss: 0.2955\n",
            "Epoch 4/300\n",
            "2025/2025 [==============================] - 16s 8ms/step - loss: 0.2456 - val_loss: 0.2910\n",
            "Epoch 5/300\n",
            "2025/2025 [==============================] - 16s 8ms/step - loss: 0.2402 - val_loss: 0.2860\n",
            "Epoch 6/300\n",
            "2025/2025 [==============================] - 16s 8ms/step - loss: 0.2350 - val_loss: 0.2816\n",
            "Epoch 7/300\n",
            "2025/2025 [==============================] - 15s 8ms/step - loss: 0.2301 - val_loss: 0.2791\n",
            "Epoch 8/300\n",
            "2025/2025 [==============================] - 16s 8ms/step - loss: 0.2254 - val_loss: 0.2758\n",
            "Epoch 9/300\n",
            "2025/2025 [==============================] - 16s 8ms/step - loss: 0.2213 - val_loss: 0.2732\n",
            "Epoch 10/300\n",
            "2025/2025 [==============================] - 16s 8ms/step - loss: 0.2178 - val_loss: 0.2706\n",
            "Epoch 11/300\n",
            "2025/2025 [==============================] - 15s 8ms/step - loss: 0.2141 - val_loss: 0.2700\n",
            "Epoch 12/300\n",
            "2025/2025 [==============================] - 15s 8ms/step - loss: 0.2102 - val_loss: 0.2669\n",
            "Epoch 13/300\n",
            "2025/2025 [==============================] - 15s 8ms/step - loss: 0.2065 - val_loss: 0.2636\n",
            "Epoch 14/300\n",
            "2025/2025 [==============================] - 16s 8ms/step - loss: 0.2027 - val_loss: 0.2604\n",
            "Epoch 15/300\n",
            "2025/2025 [==============================] - 16s 8ms/step - loss: 0.1984 - val_loss: 0.2572\n",
            "Epoch 16/300\n",
            "2025/2025 [==============================] - 15s 8ms/step - loss: 0.1939 - val_loss: 0.2520\n",
            "Epoch 17/300\n",
            "2025/2025 [==============================] - 15s 8ms/step - loss: 0.1882 - val_loss: 0.2467\n",
            "Epoch 18/300\n",
            "2025/2025 [==============================] - 15s 8ms/step - loss: 0.1814 - val_loss: 0.2347\n",
            "Epoch 19/300\n",
            "2025/2025 [==============================] - 16s 8ms/step - loss: 0.1700 - val_loss: 0.2085\n",
            "Epoch 20/300\n",
            "2025/2025 [==============================] - 17s 8ms/step - loss: 0.1416 - val_loss: 0.1393\n",
            "Epoch 21/300\n",
            "2025/2025 [==============================] - 17s 8ms/step - loss: 0.1203 - val_loss: 0.1194\n",
            "Epoch 22/300\n",
            "2025/2025 [==============================] - 15s 8ms/step - loss: 0.1123 - val_loss: 0.1159\n",
            "Epoch 23/300\n",
            "2025/2025 [==============================] - 15s 8ms/step - loss: 0.1054 - val_loss: 0.1158\n",
            "Epoch 24/300\n",
            "2025/2025 [==============================] - 16s 8ms/step - loss: 0.0998 - val_loss: 0.1187\n",
            "Epoch 25/300\n",
            "2025/2025 [==============================] - 17s 8ms/step - loss: 0.0940 - val_loss: 0.1150\n",
            "Epoch 26/300\n",
            "2025/2025 [==============================] - 16s 8ms/step - loss: 0.0888 - val_loss: 0.1081\n",
            "Epoch 27/300\n",
            "2025/2025 [==============================] - 15s 8ms/step - loss: 0.0842 - val_loss: 0.1077\n",
            "Epoch 28/300\n",
            "2025/2025 [==============================] - 15s 8ms/step - loss: 0.0797 - val_loss: 0.1047\n",
            "Epoch 29/300\n",
            "2025/2025 [==============================] - 15s 8ms/step - loss: 0.0751 - val_loss: 0.1032\n",
            "Epoch 30/300\n",
            "2025/2025 [==============================] - 17s 8ms/step - loss: 0.0717 - val_loss: 0.1042\n",
            "Epoch 31/300\n",
            "2025/2025 [==============================] - 15s 8ms/step - loss: 0.0684 - val_loss: 0.1002\n",
            "Epoch 32/300\n",
            "2025/2025 [==============================] - 15s 8ms/step - loss: 0.0655 - val_loss: 0.1026\n",
            "Epoch 33/300\n",
            "2025/2025 [==============================] - 15s 8ms/step - loss: 0.0631 - val_loss: 0.0991\n",
            "Epoch 34/300\n",
            "2025/2025 [==============================] - 16s 8ms/step - loss: 0.0608 - val_loss: 0.1001\n",
            "Epoch 35/300\n",
            "2025/2025 [==============================] - 17s 8ms/step - loss: 0.0587 - val_loss: 0.1006\n",
            "Epoch 36/300\n",
            "2025/2025 [==============================] - 15s 8ms/step - loss: 0.0570 - val_loss: 0.0996\n",
            "Epoch 37/300\n",
            "2025/2025 [==============================] - 16s 8ms/step - loss: 0.0551 - val_loss: 0.0996\n",
            "Epoch 38/300\n",
            "2025/2025 [==============================] - 16s 8ms/step - loss: 0.0539 - val_loss: 0.0994\n",
            "Epoch 39/300\n",
            "2025/2025 [==============================] - 16s 8ms/step - loss: 0.0521 - val_loss: 0.1016\n",
            "Epoch 40/300\n",
            "2025/2025 [==============================] - 17s 8ms/step - loss: 0.0511 - val_loss: 0.0986\n",
            "Epoch 41/300\n",
            "2025/2025 [==============================] - 17s 8ms/step - loss: 0.0496 - val_loss: 0.1024\n",
            "Epoch 42/300\n",
            "2025/2025 [==============================] - 16s 8ms/step - loss: 0.0484 - val_loss: 0.1052\n",
            "Epoch 43/300\n",
            "2025/2025 [==============================] - 16s 8ms/step - loss: 0.0474 - val_loss: 0.1017\n",
            "Epoch 44/300\n",
            "2025/2025 [==============================] - 15s 8ms/step - loss: 0.0463 - val_loss: 0.0988\n",
            "Epoch 45/300\n",
            "2025/2025 [==============================] - 17s 8ms/step - loss: 0.0452 - val_loss: 0.1015\n",
            "Epoch 46/300\n",
            "2025/2025 [==============================] - 16s 8ms/step - loss: 0.0442 - val_loss: 0.0999\n",
            "Epoch 47/300\n",
            "2025/2025 [==============================] - 15s 8ms/step - loss: 0.0435 - val_loss: 0.1049\n",
            "Epoch 48/300\n",
            "2025/2025 [==============================] - 16s 8ms/step - loss: 0.0427 - val_loss: 0.1019\n",
            "Epoch 49/300\n",
            "2025/2025 [==============================] - 16s 8ms/step - loss: 0.0420 - val_loss: 0.1008\n",
            "Epoch 50/300\n",
            "2025/2025 [==============================] - 17s 8ms/step - loss: 0.0414 - val_loss: 0.1019\n",
            "Epoch 51/300\n",
            "2025/2025 [==============================] - 16s 8ms/step - loss: 0.0405 - val_loss: 0.1010\n",
            "Epoch 52/300\n",
            "2025/2025 [==============================] - 16s 8ms/step - loss: 0.0398 - val_loss: 0.1000\n",
            "Epoch 53/300\n",
            "2025/2025 [==============================] - 16s 8ms/step - loss: 0.0392 - val_loss: 0.1048\n",
            "Epoch 54/300\n",
            "2025/2025 [==============================] - 15s 8ms/step - loss: 0.0388 - val_loss: 0.1009\n",
            "Epoch 55/300\n",
            "2025/2025 [==============================] - 17s 8ms/step - loss: 0.0381 - val_loss: 0.1010\n",
            "Epoch 56/300\n",
            "2025/2025 [==============================] - 16s 8ms/step - loss: 0.0376 - val_loss: 0.1049\n",
            "Epoch 57/300\n",
            "2025/2025 [==============================] - 16s 8ms/step - loss: 0.0371 - val_loss: 0.1037\n",
            "Epoch 58/300\n",
            "2025/2025 [==============================] - 16s 8ms/step - loss: 0.0366 - val_loss: 0.1049\n",
            "Epoch 59/300\n",
            "2025/2025 [==============================] - 16s 8ms/step - loss: 0.0362 - val_loss: 0.1009\n",
            "Epoch 60/300\n",
            "2025/2025 [==============================] - 17s 8ms/step - loss: 0.0357 - val_loss: 0.1032\n",
            "Epoch 61/300\n",
            "2025/2025 [==============================] - 16s 8ms/step - loss: 0.0354 - val_loss: 0.1026\n",
            "Epoch 62/300\n",
            "2025/2025 [==============================] - 16s 8ms/step - loss: 0.0352 - val_loss: 0.1031\n",
            "Epoch 63/300\n",
            "2025/2025 [==============================] - 16s 8ms/step - loss: 0.0343 - val_loss: 0.1036\n",
            "Epoch 64/300\n",
            "2025/2025 [==============================] - 16s 8ms/step - loss: 0.0340 - val_loss: 0.1028\n",
            "Epoch 65/300\n",
            "2025/2025 [==============================] - 17s 8ms/step - loss: 0.0336 - val_loss: 0.1063\n",
            "Epoch 66/300\n",
            "2025/2025 [==============================] - 16s 8ms/step - loss: 0.0332 - val_loss: 0.1066\n",
            "Epoch 67/300\n",
            "2025/2025 [==============================] - 16s 8ms/step - loss: 0.0331 - val_loss: 0.1037\n",
            "Epoch 68/300\n",
            "2025/2025 [==============================] - 16s 8ms/step - loss: 0.0325 - val_loss: 0.1039\n",
            "Epoch 69/300\n",
            "2025/2025 [==============================] - 16s 8ms/step - loss: 0.0323 - val_loss: 0.1012\n",
            "Epoch 70/300\n",
            "2025/2025 [==============================] - 19s 9ms/step - loss: 0.0317 - val_loss: 0.1060\n",
            "Epoch 71/300\n",
            "2025/2025 [==============================] - 15s 8ms/step - loss: 0.0316 - val_loss: 0.1031\n",
            "Epoch 72/300\n",
            "2025/2025 [==============================] - 16s 8ms/step - loss: 0.0311 - val_loss: 0.1036\n",
            "Epoch 73/300\n",
            "2025/2025 [==============================] - 21s 11ms/step - loss: 0.0309 - val_loss: 0.1049\n",
            "Epoch 74/300\n",
            "2025/2025 [==============================] - 23s 11ms/step - loss: 0.0304 - val_loss: 0.1056\n",
            "Epoch 75/300\n",
            "2025/2025 [==============================] - 25s 12ms/step - loss: 0.0303 - val_loss: 0.1045\n",
            "Epoch 76/300\n",
            "2025/2025 [==============================] - 25s 12ms/step - loss: 0.0300 - val_loss: 0.1048\n",
            "Epoch 77/300\n",
            "2025/2025 [==============================] - 27s 13ms/step - loss: 0.0297 - val_loss: 0.1050\n",
            "Epoch 78/300\n",
            "2025/2025 [==============================] - 26s 13ms/step - loss: 0.0295 - val_loss: 0.1034\n",
            "Epoch 79/300\n",
            "2025/2025 [==============================] - 26s 13ms/step - loss: 0.0291 - val_loss: 0.1025\n",
            "Epoch 80/300\n",
            "2025/2025 [==============================] - 25s 12ms/step - loss: 0.0288 - val_loss: 0.1028\n",
            "Epoch 81/300\n",
            "2025/2025 [==============================] - 26s 13ms/step - loss: 0.0287 - val_loss: 0.1013\n",
            "Epoch 82/300\n",
            "2025/2025 [==============================] - 26s 13ms/step - loss: 0.0283 - val_loss: 0.1018\n",
            "Epoch 83/300\n",
            "2025/2025 [==============================] - 25s 13ms/step - loss: 0.0282 - val_loss: 0.1044\n",
            "Epoch 84/300\n",
            "2025/2025 [==============================] - 26s 13ms/step - loss: 0.0280 - val_loss: 0.1071\n",
            "Epoch 85/300\n",
            "2025/2025 [==============================] - 26s 13ms/step - loss: 0.0277 - val_loss: 0.1054\n",
            "Epoch 86/300\n",
            "2025/2025 [==============================] - 25s 12ms/step - loss: 0.0274 - val_loss: 0.1030\n",
            "Epoch 87/300\n",
            "2025/2025 [==============================] - 26s 13ms/step - loss: 0.0273 - val_loss: 0.1021\n",
            "Epoch 88/300\n",
            "2025/2025 [==============================] - 25s 12ms/step - loss: 0.0272 - val_loss: 0.1059\n",
            "Epoch 89/300\n",
            "2025/2025 [==============================] - 26s 13ms/step - loss: 0.0270 - val_loss: 0.1081\n",
            "Epoch 90/300\n",
            "2025/2025 [==============================] - 26s 13ms/step - loss: 0.0268 - val_loss: 0.1063\n",
            "Epoch 91/300\n",
            "2025/2025 [==============================] - 26s 13ms/step - loss: 0.0265 - val_loss: 0.1043\n",
            "Epoch 92/300\n",
            "2025/2025 [==============================] - 25s 12ms/step - loss: 0.0263 - val_loss: 0.1074\n",
            "Epoch 93/300\n",
            "2025/2025 [==============================] - 26s 13ms/step - loss: 0.0262 - val_loss: 0.1049\n",
            "Epoch 94/300\n",
            "2025/2025 [==============================] - 25s 13ms/step - loss: 0.0258 - val_loss: 0.1044\n",
            "Epoch 95/300\n",
            "2025/2025 [==============================] - 25s 12ms/step - loss: 0.0258 - val_loss: 0.1046\n",
            "Epoch 96/300\n",
            "2025/2025 [==============================] - 25s 12ms/step - loss: 0.0256 - val_loss: 0.1079\n",
            "Epoch 97/300\n",
            "2025/2025 [==============================] - 26s 13ms/step - loss: 0.0254 - val_loss: 0.1063\n",
            "Epoch 98/300\n",
            "2025/2025 [==============================] - 25s 12ms/step - loss: 0.0252 - val_loss: 0.1052\n",
            "Epoch 99/300\n",
            "2025/2025 [==============================] - 25s 12ms/step - loss: 0.0251 - val_loss: 0.1084\n",
            "Epoch 100/300\n",
            "2025/2025 [==============================] - 26s 13ms/step - loss: 0.0249 - val_loss: 0.1066\n",
            "Epoch 101/300\n",
            "2025/2025 [==============================] - 26s 13ms/step - loss: 0.0247 - val_loss: 0.1058\n",
            "Epoch 102/300\n",
            "2025/2025 [==============================] - 25s 12ms/step - loss: 0.0247 - val_loss: 0.1057\n",
            "Epoch 103/300\n",
            "2025/2025 [==============================] - 23s 11ms/step - loss: 0.0245 - val_loss: 0.1067\n",
            "Epoch 104/300\n",
            "2025/2025 [==============================] - 22s 11ms/step - loss: 0.0242 - val_loss: 0.1037\n",
            "Epoch 105/300\n",
            "2025/2025 [==============================] - 25s 12ms/step - loss: 0.0240 - val_loss: 0.1068\n",
            "Epoch 106/300\n",
            "2025/2025 [==============================] - 26s 13ms/step - loss: 0.0239 - val_loss: 0.1045\n",
            "Epoch 107/300\n",
            "2025/2025 [==============================] - 25s 12ms/step - loss: 0.0238 - val_loss: 0.1051\n",
            "Epoch 108/300\n",
            "2025/2025 [==============================] - 25s 12ms/step - loss: 0.0237 - val_loss: 0.1078\n",
            "Epoch 109/300\n",
            "2025/2025 [==============================] - 26s 13ms/step - loss: 0.0235 - val_loss: 0.1062\n",
            "Epoch 110/300\n",
            "2025/2025 [==============================] - 25s 12ms/step - loss: 0.0235 - val_loss: 0.1076\n",
            "Epoch 111/300\n",
            "2025/2025 [==============================] - 26s 13ms/step - loss: 0.0232 - val_loss: 0.1076\n",
            "Epoch 112/300\n",
            "2025/2025 [==============================] - 25s 13ms/step - loss: 0.0231 - val_loss: 0.1123\n",
            "Epoch 113/300\n",
            "2025/2025 [==============================] - 25s 12ms/step - loss: 0.0229 - val_loss: 0.1030\n",
            "Epoch 114/300\n",
            "2025/2025 [==============================] - 26s 13ms/step - loss: 0.0227 - val_loss: 0.1081\n",
            "Epoch 115/300\n",
            "2025/2025 [==============================] - 25s 12ms/step - loss: 0.0228 - val_loss: 0.1074\n",
            "Epoch 116/300\n",
            "2025/2025 [==============================] - 26s 13ms/step - loss: 0.0223 - val_loss: 0.1057\n",
            "Epoch 117/300\n",
            "2025/2025 [==============================] - 26s 13ms/step - loss: 0.0223 - val_loss: 0.1097\n",
            "Epoch 118/300\n",
            "2025/2025 [==============================] - 25s 12ms/step - loss: 0.0222 - val_loss: 0.1072\n",
            "Epoch 119/300\n",
            "2025/2025 [==============================] - 26s 13ms/step - loss: 0.0222 - val_loss: 0.1081\n",
            "Epoch 120/300\n",
            "2025/2025 [==============================] - 22s 11ms/step - loss: 0.0221 - val_loss: 0.1060\n",
            "Epoch 121/300\n",
            "2025/2025 [==============================] - 21s 10ms/step - loss: 0.0219 - val_loss: 0.1078\n",
            "Epoch 122/300\n",
            "2025/2025 [==============================] - 23s 11ms/step - loss: 0.0218 - val_loss: 0.1080\n",
            "Epoch 123/300\n",
            "2025/2025 [==============================] - 23s 11ms/step - loss: 0.0218 - val_loss: 0.1136\n",
            "Epoch 124/300\n",
            "2025/2025 [==============================] - 24s 12ms/step - loss: 0.0215 - val_loss: 0.1080\n",
            "Epoch 125/300\n",
            "2025/2025 [==============================] - 25s 12ms/step - loss: 0.0217 - val_loss: 0.1086\n",
            "Epoch 126/300\n",
            "2025/2025 [==============================] - 27s 13ms/step - loss: 0.0209 - val_loss: 0.1104\n",
            "Epoch 127/300\n",
            "2025/2025 [==============================] - 26s 13ms/step - loss: 0.0210 - val_loss: 0.1125\n",
            "Epoch 128/300\n",
            "2025/2025 [==============================] - 25s 12ms/step - loss: 0.0212 - val_loss: 0.1090\n",
            "Epoch 129/300\n",
            "2025/2025 [==============================] - 25s 12ms/step - loss: 0.0210 - val_loss: 0.1086\n",
            "Epoch 130/300\n",
            "2025/2025 [==============================] - 26s 13ms/step - loss: 0.0209 - val_loss: 0.1103\n",
            "Epoch 131/300\n",
            "2025/2025 [==============================] - 25s 13ms/step - loss: 0.0209 - val_loss: 0.1120\n",
            "Epoch 132/300\n",
            "2025/2025 [==============================] - 26s 13ms/step - loss: 0.0206 - val_loss: 0.1122\n",
            "Epoch 133/300\n",
            "2025/2025 [==============================] - 25s 12ms/step - loss: 0.0206 - val_loss: 0.1135\n",
            "Epoch 134/300\n",
            "2025/2025 [==============================] - 24s 12ms/step - loss: 0.0205 - val_loss: 0.1117\n",
            "Epoch 135/300\n",
            "2025/2025 [==============================] - 25s 12ms/step - loss: 0.0205 - val_loss: 0.1110\n",
            "Epoch 136/300\n",
            "2025/2025 [==============================] - 25s 13ms/step - loss: 0.0204 - val_loss: 0.1096\n",
            "Epoch 137/300\n",
            "2025/2025 [==============================] - 24s 12ms/step - loss: 0.0203 - val_loss: 0.1130\n",
            "Epoch 138/300\n",
            "2025/2025 [==============================] - 25s 12ms/step - loss: 0.0202 - val_loss: 0.1119\n",
            "Epoch 139/300\n",
            "2025/2025 [==============================] - 27s 13ms/step - loss: 0.0200 - val_loss: 0.1099\n",
            "Epoch 140/300\n",
            "2025/2025 [==============================] - 25s 12ms/step - loss: 0.0198 - val_loss: 0.1123\n",
            "Epoch 141/300\n",
            "2025/2025 [==============================] - 26s 13ms/step - loss: 0.0198 - val_loss: 0.1101\n",
            "Epoch 142/300\n",
            "2025/2025 [==============================] - 26s 13ms/step - loss: 0.0199 - val_loss: 0.1085\n",
            "Epoch 143/300\n",
            "2025/2025 [==============================] - 26s 13ms/step - loss: 0.0198 - val_loss: 0.1099\n",
            "Epoch 144/300\n",
            "2025/2025 [==============================] - 25s 12ms/step - loss: 0.0196 - val_loss: 0.1082\n",
            "Epoch 145/300\n",
            "2025/2025 [==============================] - 26s 13ms/step - loss: 0.0197 - val_loss: 0.1120\n",
            "Epoch 146/300\n",
            "2025/2025 [==============================] - 24s 12ms/step - loss: 0.0193 - val_loss: 0.1129\n",
            "Epoch 147/300\n",
            "2025/2025 [==============================] - 25s 12ms/step - loss: 0.0194 - val_loss: 0.1137\n",
            "Epoch 148/300\n",
            "2025/2025 [==============================] - 26s 13ms/step - loss: 0.0193 - val_loss: 0.1118\n",
            "Epoch 149/300\n",
            "2025/2025 [==============================] - 25s 12ms/step - loss: 0.0191 - val_loss: 0.1115\n",
            "Epoch 150/300\n",
            "2025/2025 [==============================] - 25s 12ms/step - loss: 0.0192 - val_loss: 0.1113\n",
            "Epoch 151/300\n",
            "2025/2025 [==============================] - 24s 12ms/step - loss: 0.0191 - val_loss: 0.1126\n",
            "Epoch 152/300\n",
            "2025/2025 [==============================] - 22s 11ms/step - loss: 0.0190 - val_loss: 0.1149\n",
            "Epoch 153/300\n",
            "2025/2025 [==============================] - 21s 10ms/step - loss: 0.0189 - val_loss: 0.1101\n",
            "Epoch 154/300\n",
            "2025/2025 [==============================] - 24s 12ms/step - loss: 0.0187 - val_loss: 0.1105\n",
            "Epoch 155/300\n",
            "2025/2025 [==============================] - 25s 12ms/step - loss: 0.0187 - val_loss: 0.1139\n",
            "Epoch 156/300\n",
            "2025/2025 [==============================] - 25s 12ms/step - loss: 0.0186 - val_loss: 0.1126\n",
            "Epoch 157/300\n",
            "2025/2025 [==============================] - 25s 12ms/step - loss: 0.0186 - val_loss: 0.1146\n",
            "Epoch 158/300\n",
            "2025/2025 [==============================] - 27s 13ms/step - loss: 0.0185 - val_loss: 0.1195\n",
            "Epoch 159/300\n",
            "2025/2025 [==============================] - 25s 12ms/step - loss: 0.0184 - val_loss: 0.1157\n",
            "Epoch 160/300\n",
            "2025/2025 [==============================] - 25s 12ms/step - loss: 0.0184 - val_loss: 0.1163\n",
            "Epoch 161/300\n",
            "2025/2025 [==============================] - 26s 13ms/step - loss: 0.0183 - val_loss: 0.1138\n",
            "Epoch 162/300\n",
            "2025/2025 [==============================] - 25s 12ms/step - loss: 0.0182 - val_loss: 0.1140\n",
            "Epoch 163/300\n",
            "2025/2025 [==============================] - 25s 12ms/step - loss: 0.0181 - val_loss: 0.1126\n",
            "Epoch 164/300\n",
            "2025/2025 [==============================] - 26s 13ms/step - loss: 0.0180 - val_loss: 0.1144\n",
            "Epoch 165/300\n",
            "2025/2025 [==============================] - 25s 12ms/step - loss: 0.0180 - val_loss: 0.1162\n",
            "Epoch 166/300\n",
            "2025/2025 [==============================] - 24s 12ms/step - loss: 0.0179 - val_loss: 0.1121\n",
            "Epoch 167/300\n",
            "2025/2025 [==============================] - 25s 13ms/step - loss: 0.0178 - val_loss: 0.1147\n",
            "Epoch 168/300\n",
            "2025/2025 [==============================] - 26s 13ms/step - loss: 0.0177 - val_loss: 0.1156\n",
            "Epoch 169/300\n",
            "2025/2025 [==============================] - 24s 12ms/step - loss: 0.0177 - val_loss: 0.1145\n",
            "Epoch 170/300\n",
            "2025/2025 [==============================] - 27s 13ms/step - loss: 0.0177 - val_loss: 0.1153\n",
            "Epoch 171/300\n",
            "2025/2025 [==============================] - 26s 13ms/step - loss: 0.0176 - val_loss: 0.1171\n",
            "Epoch 172/300\n",
            "2025/2025 [==============================] - 24s 12ms/step - loss: 0.0175 - val_loss: 0.1160\n",
            "Epoch 173/300\n",
            "2025/2025 [==============================] - 24s 12ms/step - loss: 0.0175 - val_loss: 0.1142\n",
            "Epoch 174/300\n",
            "2025/2025 [==============================] - 26s 13ms/step - loss: 0.0173 - val_loss: 0.1156\n",
            "Epoch 175/300\n",
            "2025/2025 [==============================] - 24s 12ms/step - loss: 0.0173 - val_loss: 0.1157\n",
            "Epoch 176/300\n",
            "2025/2025 [==============================] - 26s 13ms/step - loss: 0.0172 - val_loss: 0.1177\n",
            "Epoch 177/300\n",
            "2025/2025 [==============================] - 27s 13ms/step - loss: 0.0171 - val_loss: 0.1192\n",
            "Epoch 178/300\n",
            "2025/2025 [==============================] - 25s 12ms/step - loss: 0.0170 - val_loss: 0.1187\n",
            "Epoch 179/300\n",
            "2025/2025 [==============================] - 24s 12ms/step - loss: 0.0170 - val_loss: 0.1142\n",
            "Epoch 180/300\n",
            "2025/2025 [==============================] - 22s 11ms/step - loss: 0.0168 - val_loss: 0.1187\n",
            "Epoch 181/300\n",
            "2025/2025 [==============================] - 22s 11ms/step - loss: 0.0168 - val_loss: 0.1142\n",
            "Epoch 182/300\n",
            "2025/2025 [==============================] - 15s 8ms/step - loss: 0.0169 - val_loss: 0.1153\n",
            "Epoch 183/300\n",
            "2025/2025 [==============================] - 16s 8ms/step - loss: 0.0168 - val_loss: 0.1188\n",
            "Epoch 184/300\n",
            "2025/2025 [==============================] - 15s 8ms/step - loss: 0.0166 - val_loss: 0.1148\n",
            "Epoch 185/300\n",
            "2025/2025 [==============================] - 16s 8ms/step - loss: 0.0165 - val_loss: 0.1169\n",
            "Epoch 186/300\n",
            "2025/2025 [==============================] - 15s 8ms/step - loss: 0.0164 - val_loss: 0.1166\n",
            "Epoch 187/300\n",
            "2025/2025 [==============================] - 15s 8ms/step - loss: 0.0165 - val_loss: 0.1172\n",
            "Epoch 188/300\n",
            "2025/2025 [==============================] - 15s 8ms/step - loss: 0.0164 - val_loss: 0.1158\n",
            "Epoch 189/300\n",
            "2025/2025 [==============================] - 15s 8ms/step - loss: 0.0163 - val_loss: 0.1184\n",
            "Epoch 190/300\n",
            "2025/2025 [==============================] - 16s 8ms/step - loss: 0.0164 - val_loss: 0.1172\n",
            "Epoch 191/300\n",
            "2025/2025 [==============================] - 16s 8ms/step - loss: 0.0164 - val_loss: 0.1155\n",
            "Epoch 192/300\n",
            "2025/2025 [==============================] - 16s 8ms/step - loss: 0.0163 - val_loss: 0.1174\n",
            "Epoch 193/300\n",
            "2025/2025 [==============================] - 24s 12ms/step - loss: 0.0162 - val_loss: 0.1165\n",
            "Epoch 194/300\n",
            "2025/2025 [==============================] - 24s 12ms/step - loss: 0.0162 - val_loss: 0.1208\n",
            "Epoch 195/300\n",
            "2025/2025 [==============================] - 24s 12ms/step - loss: 0.0160 - val_loss: 0.1191\n",
            "Epoch 196/300\n",
            "2025/2025 [==============================] - 24s 12ms/step - loss: 0.0160 - val_loss: 0.1174\n",
            "Epoch 197/300\n",
            "2025/2025 [==============================] - 24s 12ms/step - loss: 0.0160 - val_loss: 0.1179\n",
            "Epoch 198/300\n",
            "2025/2025 [==============================] - 26s 13ms/step - loss: 0.0160 - val_loss: 0.1209\n",
            "Epoch 199/300\n",
            "2025/2025 [==============================] - 25s 12ms/step - loss: 0.0158 - val_loss: 0.1188\n",
            "Epoch 200/300\n",
            "2025/2025 [==============================] - 25s 12ms/step - loss: 0.0158 - val_loss: 0.1152\n",
            "Epoch 201/300\n",
            "2025/2025 [==============================] - 26s 13ms/step - loss: 0.0158 - val_loss: 0.1183\n",
            "Epoch 202/300\n",
            "2025/2025 [==============================] - 25s 13ms/step - loss: 0.0156 - val_loss: 0.1187\n",
            "Epoch 203/300\n",
            "2025/2025 [==============================] - 24s 12ms/step - loss: 0.0155 - val_loss: 0.1208\n",
            "Epoch 204/300\n",
            "2025/2025 [==============================] - 26s 13ms/step - loss: 0.0156 - val_loss: 0.1219\n",
            "Epoch 205/300\n",
            "2025/2025 [==============================] - 25s 12ms/step - loss: 0.0156 - val_loss: 0.1224\n",
            "Epoch 206/300\n",
            "2025/2025 [==============================] - 27s 13ms/step - loss: 0.0154 - val_loss: 0.1156\n",
            "Epoch 207/300\n",
            "2025/2025 [==============================] - 26s 13ms/step - loss: 0.0154 - val_loss: 0.1211\n",
            "Epoch 208/300\n",
            "2025/2025 [==============================] - 24s 12ms/step - loss: 0.0154 - val_loss: 0.1197\n",
            "Epoch 209/300\n",
            "2025/2025 [==============================] - 25s 12ms/step - loss: 0.0152 - val_loss: 0.1211\n",
            "Epoch 210/300\n",
            "2025/2025 [==============================] - 25s 12ms/step - loss: 0.0154 - val_loss: 0.1188\n",
            "Epoch 211/300\n",
            "2025/2025 [==============================] - 26s 13ms/step - loss: 0.0153 - val_loss: 0.1233\n",
            "Epoch 212/300\n",
            "2025/2025 [==============================] - 24s 12ms/step - loss: 0.0152 - val_loss: 0.1181\n",
            "Epoch 213/300\n",
            "2025/2025 [==============================] - 25s 12ms/step - loss: 0.0152 - val_loss: 0.1205\n",
            "Epoch 214/300\n",
            "2025/2025 [==============================] - 25s 12ms/step - loss: 0.0151 - val_loss: 0.1211\n",
            "Epoch 215/300\n",
            "2025/2025 [==============================] - 24s 12ms/step - loss: 0.0151 - val_loss: 0.1194\n",
            "Epoch 216/300\n",
            "2025/2025 [==============================] - 25s 12ms/step - loss: 0.0150 - val_loss: 0.1181\n",
            "Epoch 217/300\n",
            "2025/2025 [==============================] - 26s 13ms/step - loss: 0.0147 - val_loss: 0.1219\n",
            "Epoch 218/300\n",
            "2025/2025 [==============================] - 26s 13ms/step - loss: 0.0148 - val_loss: 0.1222\n",
            "Epoch 219/300\n",
            "2025/2025 [==============================] - 24s 12ms/step - loss: 0.0149 - val_loss: 0.1170\n",
            "Epoch 220/300\n",
            "2025/2025 [==============================] - 27s 13ms/step - loss: 0.0147 - val_loss: 0.1178\n",
            "Epoch 221/300\n",
            "2025/2025 [==============================] - 25s 12ms/step - loss: 0.0147 - val_loss: 0.1212\n",
            "Epoch 222/300\n",
            "2025/2025 [==============================] - 23s 12ms/step - loss: 0.0147 - val_loss: 0.1188\n",
            "Epoch 223/300\n",
            "2025/2025 [==============================] - 22s 11ms/step - loss: 0.0146 - val_loss: 0.1218\n",
            "Epoch 224/300\n",
            "2025/2025 [==============================] - 23s 11ms/step - loss: 0.0146 - val_loss: 0.1215\n",
            "Epoch 225/300\n",
            "2025/2025 [==============================] - 22s 11ms/step - loss: 0.0145 - val_loss: 0.1234\n",
            "Epoch 226/300\n",
            "2025/2025 [==============================] - 22s 11ms/step - loss: 0.0144 - val_loss: 0.1208\n",
            "Epoch 227/300\n",
            "2025/2025 [==============================] - 22s 11ms/step - loss: 0.0145 - val_loss: 0.1228\n",
            "Epoch 228/300\n",
            "2025/2025 [==============================] - 15s 8ms/step - loss: 0.0144 - val_loss: 0.1234\n",
            "Epoch 229/300\n",
            "2025/2025 [==============================] - 15s 8ms/step - loss: 0.0144 - val_loss: 0.1196\n",
            "Epoch 230/300\n",
            "2025/2025 [==============================] - 15s 8ms/step - loss: 0.0144 - val_loss: 0.1233\n",
            "Epoch 231/300\n",
            "2025/2025 [==============================] - 15s 8ms/step - loss: 0.0142 - val_loss: 0.1229\n",
            "Epoch 232/300\n",
            "2025/2025 [==============================] - 17s 8ms/step - loss: 0.0143 - val_loss: 0.1232\n",
            "Epoch 233/300\n",
            "2025/2025 [==============================] - 17s 8ms/step - loss: 0.0143 - val_loss: 0.1211\n",
            "Epoch 234/300\n",
            "2025/2025 [==============================] - 15s 7ms/step - loss: 0.0141 - val_loss: 0.1260\n",
            "Epoch 235/300\n",
            "2025/2025 [==============================] - 15s 8ms/step - loss: 0.0143 - val_loss: 0.1208\n",
            "Epoch 236/300\n",
            "2025/2025 [==============================] - 15s 8ms/step - loss: 0.0141 - val_loss: 0.1191\n",
            "Epoch 237/300\n",
            "2025/2025 [==============================] - 16s 8ms/step - loss: 0.0141 - val_loss: 0.1227\n",
            "Epoch 238/300\n",
            "2025/2025 [==============================] - 15s 8ms/step - loss: 0.0139 - val_loss: 0.1224\n",
            "Epoch 239/300\n",
            "2025/2025 [==============================] - 15s 8ms/step - loss: 0.0138 - val_loss: 0.1221\n",
            "Epoch 240/300\n",
            "2025/2025 [==============================] - 15s 7ms/step - loss: 0.0138 - val_loss: 0.1219\n",
            "Epoch 241/300\n",
            "2025/2025 [==============================] - 16s 8ms/step - loss: 0.0139 - val_loss: 0.1233\n",
            "Epoch 242/300\n",
            "2025/2025 [==============================] - 16s 8ms/step - loss: 0.0138 - val_loss: 0.1225\n",
            "Epoch 243/300\n",
            "2025/2025 [==============================] - 16s 8ms/step - loss: 0.0138 - val_loss: 0.1214\n",
            "Epoch 244/300\n",
            "2025/2025 [==============================] - 15s 8ms/step - loss: 0.0137 - val_loss: 0.1217\n",
            "Epoch 245/300\n",
            "2025/2025 [==============================] - 15s 8ms/step - loss: 0.0136 - val_loss: 0.1249\n",
            "Epoch 246/300\n",
            "2025/2025 [==============================] - 15s 8ms/step - loss: 0.0137 - val_loss: 0.1211\n",
            "Epoch 247/300\n",
            "2025/2025 [==============================] - 16s 8ms/step - loss: 0.0136 - val_loss: 0.1242\n",
            "Epoch 248/300\n",
            "2025/2025 [==============================] - 16s 8ms/step - loss: 0.0136 - val_loss: 0.1256\n",
            "Epoch 249/300\n",
            "2025/2025 [==============================] - 15s 7ms/step - loss: 0.0135 - val_loss: 0.1230\n",
            "Epoch 250/300\n",
            "2025/2025 [==============================] - 15s 8ms/step - loss: 0.0136 - val_loss: 0.1230\n",
            "Epoch 251/300\n",
            "2025/2025 [==============================] - 15s 7ms/step - loss: 0.0135 - val_loss: 0.1219\n",
            "Epoch 252/300\n",
            "2025/2025 [==============================] - 15s 8ms/step - loss: 0.0135 - val_loss: 0.1264\n",
            "Epoch 253/300\n",
            "2025/2025 [==============================] - 17s 9ms/step - loss: 0.0134 - val_loss: 0.1243\n",
            "Epoch 254/300\n",
            "2025/2025 [==============================] - 15s 7ms/step - loss: 0.0134 - val_loss: 0.1208\n",
            "Epoch 255/300\n",
            "2025/2025 [==============================] - 15s 8ms/step - loss: 0.0134 - val_loss: 0.1255\n",
            "Epoch 256/300\n",
            "2025/2025 [==============================] - 15s 8ms/step - loss: 0.0133 - val_loss: 0.1221\n",
            "Epoch 257/300\n",
            "2025/2025 [==============================] - 15s 8ms/step - loss: 0.0132 - val_loss: 0.1235\n",
            "Epoch 258/300\n",
            "2025/2025 [==============================] - 17s 8ms/step - loss: 0.0132 - val_loss: 0.1224\n",
            "Epoch 259/300\n",
            "2025/2025 [==============================] - 15s 8ms/step - loss: 0.0131 - val_loss: 0.1211\n",
            "Epoch 260/300\n",
            "2025/2025 [==============================] - 16s 8ms/step - loss: 0.0132 - val_loss: 0.1234\n",
            "Epoch 261/300\n",
            "2025/2025 [==============================] - 16s 8ms/step - loss: 0.0131 - val_loss: 0.1252\n",
            "Epoch 262/300\n",
            "2025/2025 [==============================] - 15s 8ms/step - loss: 0.0132 - val_loss: 0.1240\n",
            "Epoch 263/300\n",
            "2025/2025 [==============================] - 17s 8ms/step - loss: 0.0131 - val_loss: 0.1237\n",
            "Epoch 264/300\n",
            "2025/2025 [==============================] - 15s 8ms/step - loss: 0.0130 - val_loss: 0.1254\n",
            "Epoch 265/300\n",
            "2025/2025 [==============================] - 15s 8ms/step - loss: 0.0129 - val_loss: 0.1256\n",
            "Epoch 266/300\n",
            "2025/2025 [==============================] - 15s 8ms/step - loss: 0.0129 - val_loss: 0.1236\n",
            "Epoch 267/300\n",
            "2025/2025 [==============================] - 15s 8ms/step - loss: 0.0128 - val_loss: 0.1241\n",
            "Epoch 268/300\n",
            "2025/2025 [==============================] - 17s 8ms/step - loss: 0.0129 - val_loss: 0.1264\n",
            "Epoch 269/300\n",
            "2025/2025 [==============================] - 15s 8ms/step - loss: 0.0128 - val_loss: 0.1273\n",
            "Epoch 270/300\n",
            "2025/2025 [==============================] - 15s 8ms/step - loss: 0.0128 - val_loss: 0.1232\n",
            "Epoch 271/300\n",
            "2025/2025 [==============================] - 16s 8ms/step - loss: 0.0127 - val_loss: 0.1238\n",
            "Epoch 272/300\n",
            "2025/2025 [==============================] - 15s 8ms/step - loss: 0.0127 - val_loss: 0.1261\n",
            "Epoch 273/300\n",
            "2025/2025 [==============================] - 18s 9ms/step - loss: 0.0127 - val_loss: 0.1261\n",
            "Epoch 274/300\n",
            "2025/2025 [==============================] - 15s 8ms/step - loss: 0.0126 - val_loss: 0.1273\n",
            "Epoch 275/300\n",
            "2025/2025 [==============================] - 15s 8ms/step - loss: 0.0126 - val_loss: 0.1249\n",
            "Epoch 276/300\n",
            "2025/2025 [==============================] - 15s 8ms/step - loss: 0.0126 - val_loss: 0.1268\n",
            "Epoch 277/300\n",
            "2025/2025 [==============================] - 15s 8ms/step - loss: 0.0126 - val_loss: 0.1263\n",
            "Epoch 278/300\n",
            "2025/2025 [==============================] - 17s 8ms/step - loss: 0.0125 - val_loss: 0.1260\n",
            "Epoch 279/300\n",
            "2025/2025 [==============================] - 16s 8ms/step - loss: 0.0124 - val_loss: 0.1239\n",
            "Epoch 280/300\n",
            "2025/2025 [==============================] - 16s 8ms/step - loss: 0.0124 - val_loss: 0.1265\n",
            "Epoch 281/300\n",
            "2025/2025 [==============================] - 15s 8ms/step - loss: 0.0125 - val_loss: 0.1243\n",
            "Epoch 282/300\n",
            "2025/2025 [==============================] - 15s 8ms/step - loss: 0.0124 - val_loss: 0.1271\n",
            "Epoch 283/300\n",
            "2025/2025 [==============================] - 16s 8ms/step - loss: 0.0124 - val_loss: 0.1260\n",
            "Epoch 284/300\n",
            "2025/2025 [==============================] - 16s 8ms/step - loss: 0.0123 - val_loss: 0.1267\n",
            "Epoch 285/300\n",
            "2025/2025 [==============================] - 15s 8ms/step - loss: 0.0124 - val_loss: 0.1246\n",
            "Epoch 286/300\n",
            "2025/2025 [==============================] - 15s 7ms/step - loss: 0.0122 - val_loss: 0.1235\n",
            "Epoch 287/300\n",
            "2025/2025 [==============================] - 15s 8ms/step - loss: 0.0121 - val_loss: 0.1252\n",
            "Epoch 288/300\n",
            "2025/2025 [==============================] - 16s 8ms/step - loss: 0.0122 - val_loss: 0.1238\n",
            "Epoch 289/300\n",
            "2025/2025 [==============================] - 16s 8ms/step - loss: 0.0121 - val_loss: 0.1241\n",
            "Epoch 290/300\n",
            "2025/2025 [==============================] - 15s 8ms/step - loss: 0.0121 - val_loss: 0.1233\n",
            "Epoch 291/300\n",
            "2025/2025 [==============================] - 15s 8ms/step - loss: 0.0121 - val_loss: 0.1276\n",
            "Epoch 292/300\n",
            "2025/2025 [==============================] - 16s 8ms/step - loss: 0.0121 - val_loss: 0.1297\n",
            "Epoch 293/300\n",
            "2025/2025 [==============================] - 17s 8ms/step - loss: 0.0121 - val_loss: 0.1289\n",
            "Epoch 294/300\n",
            "2025/2025 [==============================] - 16s 8ms/step - loss: 0.0121 - val_loss: 0.1252\n",
            "Epoch 295/300\n",
            "2025/2025 [==============================] - 15s 8ms/step - loss: 0.0121 - val_loss: 0.1260\n",
            "Epoch 296/300\n",
            "2025/2025 [==============================] - 15s 7ms/step - loss: 0.0120 - val_loss: 0.1259\n",
            "Epoch 297/300\n",
            "2025/2025 [==============================] - 15s 8ms/step - loss: 0.0119 - val_loss: 0.1256\n",
            "Epoch 298/300\n",
            "2025/2025 [==============================] - 15s 8ms/step - loss: 0.0117 - val_loss: 0.1244\n",
            "Epoch 299/300\n",
            "2025/2025 [==============================] - 17s 8ms/step - loss: 0.0119 - val_loss: 0.1243\n",
            "Epoch 300/300\n",
            "2025/2025 [==============================] - 15s 8ms/step - loss: 0.0118 - val_loss: 0.1264\n",
            "2019-04-20 19:37:39,660 : \n",
            "Training time finished.\n",
            "300 epochs in 1:40:44.614156\n",
            "2019-04-20 19:37:39,660 : Saving Model\n",
            "2019-04-20 19:37:41,741 : Creating result file\n",
            " Pearson: 0.727899\n",
            "Results stored in<_io.TextIOWrapper name='/content/drive/My Drive/Text Similarity/semantic-neural-network/results/output/output_SemEval2014.txt' mode='w' encoding='UTF-8'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9fQZOXZdiwpY",
        "colab_type": "code",
        "outputId": "7ae9c455-23b7-4466-9cc6-63dc1c026d64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 11215
        }
      },
      "cell_type": "code",
      "source": [
        "!python3 \"/content/drive/My Drive/Text Similarity/semantic-neural-network/main.py\" \"sick_2014/SICK_train.txt\" \"sick_2014/SICK_trial.txt\" \"sick_2014/SICK_test_annotated.txt\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "2019-04-21 19:09:40,605 : paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
            "2019-04-21 19:09:40,642 : 'pattern' package not found; tag filters are not available for English\n",
            "[0.875 0.55  0.925 ... 0.    0.    0.   ]\n",
            "[0.575 0.675 0.5   ... 0.    0.    0.   ]\n",
            "True\n",
            "2019-04-21 19:09:42,968 : Loading embedding model from /content/drive/My Drive/Text Similarity/semantic-neural-network/word2vec/GoogleNews-vectors-negative300.bin\n",
            "2019-04-21 19:09:42,969 : Loading existing embedding matrix\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "2019-04-21 19:09:43,916 : From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "2019-04-21 19:09:43.922652: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-04-21 19:09:43.922878: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x27911e0 executing computations on platform Host. Devices:\n",
            "2019-04-21 19:09:43.922909: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-04-21 19:09:44.081046: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-04-21 19:09:44.081564: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x2790dc0 executing computations on platform CUDA. Devices:\n",
            "2019-04-21 19:09:44.081593: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-04-21 19:09:44.081957: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "totalMemory: 14.73GiB freeMemory: 14.60GiB\n",
            "2019-04-21 19:09:44.081980: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
            "2019-04-21 19:09:44.543597: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-04-21 19:09:44.543664: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
            "2019-04-21 19:09:44.543678: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
            "2019-04-21 19:09:44.543957: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-04-21 19:09:44.544004: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14115 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "2019-04-21 19:09:44,897 : START TRAIN\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "2019-04-21 19:09:44,950 : From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 3834 samples, validate on 427 samples\n",
            "Epoch 1/300\n",
            "2019-04-21 19:09:46.422606: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\n",
            "3834/3834 [==============================] - 32s 8ms/step - loss: 0.2944 - val_loss: 0.3083\n",
            "Epoch 2/300\n",
            "3834/3834 [==============================] - 31s 8ms/step - loss: 0.2678 - val_loss: 0.2825\n",
            "Epoch 3/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.2457 - val_loss: 0.2607\n",
            "Epoch 4/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.2152 - val_loss: 0.2205\n",
            "Epoch 5/300\n",
            "3834/3834 [==============================] - 31s 8ms/step - loss: 0.0986 - val_loss: 0.0591\n",
            "Epoch 6/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0512 - val_loss: 0.0521\n",
            "Epoch 7/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0453 - val_loss: 0.0489\n",
            "Epoch 8/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0414 - val_loss: 0.0471\n",
            "Epoch 9/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0383 - val_loss: 0.0443\n",
            "Epoch 10/300\n",
            "3834/3834 [==============================] - 31s 8ms/step - loss: 0.0358 - val_loss: 0.0435\n",
            "Epoch 11/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0336 - val_loss: 0.0429\n",
            "Epoch 12/300\n",
            "3834/3834 [==============================] - 31s 8ms/step - loss: 0.0319 - val_loss: 0.0423\n",
            "Epoch 13/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0305 - val_loss: 0.0410\n",
            "Epoch 14/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0295 - val_loss: 0.0402\n",
            "Epoch 15/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0286 - val_loss: 0.0402\n",
            "Epoch 16/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0277 - val_loss: 0.0393\n",
            "Epoch 17/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0270 - val_loss: 0.0388\n",
            "Epoch 18/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0264 - val_loss: 0.0387\n",
            "Epoch 19/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0257 - val_loss: 0.0380\n",
            "Epoch 20/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0252 - val_loss: 0.0378\n",
            "Epoch 21/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0246 - val_loss: 0.0384\n",
            "Epoch 22/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0242 - val_loss: 0.0369\n",
            "Epoch 23/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0238 - val_loss: 0.0377\n",
            "Epoch 24/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0233 - val_loss: 0.0367\n",
            "Epoch 25/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0230 - val_loss: 0.0367\n",
            "Epoch 26/300\n",
            "3834/3834 [==============================] - 31s 8ms/step - loss: 0.0226 - val_loss: 0.0366\n",
            "Epoch 27/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0223 - val_loss: 0.0363\n",
            "Epoch 28/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0220 - val_loss: 0.0359\n",
            "Epoch 29/300\n",
            "3834/3834 [==============================] - 31s 8ms/step - loss: 0.0216 - val_loss: 0.0353\n",
            "Epoch 30/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0213 - val_loss: 0.0355\n",
            "Epoch 31/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0211 - val_loss: 0.0356\n",
            "Epoch 32/300\n",
            "3834/3834 [==============================] - 31s 8ms/step - loss: 0.0208 - val_loss: 0.0352\n",
            "Epoch 33/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0205 - val_loss: 0.0362\n",
            "Epoch 34/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0203 - val_loss: 0.0349\n",
            "Epoch 35/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0200 - val_loss: 0.0348\n",
            "Epoch 36/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0199 - val_loss: 0.0352\n",
            "Epoch 37/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0196 - val_loss: 0.0347\n",
            "Epoch 38/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0194 - val_loss: 0.0343\n",
            "Epoch 39/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0192 - val_loss: 0.0346\n",
            "Epoch 40/300\n",
            "3834/3834 [==============================] - 31s 8ms/step - loss: 0.0191 - val_loss: 0.0345\n",
            "Epoch 41/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0188 - val_loss: 0.0350\n",
            "Epoch 42/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0186 - val_loss: 0.0345\n",
            "Epoch 43/300\n",
            "3834/3834 [==============================] - 31s 8ms/step - loss: 0.0185 - val_loss: 0.0346\n",
            "Epoch 44/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0183 - val_loss: 0.0351\n",
            "Epoch 45/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0181 - val_loss: 0.0342\n",
            "Epoch 46/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0180 - val_loss: 0.0353\n",
            "Epoch 47/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0178 - val_loss: 0.0335\n",
            "Epoch 48/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0177 - val_loss: 0.0338\n",
            "Epoch 49/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0175 - val_loss: 0.0350\n",
            "Epoch 50/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0173 - val_loss: 0.0336\n",
            "Epoch 51/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0172 - val_loss: 0.0341\n",
            "Epoch 52/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0171 - val_loss: 0.0341\n",
            "Epoch 53/300\n",
            "3834/3834 [==============================] - 31s 8ms/step - loss: 0.0170 - val_loss: 0.0342\n",
            "Epoch 54/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0168 - val_loss: 0.0336\n",
            "Epoch 55/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0167 - val_loss: 0.0334\n",
            "Epoch 56/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0166 - val_loss: 0.0334\n",
            "Epoch 57/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0164 - val_loss: 0.0338\n",
            "Epoch 58/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0163 - val_loss: 0.0339\n",
            "Epoch 59/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0162 - val_loss: 0.0332\n",
            "Epoch 60/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0161 - val_loss: 0.0344\n",
            "Epoch 61/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0160 - val_loss: 0.0339\n",
            "Epoch 62/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0159 - val_loss: 0.0342\n",
            "Epoch 63/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0158 - val_loss: 0.0351\n",
            "Epoch 64/300\n",
            "3834/3834 [==============================] - 32s 8ms/step - loss: 0.0157 - val_loss: 0.0334\n",
            "Epoch 65/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0156 - val_loss: 0.0333\n",
            "Epoch 66/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0155 - val_loss: 0.0332\n",
            "Epoch 67/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0154 - val_loss: 0.0337\n",
            "Epoch 68/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0153 - val_loss: 0.0340\n",
            "Epoch 69/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0151 - val_loss: 0.0338\n",
            "Epoch 70/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0151 - val_loss: 0.0333\n",
            "Epoch 71/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0150 - val_loss: 0.0336\n",
            "Epoch 72/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0149 - val_loss: 0.0338\n",
            "Epoch 73/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0149 - val_loss: 0.0331\n",
            "Epoch 74/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0148 - val_loss: 0.0330\n",
            "Epoch 75/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0147 - val_loss: 0.0341\n",
            "Epoch 76/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0146 - val_loss: 0.0338\n",
            "Epoch 77/300\n",
            "3834/3834 [==============================] - 29s 7ms/step - loss: 0.0145 - val_loss: 0.0345\n",
            "Epoch 78/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0144 - val_loss: 0.0335\n",
            "Epoch 79/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0143 - val_loss: 0.0335\n",
            "Epoch 80/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0143 - val_loss: 0.0334\n",
            "Epoch 81/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0142 - val_loss: 0.0335\n",
            "Epoch 82/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0141 - val_loss: 0.0333\n",
            "Epoch 83/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0141 - val_loss: 0.0338\n",
            "Epoch 84/300\n",
            "3834/3834 [==============================] - 29s 7ms/step - loss: 0.0140 - val_loss: 0.0336\n",
            "Epoch 85/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0139 - val_loss: 0.0332\n",
            "Epoch 86/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0138 - val_loss: 0.0338\n",
            "Epoch 87/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0138 - val_loss: 0.0348\n",
            "Epoch 88/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0137 - val_loss: 0.0334\n",
            "Epoch 89/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0137 - val_loss: 0.0330\n",
            "Epoch 90/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0136 - val_loss: 0.0327\n",
            "Epoch 91/300\n",
            "3834/3834 [==============================] - 31s 8ms/step - loss: 0.0136 - val_loss: 0.0334\n",
            "Epoch 92/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0135 - val_loss: 0.0346\n",
            "Epoch 93/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0134 - val_loss: 0.0332\n",
            "Epoch 94/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0134 - val_loss: 0.0330\n",
            "Epoch 95/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0133 - val_loss: 0.0342\n",
            "Epoch 96/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0132 - val_loss: 0.0324\n",
            "Epoch 97/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0132 - val_loss: 0.0333\n",
            "Epoch 98/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0131 - val_loss: 0.0342\n",
            "Epoch 99/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0131 - val_loss: 0.0338\n",
            "Epoch 100/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0130 - val_loss: 0.0342\n",
            "Epoch 101/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0130 - val_loss: 0.0340\n",
            "Epoch 102/300\n",
            "3834/3834 [==============================] - 31s 8ms/step - loss: 0.0129 - val_loss: 0.0329\n",
            "Epoch 103/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0129 - val_loss: 0.0333\n",
            "Epoch 104/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0128 - val_loss: 0.0322\n",
            "Epoch 105/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0128 - val_loss: 0.0334\n",
            "Epoch 106/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0127 - val_loss: 0.0328\n",
            "Epoch 107/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0126 - val_loss: 0.0328\n",
            "Epoch 108/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0126 - val_loss: 0.0327\n",
            "Epoch 109/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0126 - val_loss: 0.0334\n",
            "Epoch 110/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0125 - val_loss: 0.0332\n",
            "Epoch 111/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0125 - val_loss: 0.0328\n",
            "Epoch 112/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0125 - val_loss: 0.0331\n",
            "Epoch 113/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0124 - val_loss: 0.0332\n",
            "Epoch 114/300\n",
            "3834/3834 [==============================] - 29s 7ms/step - loss: 0.0123 - val_loss: 0.0325\n",
            "Epoch 115/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0123 - val_loss: 0.0326\n",
            "Epoch 116/300\n",
            "3834/3834 [==============================] - 31s 8ms/step - loss: 0.0123 - val_loss: 0.0322\n",
            "Epoch 117/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0122 - val_loss: 0.0334\n",
            "Epoch 118/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0122 - val_loss: 0.0331\n",
            "Epoch 119/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0121 - val_loss: 0.0330\n",
            "Epoch 120/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0121 - val_loss: 0.0335\n",
            "Epoch 121/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0120 - val_loss: 0.0339\n",
            "Epoch 122/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0120 - val_loss: 0.0343\n",
            "Epoch 123/300\n",
            "3834/3834 [==============================] - 29s 7ms/step - loss: 0.0119 - val_loss: 0.0331\n",
            "Epoch 124/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0119 - val_loss: 0.0328\n",
            "Epoch 125/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0119 - val_loss: 0.0326\n",
            "Epoch 126/300\n",
            "3834/3834 [==============================] - 29s 7ms/step - loss: 0.0118 - val_loss: 0.0324\n",
            "Epoch 127/300\n",
            "3834/3834 [==============================] - 31s 8ms/step - loss: 0.0118 - val_loss: 0.0340\n",
            "Epoch 128/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0118 - val_loss: 0.0334\n",
            "Epoch 129/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0117 - val_loss: 0.0331\n",
            "Epoch 130/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0117 - val_loss: 0.0323\n",
            "Epoch 131/300\n",
            "3834/3834 [==============================] - 29s 7ms/step - loss: 0.0116 - val_loss: 0.0329\n",
            "Epoch 132/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0116 - val_loss: 0.0326\n",
            "Epoch 133/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0116 - val_loss: 0.0324\n",
            "Epoch 134/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0115 - val_loss: 0.0322\n",
            "Epoch 135/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0115 - val_loss: 0.0327\n",
            "Epoch 136/300\n",
            "3834/3834 [==============================] - 29s 7ms/step - loss: 0.0115 - val_loss: 0.0332\n",
            "Epoch 137/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0114 - val_loss: 0.0331\n",
            "Epoch 138/300\n",
            "3834/3834 [==============================] - 31s 8ms/step - loss: 0.0114 - val_loss: 0.0335\n",
            "Epoch 139/300\n",
            "3834/3834 [==============================] - 29s 7ms/step - loss: 0.0114 - val_loss: 0.0324\n",
            "Epoch 140/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0113 - val_loss: 0.0340\n",
            "Epoch 141/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0113 - val_loss: 0.0333\n",
            "Epoch 142/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0113 - val_loss: 0.0318\n",
            "Epoch 143/300\n",
            "3834/3834 [==============================] - 31s 8ms/step - loss: 0.0113 - val_loss: 0.0325\n",
            "Epoch 144/300\n",
            "3834/3834 [==============================] - 29s 7ms/step - loss: 0.0112 - val_loss: 0.0328\n",
            "Epoch 145/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0112 - val_loss: 0.0337\n",
            "Epoch 146/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0111 - val_loss: 0.0321\n",
            "Epoch 147/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0111 - val_loss: 0.0324\n",
            "Epoch 148/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0110 - val_loss: 0.0333\n",
            "Epoch 149/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0111 - val_loss: 0.0324\n",
            "Epoch 150/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0110 - val_loss: 0.0331\n",
            "Epoch 151/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0110 - val_loss: 0.0330\n",
            "Epoch 152/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0109 - val_loss: 0.0327\n",
            "Epoch 153/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0109 - val_loss: 0.0327\n",
            "Epoch 154/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0109 - val_loss: 0.0322\n",
            "Epoch 155/300\n",
            "3834/3834 [==============================] - 29s 7ms/step - loss: 0.0108 - val_loss: 0.0316\n",
            "Epoch 156/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0108 - val_loss: 0.0329\n",
            "Epoch 157/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0108 - val_loss: 0.0328\n",
            "Epoch 158/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0108 - val_loss: 0.0328\n",
            "Epoch 159/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0107 - val_loss: 0.0329\n",
            "Epoch 160/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0107 - val_loss: 0.0324\n",
            "Epoch 161/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0107 - val_loss: 0.0325\n",
            "Epoch 162/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0106 - val_loss: 0.0324\n",
            "Epoch 163/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0106 - val_loss: 0.0323\n",
            "Epoch 164/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0106 - val_loss: 0.0319\n",
            "Epoch 165/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0106 - val_loss: 0.0322\n",
            "Epoch 166/300\n",
            "3834/3834 [==============================] - 29s 7ms/step - loss: 0.0105 - val_loss: 0.0330\n",
            "Epoch 167/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0105 - val_loss: 0.0329\n",
            "Epoch 168/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0105 - val_loss: 0.0320\n",
            "Epoch 169/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0104 - val_loss: 0.0324\n",
            "Epoch 170/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0104 - val_loss: 0.0321\n",
            "Epoch 171/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0104 - val_loss: 0.0333\n",
            "Epoch 172/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0104 - val_loss: 0.0325\n",
            "Epoch 173/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0103 - val_loss: 0.0326\n",
            "Epoch 174/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0103 - val_loss: 0.0332\n",
            "Epoch 175/300\n",
            "3834/3834 [==============================] - 29s 7ms/step - loss: 0.0103 - val_loss: 0.0336\n",
            "Epoch 176/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0103 - val_loss: 0.0321\n",
            "Epoch 177/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0102 - val_loss: 0.0321\n",
            "Epoch 178/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0102 - val_loss: 0.0329\n",
            "Epoch 179/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0102 - val_loss: 0.0320\n",
            "Epoch 180/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0102 - val_loss: 0.0320\n",
            "Epoch 181/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0101 - val_loss: 0.0325\n",
            "Epoch 182/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0101 - val_loss: 0.0330\n",
            "Epoch 183/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0101 - val_loss: 0.0326\n",
            "Epoch 184/300\n",
            "3834/3834 [==============================] - 31s 8ms/step - loss: 0.0101 - val_loss: 0.0327\n",
            "Epoch 185/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0101 - val_loss: 0.0322\n",
            "Epoch 186/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0100 - val_loss: 0.0337\n",
            "Epoch 187/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0100 - val_loss: 0.0321\n",
            "Epoch 188/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0100 - val_loss: 0.0323\n",
            "Epoch 189/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0100 - val_loss: 0.0334\n",
            "Epoch 190/300\n",
            "3834/3834 [==============================] - 32s 8ms/step - loss: 0.0099 - val_loss: 0.0326\n",
            "Epoch 191/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0099 - val_loss: 0.0323\n",
            "Epoch 192/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0099 - val_loss: 0.0325\n",
            "Epoch 193/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0099 - val_loss: 0.0338\n",
            "Epoch 194/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0098 - val_loss: 0.0326\n",
            "Epoch 195/300\n",
            "3834/3834 [==============================] - 31s 8ms/step - loss: 0.0098 - val_loss: 0.0335\n",
            "Epoch 196/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0098 - val_loss: 0.0323\n",
            "Epoch 197/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0098 - val_loss: 0.0326\n",
            "Epoch 198/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0097 - val_loss: 0.0333\n",
            "Epoch 199/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0098 - val_loss: 0.0341\n",
            "Epoch 200/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0097 - val_loss: 0.0331\n",
            "Epoch 201/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0097 - val_loss: 0.0328\n",
            "Epoch 202/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0097 - val_loss: 0.0327\n",
            "Epoch 203/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0097 - val_loss: 0.0315\n",
            "Epoch 204/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0096 - val_loss: 0.0321\n",
            "Epoch 205/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0096 - val_loss: 0.0315\n",
            "Epoch 206/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0096 - val_loss: 0.0323\n",
            "Epoch 207/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0096 - val_loss: 0.0318\n",
            "Epoch 208/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0095 - val_loss: 0.0315\n",
            "Epoch 209/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0095 - val_loss: 0.0314\n",
            "Epoch 210/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0096 - val_loss: 0.0324\n",
            "Epoch 211/300\n",
            "3834/3834 [==============================] - 31s 8ms/step - loss: 0.0095 - val_loss: 0.0326\n",
            "Epoch 212/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0095 - val_loss: 0.0326\n",
            "Epoch 213/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0095 - val_loss: 0.0320\n",
            "Epoch 214/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0094 - val_loss: 0.0322\n",
            "Epoch 215/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0095 - val_loss: 0.0314\n",
            "Epoch 216/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0094 - val_loss: 0.0324\n",
            "Epoch 217/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0094 - val_loss: 0.0321\n",
            "Epoch 218/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0094 - val_loss: 0.0322\n",
            "Epoch 219/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0094 - val_loss: 0.0328\n",
            "Epoch 220/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0094 - val_loss: 0.0330\n",
            "Epoch 221/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0093 - val_loss: 0.0333\n",
            "Epoch 222/300\n",
            "3834/3834 [==============================] - 32s 8ms/step - loss: 0.0093 - val_loss: 0.0323\n",
            "Epoch 223/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0093 - val_loss: 0.0316\n",
            "Epoch 224/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0093 - val_loss: 0.0327\n",
            "Epoch 225/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0093 - val_loss: 0.0322\n",
            "Epoch 226/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0092 - val_loss: 0.0333\n",
            "Epoch 227/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0092 - val_loss: 0.0328\n",
            "Epoch 228/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0092 - val_loss: 0.0325\n",
            "Epoch 229/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0092 - val_loss: 0.0319\n",
            "Epoch 230/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0092 - val_loss: 0.0318\n",
            "Epoch 231/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0092 - val_loss: 0.0323\n",
            "Epoch 232/300\n",
            "3834/3834 [==============================] - 31s 8ms/step - loss: 0.0092 - val_loss: 0.0335\n",
            "Epoch 233/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0091 - val_loss: 0.0324\n",
            "Epoch 234/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0091 - val_loss: 0.0315\n",
            "Epoch 235/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0091 - val_loss: 0.0327\n",
            "Epoch 236/300\n",
            "3834/3834 [==============================] - 31s 8ms/step - loss: 0.0091 - val_loss: 0.0319\n",
            "Epoch 237/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0091 - val_loss: 0.0322\n",
            "Epoch 238/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0090 - val_loss: 0.0323\n",
            "Epoch 239/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0090 - val_loss: 0.0313\n",
            "Epoch 240/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0090 - val_loss: 0.0332\n",
            "Epoch 241/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0090 - val_loss: 0.0318\n",
            "Epoch 242/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0090 - val_loss: 0.0342\n",
            "Epoch 243/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0090 - val_loss: 0.0310\n",
            "Epoch 244/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0089 - val_loss: 0.0316\n",
            "Epoch 245/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0089 - val_loss: 0.0333\n",
            "Epoch 246/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0089 - val_loss: 0.0325\n",
            "Epoch 247/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0089 - val_loss: 0.0319\n",
            "Epoch 248/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0089 - val_loss: 0.0319\n",
            "Epoch 249/300\n",
            "3834/3834 [==============================] - 31s 8ms/step - loss: 0.0089 - val_loss: 0.0339\n",
            "Epoch 250/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0089 - val_loss: 0.0315\n",
            "Epoch 251/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0088 - val_loss: 0.0318\n",
            "Epoch 252/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0088 - val_loss: 0.0329\n",
            "Epoch 253/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0088 - val_loss: 0.0323\n",
            "Epoch 254/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0088 - val_loss: 0.0328\n",
            "Epoch 255/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0088 - val_loss: 0.0316\n",
            "Epoch 256/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0088 - val_loss: 0.0318\n",
            "Epoch 257/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0088 - val_loss: 0.0336\n",
            "Epoch 258/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0087 - val_loss: 0.0331\n",
            "Epoch 259/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0087 - val_loss: 0.0323\n",
            "Epoch 260/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0087 - val_loss: 0.0319\n",
            "Epoch 261/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0087 - val_loss: 0.0322\n",
            "Epoch 262/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0087 - val_loss: 0.0319\n",
            "Epoch 263/300\n",
            "3834/3834 [==============================] - 31s 8ms/step - loss: 0.0087 - val_loss: 0.0327\n",
            "Epoch 264/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0086 - val_loss: 0.0326\n",
            "Epoch 265/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0086 - val_loss: 0.0337\n",
            "Epoch 266/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0086 - val_loss: 0.0309\n",
            "Epoch 267/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0086 - val_loss: 0.0321\n",
            "Epoch 268/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0086 - val_loss: 0.0332\n",
            "Epoch 269/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0085 - val_loss: 0.0318\n",
            "Epoch 270/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0085 - val_loss: 0.0324\n",
            "Epoch 271/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0085 - val_loss: 0.0333\n",
            "Epoch 272/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0086 - val_loss: 0.0328\n",
            "Epoch 273/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0085 - val_loss: 0.0330\n",
            "Epoch 274/300\n",
            "3834/3834 [==============================] - 31s 8ms/step - loss: 0.0085 - val_loss: 0.0318\n",
            "Epoch 275/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0085 - val_loss: 0.0322\n",
            "Epoch 276/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0085 - val_loss: 0.0329\n",
            "Epoch 277/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0085 - val_loss: 0.0332\n",
            "Epoch 278/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0085 - val_loss: 0.0326\n",
            "Epoch 279/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0084 - val_loss: 0.0333\n",
            "Epoch 280/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0084 - val_loss: 0.0326\n",
            "Epoch 281/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0084 - val_loss: 0.0318\n",
            "Epoch 282/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0084 - val_loss: 0.0330\n",
            "Epoch 283/300\n",
            "3834/3834 [==============================] - 29s 7ms/step - loss: 0.0084 - val_loss: 0.0323\n",
            "Epoch 284/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0084 - val_loss: 0.0319\n",
            "Epoch 285/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0084 - val_loss: 0.0315\n",
            "Epoch 286/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0083 - val_loss: 0.0323\n",
            "Epoch 287/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0083 - val_loss: 0.0316\n",
            "Epoch 288/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0083 - val_loss: 0.0319\n",
            "Epoch 289/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0083 - val_loss: 0.0316\n",
            "Epoch 290/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0083 - val_loss: 0.0322\n",
            "Epoch 291/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0083 - val_loss: 0.0327\n",
            "Epoch 292/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0083 - val_loss: 0.0324\n",
            "Epoch 293/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0083 - val_loss: 0.0327\n",
            "Epoch 294/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0083 - val_loss: 0.0322\n",
            "Epoch 295/300\n",
            "3834/3834 [==============================] - 31s 8ms/step - loss: 0.0082 - val_loss: 0.0318\n",
            "Epoch 296/300\n",
            "3834/3834 [==============================] - 30s 8ms/step - loss: 0.0082 - val_loss: 0.0323\n",
            "Epoch 297/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0082 - val_loss: 0.0324\n",
            "Epoch 298/300\n",
            "3834/3834 [==============================] - 31s 8ms/step - loss: 0.0082 - val_loss: 0.0341\n",
            "Epoch 299/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0082 - val_loss: 0.0331\n",
            "Epoch 300/300\n",
            "3834/3834 [==============================] - 29s 8ms/step - loss: 0.0082 - val_loss: 0.0325\n",
            "2019-04-21 21:37:51,566 : \n",
            "Training time finished.\n",
            "300 epochs in 2:28:06.667855\n",
            "2019-04-21 21:37:51,566 : Saving Model\n",
            "2019-04-21 21:38:05,175 : Creating result file\n",
            " Pearson: 0.8068452\n",
            "Results stored in/content/drive/My Drive/Text Similarity/semantic-neural-network/results/output/output_SICK.txt\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}